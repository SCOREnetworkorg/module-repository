[
  {
    "objectID": "by-statsds-topic.html",
    "href": "by-statsds-topic.html",
    "title": "Modules By Topic",
    "section": "",
    "text": "2023 Boston Marathon - Variability in Finish Times\n\n\n\n\n\n\nhistograms\n\n\nsummary statistics\n\n\nbimodal data\n\n\n\nDescribing finish time for runners in the 2023 Boston Marathon\n\n\n\n\n\nMay 13, 2024\n\n\nIvan Ramler, Jack Fay\n\n\n\n\n\n\n\n\n\n\n\n\nFIRST Robotics Competition - Winning Chances\n\n\n\n\n\n\nBrier score\n\n\nprediction assessment\n\n\n\nEvaluating the predicted winning probabilities against the actual outcomes.\n\n\n\n\n\nMar 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIronman Triathlete Performance\n\n\n\n\n\n\nScatterplots\n\n\nCorrelation\n\n\n\nGaining insight into the performance patterns of triathletes by exploring the relationships between swimming, biking, and running times.\n\n\n\n\n\nJul 23, 2023\n\n\nMichael Schuckers, Matt Abell, AJ Dykstra, Sarah Weaver, Ivan Ramler, and Robin Lock\n\n\n\n\n\n\n\n\n\n\n\n\nIronman Triathlon (Canadian Females) - Multiple Linear Regression\n\n\n\n\n\n\nLinear regression\n\n\n\nUsing Lake Placid Ironman triathlon results for female Canadian finishers to predict run times for participants based on both swim and bike times.\n\n\n\n\n\nFeb 5, 2024\n\n\nA.J. Dykstra, Ivan Ramler\n\n\n\n\n\n\n\n\n\n\n\n\nLacrosse Faceoff Proportions\n\n\n\n\n\n\nHypothesis testing\n\n\nSingle proportion\n\n\n\nUsing data from NCAA Div I lacrosse teams to explore the importance of winning faceoffs\n\n\n\n\n\nFeb 5, 2024\n\n\nJack Fay, Ivan Ramler, A.J. Dykstra\n\n\n\n\n\n\n\n\n\n\n\n\nLacrosse PLL vs. NLL\n\n\n\n\n\n\nDifference in two means\n\n\n\nComparing scoring rates between indoor and outdoor profesional lacrosse leagues.\n\n\n\n\n\nFeb 5, 2024\n\n\nJack Cowan, Ivan Ramler, A.J. Dykstra, Robin Lock\n\n\n\n\n\n\n\n\n\n\n\n\nLeague of Legends - Buffing and Nerfing\n\n\n\n\n\n\noutliers\n\n\nsummary statistics\n\n\n\nInvestigating game play statistics for League of Legends champions in two different patches.\n\n\n\n\n\nFeb 21, 2024\n\n\nIvan Ramler, George Charalambous, A.J. Dykstra\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface\n\n\n\n\n\n\nANOVA\n\n\nTennis\n\n\n\nUsing tennis to teach ANOVA and linear regression with categorical variables\n\n\n\n\n\nJan 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMLB Injuries - Introductory Time Series Analysis\n\n\n\n\n\n\nTime series plots\n\n\nTime series decomposition\n\n\nResidual analysis\n\n\nSimple forecasting\n\n\n\nExploring MLB injury data through time series analysis and forecasting.\n\n\n\n\n\nNov 26, 2024\n\n\nJonathan Lieb\n\n\n\n\n\n\n\n\n\n\n\n\nMarathon Record-Setting Over Time\n\n\n\n\n\n\nExponential distribution\n\n\nPoisson process\n\n\n\nDetermining whether the setting of world records in the marathon is historically a Poisson process.\n\n\n\n\n\nJul 23, 2023\n\n\nNicholas Clark, Rodney Sturdivant, and Kate Sanborn\n\n\n\n\n\n\n\n\n\n\n\n\nNASCAR Transformation Module\n\n\n\n\n\n\nLinear regression\n\n\nTransformations\n\n\nPolynomial regression\n\n\n\nUsing NASCAR driver rating data to explore a series of transformations to improve linearity in regression.\n\n\n\n\n\nFeb 5, 2024\n\n\nAlyssa Bigness, Ivan Ramler, Jack Fay\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting NHL Shooting Percentages\n\n\n\n\n\n\nlinear regression\n\n\n\nAn Introduction to Simple Linear Regression\n\n\n\n\n\nJul 23, 2023\n\n\nSam Ventura\n\n\n\n\n\n\n\n\n\n\n\n\nStolen Bases\n\n\n\n\n\n\nNormality tests\n\n\n\n\n\n\n\n\n\nJul 23, 2023\n\n\nAndrew Lee and Jacob Hurtubise\n\n\n\n\n\n\n\n\n\n\n\n\nUnbreakable Records in Baseball\n\n\n\n\n\n\nBernoulli distribution\n\n\nBinomial distribution\n\n\nChi-Square Test\n\n\n\n\n\n\n\n\n\nJul 23, 2023\n\n\nAndrew Lee and Fr Gabriel Costa\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Modules By Topic"
    ]
  },
  {
    "objectID": "triathlons/ironman-triathlete-performance/index.html",
    "href": "triathlons/ironman-triathlete-performance/index.html",
    "title": "Ironman Triathlete Performance",
    "section": "",
    "text": "The motivation for this data analysis is to explore the relationships between swim times, bike times, and run times (in minutes) in order to gain insights into the performance patterns of the athletes. By analyzing these relationships, we can understand the interplay between different segments of the race and potentially identify areas of improvement for athletes. For this activity, we will specifically focus on times from finishers in the years 2018 and 2019.",
    "crumbs": [
      "Home",
      "Triathlons",
      "Ironman Triathlete Performance"
    ]
  },
  {
    "objectID": "triathlons/ironman-triathlete-performance/index.html#motivation",
    "href": "triathlons/ironman-triathlete-performance/index.html#motivation",
    "title": "Ironman Triathlete Performance",
    "section": "",
    "text": "The motivation for this data analysis is to explore the relationships between swim times, bike times, and run times (in minutes) in order to gain insights into the performance patterns of the athletes. By analyzing these relationships, we can understand the interplay between different segments of the race and potentially identify areas of improvement for athletes. For this activity, we will specifically focus on times from finishers in the years 2018 and 2019.",
    "crumbs": [
      "Home",
      "Triathlons",
      "Ironman Triathlete Performance"
    ]
  },
  {
    "objectID": "triathlons/ironman-triathlete-performance/index.html#module",
    "href": "triathlons/ironman-triathlete-performance/index.html#module",
    "title": "Ironman Triathlete Performance",
    "section": "Module",
    "text": "Module\nhttps://isle.stat.cmu.edu/SCORE/ironman_triathlon/",
    "crumbs": [
      "Home",
      "Triathlons",
      "Ironman Triathlete Performance"
    ]
  },
  {
    "objectID": "tennis/Teaching_ANOVA_Through_Aces/index.html",
    "href": "tennis/Teaching_ANOVA_Through_Aces/index.html",
    "title": "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface",
    "section": "",
    "text": "An accompanying worksheet for instructors is available here\nA worksheet for students to follow is available here\nPlease watch this intro video from tennis analyst Dr. Stephanie Kovalchik to learn about the questions we’ll be investigating today!",
    "crumbs": [
      "Home",
      "Tennis",
      "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface"
    ]
  },
  {
    "objectID": "tennis/Teaching_ANOVA_Through_Aces/index.html#welcome",
    "href": "tennis/Teaching_ANOVA_Through_Aces/index.html#welcome",
    "title": "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface",
    "section": "",
    "text": "An accompanying worksheet for instructors is available here\nA worksheet for students to follow is available here\nPlease watch this intro video from tennis analyst Dr. Stephanie Kovalchik to learn about the questions we’ll be investigating today!",
    "crumbs": [
      "Home",
      "Tennis",
      "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface"
    ]
  },
  {
    "objectID": "tennis/Teaching_ANOVA_Through_Aces/index.html#background",
    "href": "tennis/Teaching_ANOVA_Through_Aces/index.html#background",
    "title": "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface",
    "section": "Background",
    "text": "Background\nThis lesson is going to investigate whether ace rates differ across different surfaces in tennis. We will begin by exploring the data and conducting some descriptive analyses and visualizations. Then we will answer our question using linear regression and dummy variables for categorical data.\nAnd, in the process, we will learn that analysis of variance (ANOVA) - commonly used to compare means of a continuous variable across three or more groups - is just a special case of linear regression!\n\nTennis Basics\nIf you are not familiar with tennis, consider watching this 4-minute introductory video.\nIn a basic singles match, two players face off against each other on either side of the court, hitting a tennis ball back and forth.\nA single tennis match is typically split into either 3 or 5 sets, and the winner of a majority of the sets wins the match.\nEach set is split into games (or service games) - a player must win 6 games and at least 2 more than an opponent or win a tiebreak game to win the set.\nTo win a game a player must score at least 4 points and at least 2 more than their opponent.\nEach point starts off with a player serving the ball to another by tossing it into the air and hitting it with their racket. The opposing player then tries to return it back to the server’s side of the court, and play continues like this until one player fails to legally return the ball. The other player scores a point.\nThe player who serves switches each game, hence the term “service game.”\n\n\nSurface\nIn tennis, the surface refers to the type of court on which the game is played. There are three types of surfaces in professional tennis:\n\nClay: Clay courts are made of crushed brick, shale, or stone, and they have a slower pace compared to grass courts. The ball tends to bounce higher on clay, making it favorable for baseline rallies. The French Open is played on clay courts.\nGrass: Grass courts are known for their fast and low-bouncing nature. Wimbledon, one of the most prestigious tennis tournaments, is played on grass courts.\nHard courts: Hard courts are typically made of asphalt or concrete covered with a top layer of synthetic materials. They offer a medium-paced game and are the most common type of court. The US Open and the Australian Open are played on hard courts.\n\n\n\nAces\nAn ace occurs when a player serves the ball and the opponent fails to touch it with their racket, winning an immediate point for the server. Aces are considered a significant achievement for a server.\n\n\n\n\n\n\n\n\n\n\n\n\nAce rate, defined as the number of aces per service game, will be our main outcome. We will create it below.",
    "crumbs": [
      "Home",
      "Tennis",
      "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface"
    ]
  },
  {
    "objectID": "tennis/Teaching_ANOVA_Through_Aces/index.html#data-exploration-and-descriptive-analysis",
    "href": "tennis/Teaching_ANOVA_Through_Aces/index.html#data-exploration-and-descriptive-analysis",
    "title": "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface",
    "section": "Data Exploration and Descriptive Analysis",
    "text": "Data Exploration and Descriptive Analysis\nBelow is the data from all 2023 Association of Tennis Professionals (ATP) tour-level main draw matches. This is the top-tier men’s tennis tour.\nThe original data was sourced from a Github repository by Jeff Sackmann and Tennis Abstract: https://github.com/JeffSackmann/tennis_atp. Its formatting has been modified somewhat for this tutorial.\nLet’s begin by exploring the data. The data is stored in a table called match2023_df.\n\n\n# Print the (first 1000 rows of) the data\nmatch2023_df\n\n# Get a summary of the data\nsummary(match2023_df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGiven that each row represents a match, based on the previous terminology, can you guess what each of these columns represents?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculate Ace Rate\nRemember what we said above about simply looking at the number of aces in a match being insufficient? It is easy to get more aces if the match is longer (that is, there are more service games). We can look at the data to verify our hypothesis:\n\n\nggplot(match2023_df, aes(x = total_ServiceGames, y = total_aces)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(x = \"Service Games in Match\", y = \"Aces in Match\") +\n  ggtitle(\"Association Between Number of Aces and Service Games in Tennis Matches\")\n\n\n\nThere indeed is a positive association between winner’s number of ace and winner’s service games.\nSince some surfaces lend themselves to longer matches than others we need to adjust for match length so that any differences we see are more likely due to the surfaces themselves rather than the fact that longer matches are played on them.\nTo do this we calculate ace rate, which we define as the number of aces divided by the number of service games in a match.\n\\[ace\\_rate = \\frac{total\\_aces}{total\\_ServiceGame}\\]\nHere is some code to create that variable:\n\n\n match2023_df &lt;- match2023_df %&gt;%\n    mutate(ace_rate = total_aces/total_ServiceGames)\n\n# Look at the first 6 rows of the data with the new variable\nhead(match2023_df)\n\n\n\nIdeally we would have used the number of aces per serve, because if the number of serves per service game differs by surface that could explain any differences we observe in our measure of ace rate. For a simple if unrealistic example, say there are 10 serves per service game on clay versus 20 serves per service game on hard court and grass. Even if exactly 20% of serves are aces on each surface, we would observe an ace rate per service game of 2.0 for clay and 4.0 for hard court and grass. This difference would appear even though the chance of any given serve being an ace doesn’t vary by surface.\nUnfortunately our data does not have the number of serves so we will have to continue with this limitation.\n\n\nAce Rate by Surface\nWe can finally look into how aces vary across the three types of tennis surfaces. We should begin by investigating the distribution of ace rate in matches on each type of surface. A good place to start is with boxplots, which the code below will create for us:\n\n\n# Get average overall ace rate\nmean_acerate &lt;- mean(match2023_df$ace_rate, na.rm = TRUE)\n\n# Create boxplots with overall mean ace rate as red line\nggplot(match2023_df, aes(x = surface, y = ace_rate)) +\n  geom_boxplot() +\n  labs(title = \"Ace Rate by Surface Type\",\n       x = \"Surface Type\",\n       y = \"Ace Rate\",\n       caption = \"Red line is overall mean ace rate across all surfaces.\") +\n  geom_hline(yintercept = mean_acerate, linetype = \"dashed\", color = \"red\")+\n  theme_minimal()+\n  scale_y_continuous(limits = c(0, 0.75), breaks = seq(0, 0.75, by = 0.25))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s now calculate the three means and medians for each surface to further show the difference.\n\n\nmatch2023_df %&gt;%\n  group_by(surface) %&gt;%\n  summarise(\n    Mean_ace_Rate = mean(ace_rate, na.rm = TRUE),\n    Median_ace_Rate = median(ace_rate, na.rm = TRUE)\n  )",
    "crumbs": [
      "Home",
      "Tennis",
      "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface"
    ]
  },
  {
    "objectID": "tennis/Teaching_ANOVA_Through_Aces/index.html#ace-rate-vs.-surface-analysis",
    "href": "tennis/Teaching_ANOVA_Through_Aces/index.html#ace-rate-vs.-surface-analysis",
    "title": "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface",
    "section": "Ace Rate vs. Surface Analysis",
    "text": "Ace Rate vs. Surface Analysis\nTo more formally investigate the association between ace rate and surface, one might first think, “We have a continuous variable (ace rate) and a three-level categorical variable (Surface: Grass, Hard, Clay), so we should use Analysis of Variance (ANOVA)!”\nBut we will first show you to how to do the same thing using linear regression, and then demonstrate how this is identical to ANOVA.\n\nAnalyzing Association Using Linear Regression\nOften linear regression is first introduced using one continuous independent variable and a continuous outcome. Consider a simple model relating the number of aces to the number of service games in a match using a straight line:\n\\[Aces=β0 + β1*ServiceGames\\]\nThe blue line below illustrates the regression line from this equation:\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nHere, β1 is the slope of the line: the average increase in the number of aces for a 1-unit increase in ServiceGames. In our data, that is approximately 0.6 more aces for each additional 1 service game.\nβ0 is the “intercept”, or the average value of the outcome (aces) when the independent variables are zero (that is, for a match with ServiceGames = 0). This does not have a useful real-world interpretation, but it will be useful for our surface model below.\nSo, how can we include surface - a categorical independent variable - in the equation instead?\n\nDummy Variable for Surface\nThe answer is to use dummy variables. These require transforming a categorical variable with K levels (for surface, this is 3) into K-1 (in our case, 2) variables that each take on the value 0 or 1.\nWe will create two new variables: Hard and Clay. We define Hard as 1 if the match is on hard court, and 0 otherwise. Clay is defined similarly. Thus we have:\nWhen the surface is Hard, Hard = 1, Clay = 0\nWhen the surface is Clay, Hard = 0, Clay = 1\nWhen the surface is Grass, Hard = 0, Clay = 0\nNotice although we did not include an explicit variable for Grass, we did not need to since grass surfaces are defined as the absence of a hard court or clay surface. This group - the one defined by all dummy variables being 0 - is called the reference group. Why this works will become apparent soon.\nHere is the code for creating these variables.\n\n\nmatch2023_df$Hard &lt;- ifelse(match2023_df$surface == \"Hard\", 1, 0)\nmatch2023_df$Clay &lt;- ifelse(match2023_df$surface == \"Clay\", 1, 0)\nhead(match2023_df)\n\n\n\n\n\nInterpreting Dummy Variables in Linear Regression\nWe can write these two variables, Hard and Clay, in a linear regression equation like so:\n\\[AceRate=β0 + β1*Hard + β2*Clay\\]\nRemember what we learned from the simple linear regression \\[Aces=β0 + β1*ServiceGames\\]\nβ1 represented the average change in aces for a 1-unit increase in ServiceGames (that is, the slope of a straight line through a scatterplot of the data). β0 represented the average aces when ServiceGames = 0.\nDummy variables work somewhat similarly.\nInterpreting β0\nβ0 should be the average of our dependent variable - ace rate - when all the independent variables are 0. Look back above at our variable definitions. What sort of Surface does Hard = 0 and Clay = 0 correspond to? Then, ask yourself…\n\n\n\n\n\n\n\n\n\n\n\n\nWe could write this mathematically (ignoring our error term) as \\[AceRate(Grass)=β0 + β1*0 + β2*0 = β0\\]\nInterpreting β1\nTo figure out how we interpret β1, let’s start with a slightly different question: how can we get the average ace rate on a hard court surface? Let’s look at our equation:\n\\[AceRate=β0 + β1*Hard + β2*Clay\\]\nFor a hard court surface, \\[AceRate(Hard Court)=β0 + β1*1 + β2*0 \\] which simplifies to\n\\[AceRate(Hard Court)=β0 + β1 \\]\nThis helps us understand what β1 means: \\[AceRate(Hard Court) - AceRate(Grass) = (β0 + β1) - (β0) = β1\\]\nThus, β1 is the difference in average ace rate between grass and hard court. And that’s what we’re interested in!\nNote that using this equation, β1 &gt; 0 means hard court has a higher ace rate, while β1 &lt; 0 means hard court has a lower ace rate.\nInterpreting β2\nSee if you can apply this logic to interpret β2. HINT: Start by defining the average ace rate for a clay court match.\n\n\n\n\n\n\n\n\n\n\n\n\nDummy Variables Show Differences Between Groups\nIn fact, in general dummy variables can be interpreted as the average difference in our dependent variable between two groups: one defined by the group where the dummy variable is 1, and one defined by the reference group (where all dummy variables are 0).\nSo in our case: β1 = difference in ace rate between hard court and grass, and β2 = difference in ace rate between clay and grass\n\n\nLinear Regression Output\nLet’s first look back at our table of mean ace rates for each surface type:\n\n\n# A tibble: 3 × 2\n  surface Mean_ace_Rate\n  &lt;fct&gt;           &lt;dbl&gt;\n1 Grass           0.582\n2 Hard            0.534\n3 Clay            0.332\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow compare that to the output from a linear regression of \\[AceRate=β0 + β1*Hard + β2*Clay\\]\n\nace_surface_lm &lt;- lm(ace_rate ~ surface, match2023_df)\nsummary(ace_surface_lm)\n\n\nCall:\nlm(formula = ace_rate ~ surface, data = match2023_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.58197 -0.16577 -0.01947  0.13296  1.17462 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.58197    0.01257   46.31  &lt; 2e-16 ***\nsurfaceHard -0.04826    0.01441   -3.35 0.000823 ***\nsurfaceClay -0.24953    0.01476  -16.90  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2262 on 2205 degrees of freedom\nMultiple R-squared:  0.1771,    Adjusted R-squared:  0.1763 \nF-statistic: 237.2 on 2 and 2205 DF,  p-value: &lt; 2.2e-16\n\n\nLook at the Coefficients table, the Estimate column. These are our estimates of β0, β1, and β2. Can you figure out which is which? Maybe the following questions will help:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Association Using ANOVA\nLet’s now compare these results to a “traditional” approach to comparing means across 3 or more groups: ANOVA. Here is the code to run an ANOVA of ace rates by surface types in R:\n\n\nace_surface_anova &lt;- aov(ace_rate ~ surface, match2023_df)\nanova(ace_surface_anova)\n\n\n\nThe ANOVA table does not give us a lot of useful information. All it can tell us is whether the ace rates across the three surfaces are the same or different. This information is represented in the p-value for an F-test with F-statistic 237.2 and 2 and 2205 degrees of freedom. This p-value is under The Pr(&gt;F) column: &lt;2.2 x 10^-16. Because the p-value is &lt;0.05, this indicates a statistically significant difference between surfaces at that threshold.\nWithout more work it cannot tell us which surfaces are different from which others or how different they are. The linear regression, on the other hand, gave us all of that!\nLet’s do a little more work to also get p-values for pairwise comparisons between surfaces:\n\n\npairwise.t.test(match2023_df$ace_rate, match2023_df$surface, p.adj = \"none\")\n\n\n\nThis shows statistically significant differences at the 0.05 threshold for hard court versus grass (p = 0.00082), clay versus grass (p &lt; 2.2 x 10^-16), and clay versus hard court (p &lt; 2.2 x 10^-16).\n\n\nComparing Outputs: ANOVA is a Linear Regression Model!\nLet’s look again at our linear regression output:\n\nace_surface_lm &lt;- lm(ace_rate ~ surface, match2023_df)\nsummary(ace_surface_lm)\n\n\nCall:\nlm(formula = ace_rate ~ surface, data = match2023_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.58197 -0.16577 -0.01947  0.13296  1.17462 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.58197    0.01257   46.31  &lt; 2e-16 ***\nsurfaceHard -0.04826    0.01441   -3.35 0.000823 ***\nsurfaceClay -0.24953    0.01476  -16.90  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2262 on 2205 degrees of freedom\nMultiple R-squared:  0.1771,    Adjusted R-squared:  0.1763 \nF-statistic: 237.2 on 2 and 2205 DF,  p-value: &lt; 2.2e-16\n\n\nLook at the last line of the output: an F-statistic of 237.2 on 2 and 2205 degrees of freedom, with a p-value of &lt;2.2 x 10^-16. Exactly what we saw in our basic ANOVA!\nThen look under the Coefficients table at the surfaceHard and surfaceClay lines. These are our pairwise p-values comparing each of those groups to grass!\nThat is because the ANOVA and linear regression are mathematically equivalent.\nANOVA and linear regression are both linear models. They may seem different when considering conventional descriptions that emphasize ANOVA testing mean group differences across three or more groups and regression’s emphasis on estimating coefficients relating two or more variables. But they are equivalent.",
    "crumbs": [
      "Home",
      "Tennis",
      "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface"
    ]
  },
  {
    "objectID": "tennis/Teaching_ANOVA_Through_Aces/index.html#conclusion",
    "href": "tennis/Teaching_ANOVA_Through_Aces/index.html#conclusion",
    "title": "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface",
    "section": "Conclusion",
    "text": "Conclusion\n\nTennis Aces by Surface\n\n\n\n\n\n\n\n\n\n\n\n\nA potential explanation for these differences is the impact of surface type on ball speed. Factors such as increased ball speed, unpredictable trajectories, or enhanced spinning torque can all contribute to a higher ace rate. Generally, the ball tends to travel at its fastest on grass surfaces, followed by hard court surfaces, and notably slower on clay surfaces. This is the same order we saw for ace rates. As the ball’s speed decreases, the likelihood of a player serving the ball and the opponent failing to return it may be lower.\n\n\nStatistical Lesson\n\n\n\n\n\n\n\n\n\n\n\n\nANOVA is mathematically equivalent to a simple linear regression model with dummy variables for one categorical independent variable. But the linear regression gives you more information with less work, and it is a much more flexible method capable of handling any number of continuous and categorical independent variables. Why would you use ANOVA when you could use linear regression instead?\nIn fact, many statistical tests you learn in Stats 101 - t-tests, ANOVA, Wilcoxon signed-rank test, Pearson and Spearman correlation coefficients, Mann-Whitney U test, Kruskal-Wallis test, Chi Square tests, and more - are all just special cases of linear regression. We could simply teach you linear regression to accomplish the goals of all these different methods.\nTo delve deeper into the link between common statistical tests and linear regression models, we recommend this explanation from Jonas Lindeloev.",
    "crumbs": [
      "Home",
      "Tennis",
      "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface"
    ]
  },
  {
    "objectID": "tennis/Teaching_ANOVA_Through_Aces/index.html#extra-credit",
    "href": "tennis/Teaching_ANOVA_Through_Aces/index.html#extra-credit",
    "title": "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface",
    "section": "Extra Credit",
    "text": "Extra Credit\nAs an optional extra credit assignment and to test your skills, try repeating the above analysis with total_ServiceGames as your dependent variable: a.) use linear regression and ANOVA to investigate the association between surface and the length of matches (defined as the number of service games), and b.) compare the outputs to demonstrate that they are equivalent but that the linear regression depicts more information about the association.\n\nAuthors\nZachary O. Binney, PhD MPH and Heyi Yang\nEmory University",
    "crumbs": [
      "Home",
      "Tennis",
      "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface"
    ]
  },
  {
    "objectID": "motor_sports/nascar_regression_transformation/index.html",
    "href": "motor_sports/nascar_regression_transformation/index.html",
    "title": "NASCAR Transformation Module",
    "section": "",
    "text": "Introduction\nIn NASCAR, driver rating is a metric used to evaluate the performance of drivers in races. It provides a comprehensive assessment of a driver’s overall competitiveness, efficiency, and consistency during a race. The driver rating is based on several key performance factors and is designed to offer a more objective view of a driver’s abilities. For this activity, you will be exploring the relationship between average position a driver finishes per lap over a season and their corresponding driver rating. Using data transformations techniques and polynomial regression to create different variations of linear models, you will enhance the capabilities of your models to make them more effective and accurate.\n\n\n\n\n\n\nMore about NASCAR\n\n\n\n\n\nNASCAR, the National Association for Stock Car Auto Racing, is a preeminent motorsports organization in the United States, distinguished by its high-speed competitions and fervent fanbase. Established in 1948 by Bill France Sr., NASCAR has evolved into a leading racing series that encompasses three national divisions: the NASCAR Cup Series, the Xfinity Series, and the Truck Series. The Cup Series is the most prestigious, showcasing the elite drivers and teams as they compete in events that span a variety of track types, typically oval in shape, and ranging from short tracks (0.24 miles) to expansive superspeedways (over 3 miles).\nThe races take place on diverse tracks across the nation, including renowned venues such as Daytona International Speedway and Talladega Superspeedway, where vehicles frequently exceed speeds of 200 miles per hour. NASCAR races serve as a complex interplay of driver skill, team strategy, and vehicle performance. Key metrics such as lap times, pit stop efficiency, and vehicle dynamics offer insights into the determinants of success in the sport. This not only enhances the understanding of race outcomes but also contributes to the development of strategies that optimize performance.\n\n\n\n\n\n\n\n\n\nHow NASCAR uses data\n\n\n\n\n\nThe video linked below is a panel discussion from the 2023 Sloan Sports Analytics Conference entitled Start Your Engines: How Data is Fueling NASCAR’s Strategy to Engage an Evolving Customer Base.\nAlthough the panel discussion is not directly related to this module, Justin Marks discusses how data is used in NASCAR during the 26:53 to 30:44 minute interval.\n\n\n\n\n\n\n\n\n\n\n\nActivity Length\n\n\n\n\n\nThis activity would be suitable for an in-class activity lasting approximately one class period or as an out-of-class assignment.\n\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nBy the end of this activity, students will have reinforced the following skills:\n\nAssessing the effectiveness of simple linear regression models\nChecking regression model assumptions\nUsing log transformations to improve linear regression model fit\nApplying polynomial regression to model curved relationships\n\n\n\n\n\n\n\n\n\n\nMethods\n\n\n\n\n\nFor this activity, students will need to use software to create scatterplots and plots of residual vs fitted values of models they will create. They will also need to create polynomial models and mutate the data by applying mathematic functions to columns.\nIt is assumed that students are already exposed to these concepts as the activity is intended to reinforce the skills instead of introduce them.\n\n\n\n\n\nData\nThe data comes from the NASCAR website and shows the season statistics from 2007-2022. Each row displays the metrics of a racer for that specific year. The data frame contains 1111 rows of observations and 20 variables.\nDownload data: nascar_driver_statistics.csv\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nWins\nThe sum of the driver’s victories\n\n\nAvgStart\nThe sum of the driver’s starting positions divided by the number of races\n\n\nAvgMidRace\nThe sum of the driver’s mid race positions divided by the number of races\n\n\nAvgFinish\nThe sum of the driver’s finishing positions divided by the number of races\n\n\nAvgPos\nThe sum of the driver’s position each lap divided by the number of laps\n\n\nPassDiff\nThe sum of green flag passes minus green times passed\n\n\nGreenFlagPasses\nNumber of green flag passes performed by the driver\n\n\nGreenFlagPassed\nNumber of times driver is passed during green flag\n\n\nQualityPasses\nNumber of passes in the top 15 while under green flag conditions by driver\n\n\nPercentQualityPasses\nThe sum of quality passes divided by green flag passes\n\n\nNumFastestLaps\nNumber of where the driver had the fastest speed on the lap\n\n\nLapsInTop15\nNumber of laps completed while running in a top 15 position\n\n\nPercentLapsInTop15\nThe sum of the laps run in the top 15 divided by total laps completed\n\n\nLapsLed\nThe sum of the laps led in a race\n\n\nPercentLapsLed\nThe sum of the laps led in the race\n\n\nTotalLaps\nThe sum of the laps completed by a driver that year\n\n\nDriverRating\nFormula combining wins, finish, top15-finish, average running position while on lead lap, average speed under green, fastest lap, led most laps, and lead lap finish with a maximum rating of 150 points\n\n\n\n\n\n\nMaterials\nClass handout\nClass handout - with solutions: MS Word\nClass handout - with solutions: Quarto\n\n\n\n\n\n\nConclusion\n\n\n\n\n\nIn conclusion, the Transforming NASCAR Driver Data worksheet offers valuable insights into the relationship between average position and driver rating in NASCAR. Through the transformation of the average position variable, the worksheet enables students to enhance linearity in their models, thereby improving the accuracy of their predictions and analysis. The identification of curvature in the variable relationship also allows for the model using quadratic regression to be highly effective. The model can be conceptually compared to the model using a square root transformation to capture the same curve. Assessing the effectiveness of each model, students can critically evaluate the different approaches and determine the most suitable model for the data. This exercise equips students with essential data transformation and modeling skills, empowering them to make informed decisions and gain a deeper understanding of the factors influencing a driver’s performance.",
    "crumbs": [
      "Home",
      "Motor Sports",
      "NASCAR Transformation Module"
    ]
  },
  {
    "objectID": "marathons/boston-marathon-finish_times-2023/index.html",
    "href": "marathons/boston-marathon-finish_times-2023/index.html",
    "title": "2023 Boston Marathon - Variability in Finish Times",
    "section": "",
    "text": "Welcome video\n\n\n\n\nIntroduction\nFor this activity, you will be exploring the result times from female and male runners that finished the 2023 Boston Marathon.\nIn particular, you will examine both visualizations and summary statistics of result times to explore the variation in finish times as well as use comparative techniques, such as z-scores, to compare and contrast male and female participants.\nInvestigating these trends is useful for several reasons. Firstly, exploring these trends can help to deepen our understanding of how different factors, such as gender, impact marathon performances. Secondly, analyzing the distribution of finish times and the performance of top finishers against the masses provides insights into the competitive landscape of the marathon. It can identify outliers or exceptional performances and understand how elite athletes compare to average participants. Although not directly connected to this data, analyses like these can inform training strategies, highlight the effectiveness of different preparation methods, and inspire both new and experienced runners by showcasing the range of achievable performances.\n\n\n\n\n\n\nActivity Length\n\n\n\n\n\nThis activity would be suitable for an in-class example or quiz.\n\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nBy the end of the activity, you will be able to:\n\nAnalyze distributions using histograms\nIdentify potential confounding variables to explain bimodal data\nCompare and contrast distributions for a pair of groups\nCalculate and compare z-scores for individual cases\n\n\n\n\n\n\n\n\n\n\nMethods\n\n\n\n\n\nFor this activity, students will primarily use basic concepts of histograms and summary statistics to analyze distributions. Students will also likely require knowledge of z-scores.\n\n\n\n\n\n\n\n\n\nTechnology Requiremens\n\n\n\n\n\nThe provided worksheets do not require any specific statistical software. (Although they will likely require access to a calculator.)\nSince the data are provided, instructors are encouraged to modify the worksheets to have student construct visualizations and calculate summary statistics using whichever software they choose.\n\n\n\n\n\nData\nThe data set contains 26598 rows and 15 columns. Each row represents a runner who completed the Boston Marathon in 2023\nDownload data:\nAvailable on the SCORE Data Repository: boston_marathon_2023.csv\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nage_group\nage group of the runner\n\n\nplace_overall\nfinishing place of the runner out of all runners\n\n\nplace_gender\nfinishing place of runner among the same gender\n\n\nplace_division\nfinishing place of runner among runners of the same gender and age group\n\n\nname\nname of runner\n\n\ngender\ngender of runner\n\n\nteam\nteam the runner is affiliated with\n\n\nbib_number\nbib number of runner\n\n\nhalf_time\nhalf marathon time of runner\n\n\nfinish_net\nfinishing time timed from when they cross the starting gate\n\n\nfinish_gun\nfinishing time of runner timed from when the starter gun is fired\n\n\nhalf_time_sec\nhalf marathon time in seconds\n\n\nfinish_net_sec\nnet finish in seconds\n\n\nfinish_gun_sec\ngun finish in seconds\n\n\nfinish_net_minutes\nnet finish in minutes\n\n\n\nData Source\nBoston Athletic Association\n\n\n\nMaterials\n\nWe provide editable MS Word handouts along with their solutions.\n\nClass handout\n\n\nClass handout - with solutions\n\n\n\n\n\n\n\n\nConclusion\n\n\n\n\n\nIn conclusion, the Boston Marathon Times worksheet provides valuable learning opportunities for students in several key areas. It allows them to understand reasons by variability might exist and to discover multimodal distributions can occur simply due to excluding an important explanatory variable that otherwise confounds the analysis. The calculation of z-scores or other similar measurement of relative location enables students to compare and contrast the remarkable achievements of the top female and male finishers, shedding light on their talent in their respective fields. Overall, this worksheet allows students to critically analyze the 2023 marathon result data and draw meaningful conclusions about the extraordinary performances of athletes in the race.",
    "crumbs": [
      "Home",
      "Marathons",
      "2023 Boston Marathon - Variability in Finish Times"
    ]
  },
  {
    "objectID": "lacrosse/college_lacrosse_faceoffs/index.html",
    "href": "lacrosse/college_lacrosse_faceoffs/index.html",
    "title": "Lacrosse Faceoff Proportions",
    "section": "",
    "text": "Introduction\nIn this engaging activity, we explore the exciting sport of NCAA Division I Lacrosse, with a special focus on faceoff percentages—a critical aspect of the game. A faceoff occurs at the start of each quarter and after every goal, where two players compete to gain possession of the ball, setting the stage for their team’s offensive play. Winning a high percentage of faceoffs is often key to controlling the game and can significantly impact a team’s overall performance.\n\n\n\n\n\n\nVideo Demonstrating a Faceoff\n\n\n\n\n\n\n\n\n\nOur primary goal is to compare a specific team’s faceoff performance with overall league statistics for the 2022-2023 season. Through this exploration, we’ll introduce you to the concept of one-sample proportion hypothesis testing, a powerful statistical tool widely used in sports analytics. By the end of this exercise, you’ll gain a fundamental understanding of hypothesis testing and how it can be practically applied to evaluate team performance in lacrosse and beyond.\n\n\n\n\n\n\nActivity Length\n\n\n\n\n\nThis activity would be suitable for an in-class example (of approximately 10 - 20 minutes) or can be modified to be a quiz or part of an exam.\n\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\n\nComprehend the concept of one sample proportion hypothesis testing and its relevance in sports statistics.\nAnalyze and interpret dataset variables related to faceoff percentages in NCAA Division I Lacrosse.\nEvaluate a specific team’s faceoff performance by comparing it with league-wide statistics using hypothesis testing.\n\n\n\n\n\n\n\n\n\n\nMethods\n\n\n\n\n\nStudents are expected to have been exposed to the following concepts and use the activity to reinforce their understanding of these methods.\n\nBasic probability and percentages.\nNull and alternative hypotheses.\nSample size and sample proportion calculations.\nSuccess-failure condition for hypothesis testing.\nCalculation of test statistics (Z-score).\nUnderstanding significance levels (⍺) and p-values.\nDrawing conclusions and implications from hypothesis test results.\n\n\n\n\n\n\nData\nNote that because the activity only uses results from one team, students do not necessarily need to directly access this data. However, the activity can easily be adapted to use other teams. Instructors are encouraged to personalize the activity if they so choose.\nThe data set where the activities statistics come from contains 72 rows and 22 columns. Each row represents the season results for a lacrosse team at the NCAA Division 1 level from the 2022-2023 season.\nDownload data: lax_2022_2023.csv\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nTeam\ncollege of the team\n\n\navg_assists\naverage assists to goals per game\n\n\navg_caused_turnovers\naverage turnovers forced by the team per game\n\n\nclearing_pctg\npercentage of successful attempts to earn an offensive opportunity after gaining the ball in the teams own half\n\n\ntotal_faceoffs\ntotal faceoffs taken by a team for the season\n\n\nfaceoff_wins\ntotal faceoff wins by a team for the season\n\n\nfaceoff_win_pct\nproportion of total faceoff wins out of total faceoffs\n\n\navg_goals\naverage goals per game\n\n\navg_goals_allowed\naverage goals allowed by the team per game\n\n\navg_ground_balls\naverage loose balls picked up by the team per game\n\n\nman_down_defense_pctg\nproportion of times a team stops the opponent from scoring while man down due to a penalty\n\n\nman_up_offense_pctg\nproportion of times the offense scores out of total opportunities while man up\n\n\navg_scoring_margin\naverage margin of goals per game\n\n\nopp_clear_pctg\nopponents clearing percentage averaged by game\n\n\navg_points\naverage team points per game\n\n\navg_saves\naverage saves per game\n\n\nshot_pctg\nproportion of shots that go in out of total shots\n\n\navg_turnovers\naverage turnovers that are directly the fault of a player per game\n\n\nW\ntotal wins by the team\n\n\nL\ntotal losses by the team\n\n\nwin_loss_pctg\nproportion of games won out of total games\n\n\n\nData Source\nThe data were collected from the NCAA Website for Men’s Lacrosse Division I\nhttp://stats.ncaa.org/rankings/change_sport_year_div\nInstructors interested in updating the data to a newer season can do so via the following\n\nGo to http://stats.ncaa.org/rankings/change_sport_year_div\nSelect Men’s Lacrosse, season of choice, Division I, Final Statistics\nIn the “Teams”, download each of the data tables.\nRead in each file, join the tables, and do some light cleaning. The code below shows an example used for the 2022-2023 season.\n\n\n\nShow the code\nlibrary(tidyverse)\n\n\n# reading\n# the files listed here are what\n# you will download from the site\n\nassists&lt;- read_csv(\"assists_l.csv\", col_select = 1:2)\ncaused_turnovers&lt;- read_csv(\"caused_turnovers_l.csv\", col_select = 1:2)\nclearing&lt;- read_csv(\"clearing_pctg_l.csv\", col_select = 1:2)\nfo &lt;- read_csv(\"fo_win_pctg.csv\", col_select = 1:4)\ngoals_against&lt;- read_csv(\"goals_against.csv\", col_select = 1:2)\ngoals&lt;- read_csv(\"goals_l.csv\", col_select = 1:2)\ngroundballs&lt;- read_csv(\"ground_balls_l.csv\", col_select = 1:2)\nman_down &lt;- read_csv(\"man_down_defense_l.csv\", col_select = 1:2)\nman_up &lt;- read_csv(\"man_up__offense_l.csv\", col_select = 1:2)\nmargin &lt;- read_csv(\"margin_l.csv\", col_select = 1:2)\nopp_clear &lt;- read_csv(\"opp_clear_l.csv\", col_select = 1:2)\npoints &lt;- read_csv(\"points_l.csv\", col_select = 1:2)\nsaves &lt;- read_csv(\"saves_l.csv\", col_select = 1:2)\nshot &lt;- read_csv(\"shot_pctg_l.csv\", col_select = 1:2)\nturnovers&lt;- read_csv(\"turnovers_l.csv\", col_select = 1:2)\nshots_per_game &lt;- read_csv(\"shots_per_game.csv\", col_select = 1:3)\nwin_loss &lt;- read_csv(\"win_loss_l.csv\")\n\n# joining\n# students familiar with the purrr package could\n# use the reduce function to reduce the amount of code\n\nlax_2022_2023 &lt;- \n  left_join(assists, caused_turnovers, by = \"Team\") %&gt;%\n  left_join(clearing, by = \"Team\") %&gt;%\n  left_join(fo, by = \"Team\")  %&gt;%\n  left_join(goals, by = \"Team\")  %&gt;%\n  left_join(goals_against, by = \"Team\")  %&gt;%\n  left_join(groundballs, by = \"Team\") %&gt;%\n  left_join(man_down, by = \"Team\") %&gt;%\n  left_join(man_up, by = \"Team\") %&gt;%\n  left_join(margin, by = \"Team\") %&gt;%\n  left_join(opp_clear, by = \"Team\") %&gt;%\n  left_join(points, by = \"Team\") %&gt;%\n  left_join(saves, by = \"Team\") %&gt;%\n  left_join(shot, by = \"Team\") %&gt;%\n  left_join(turnovers, by = \"Team\") %&gt;%\n  left_join(shots_per_game, by = \"Team\") %&gt;%\n  left_join(win_loss, by = \"Team\")\n\n# cleaning\nlax_2022_2023 &lt;- lax_2022_2023 %&gt;%\n  separate(Team, into = c(\"Team\",\"Conference\"), sep = \"\\\\(\", extra = \"merge\")%&gt;%\n  mutate(Conference = str_remove_all(Conference,\"\\\\)\"),\n         Team = str_trim(Team))%&gt;%\n  mutate(shots_per_game = Shots/Games)%&gt;%\n  select(-20, -21)\n\n# saving\nwrite_csv(x = lax_2022_2023, file = \"lax_2022_2023.csv\")\n\n\n\n\n\nMaterials\nClass handout\nClass handout - with solutions\n\n\n\n\n\n\nConclusion\n\n\n\n\n\nIn this insightful exploration of NCAA Division I Lacrosse faceoff percentages, we have embarked on a statistical journey to evaluate a specific team’s performance in comparison to league-wide statistics. Through the application of one sample proportion hypothesis testing, we gained valuable insights into the team’s faceoff win percentage, unveiling strong evidence that their performance exceeded what we would expect by random chance alone. As we consider the broader implications of faceoffs in Division I Lacrosse, it becomes evident that faceoff wins play a pivotal role in team rankings and outcomes. The fact that Duke, the second-best team in the country, exhibited a faceoff win percentage above the league average highlights the significance of excelling in this aspect of the game. Winning faceoffs likely translates to higher goal-scoring opportunities, ultimately leading to more successful game outcomes.",
    "crumbs": [
      "Home",
      "Lacrosse",
      "Lacrosse Faceoff Proportions"
    ]
  },
  {
    "objectID": "esports/league-of-legends-buffing-nerfing/index.html",
    "href": "esports/league-of-legends-buffing-nerfing/index.html",
    "title": "League of Legends - Buffing and Nerfing",
    "section": "",
    "text": "Welcome video\n\n\n\n\nIntroduction\nLeague of Legends (LoL) is a 5 v. 5 multiplayer online battle arena (MOBA) game developed by Riot Games. In this game, players assume the role of a “champion” with unique abilities and engage in intense battles against a team of other players or computer-controlled champions. Riot Games continually collects data to evaluate the impact of each champion, adjusting and fine-tuning various aspects to ensure fair and competitive gameplay. With regular updates (patches) occurring every two weeks, champions can become either extremely efficient and strong or in need of adjustments to enhance their abilities. Maintaining overall game balance is crucial, and developers employ strategies known as “nerfing” and “buffing” to achieve this balance. “Nerfing” refers to reducing the power or effectiveness of a champion or item, while “buffing” involves increasing its power or effectiveness.\nIn this worksheet, we will analyze and describe histograms of Win Rates for different champions in LoL. The Win Rate, a key metric in the game, represents the percentage of games won by a champion out of the total games played. Understanding the distribution of Win Rates and identifying potential outliers can provide valuable insights into champion balance and performance, informing strategic decision-making in LoL gameplay.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nBy the end of this activity, you will be able to:\n\nUnderstand the concept of histograms and their relevance in statistical analysis.\nAnalyze and describe histograms to gain insights into the distribution of Win Rates in League of Legends. In particular, being able to describe the center, shape, and spread of a distribution based on the displayed graph.\nIdentify potential outliers in a numerical variable using numerical methods such as the “1.5 IQR Rule” or z-scores.\nInterpret the implications of outliers in terms of champion balance and performance.\n\n\n\n\n\n\n\n\n\n\nMethods\n\n\n\n\n\nTechnology requirement: The activity handout provides histograms and summary statistics so that no statistical software is required. However, the activity could be modified to ask students to produce that information from the raw dataset and/or extend the activity to investigate other variables available in the data.\n\nHistograms: Familiarity with histograms as a graphical representation of the distribution of a continuous variable, such as Win Rates, is crucial. You should understand how to interpret histograms, including the concepts of bins, frequencies, and the shape, center, and spread of distributions.\nOutliers: Knowledge of outliers, which are data points that deviate significantly from the overall pattern, is important.\nFamiliarity with basic statistical analysis techniques, such as measures of central tendency (mean, median) and measures of dispersion (standard deviation, range), will aid in interpreting and analyzing the histograms. These techniques can provide insights into the overall characteristics and variability of the Win Rates.\nKnowledge of outlier detection methods (such as the 1.5 IQR Rule and/or z-scores) is fundamental to the activity.\n\n\n\n\n\n\nData\nA data frame for 162 champions of the following 7 variables. Each row represents a Champion that you can choose when playing League of Legends during patches 12.22 and 12.23. Note that the activity provided in this module does not use all of the variables provided. Instead they are provided for further analyses at the discretion of the user.\nAvailable on the SCORE Data Repository\nDownload data: LOL_patch_12.22.csv\nDownload data: LOL_patch_12.23.csv\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nName\nname of the champion\n\n\nRole\nrole of the champion in a game\n\n\nKDA\nAverage kills, deaths and assists associated with each champion\n\n\nWRate\nwin rates of each champion\n\n\nPickRate\npick rates of each champion\n\n\nRolePerc\npercentage of time playing as a role\n\n\nBanPerc\nban percentages associated with each champion\n\n\n\n\n\nData Source\nLol champion stats, 12.22 master, win rates. METAsrc. (n.d.). https://www.metasrc.com/5v5/12.22/stats?ranks=master\nLol champion stats, 12.23 master, win rates. METAsrc. (n.d.-b). https://www.metasrc.com/5v5/12.23/stats?ranks=master\n\n\n\nMaterials\nClass handout\nClass handout - with solutions\n\n\n\n\n\n\nConclusion\n\n\n\n\n\nIn conclusion, the analysis of Win Rates histograms in League of Legends has provided valuable insights into champion balance and performance. One notable finding from this worksheet is the identification of Sion as a low outlier in the Win Rates for the 12.22 patch. However, in the subsequent 12.23 patch, Sion’s Win Rate improved, and was no longer an outlier. Sion was given a Buff in patch 12.23, resulting in an enhanced performance and a more balanced Win Rate. However, patch 12.23 resulted in two new outliers with low win rates.\nThe continued presence of outliers highlights the importance of continuous monitoring and adjustments by game developers to ensure fair and competitive gameplay.",
    "crumbs": [
      "Home",
      "Esports",
      "League of Legends - Buffing and Nerfing"
    ]
  },
  {
    "objectID": "baseball/stolen-bases/index.html",
    "href": "baseball/stolen-bases/index.html",
    "title": "Stolen Bases",
    "section": "",
    "text": "This lesson introduces students to the concept of normality tests (Shapiro-Wilks and Kolmogorov-Smirnov) and summation of normal distributions to investigate stolen base success rates. Featuring Jacob Hurtubise, a West Point’s all-time leader in stolen bases and baseball player for the Cincinnati Reds’ Double-A affiliate, the Chattanooga Lookouts.",
    "crumbs": [
      "Home",
      "Baseball",
      "Stolen Bases"
    ]
  },
  {
    "objectID": "baseball/stolen-bases/index.html#motivation",
    "href": "baseball/stolen-bases/index.html#motivation",
    "title": "Stolen Bases",
    "section": "",
    "text": "This lesson introduces students to the concept of normality tests (Shapiro-Wilks and Kolmogorov-Smirnov) and summation of normal distributions to investigate stolen base success rates. Featuring Jacob Hurtubise, a West Point’s all-time leader in stolen bases and baseball player for the Cincinnati Reds’ Double-A affiliate, the Chattanooga Lookouts.",
    "crumbs": [
      "Home",
      "Baseball",
      "Stolen Bases"
    ]
  },
  {
    "objectID": "baseball/stolen-bases/index.html#module",
    "href": "baseball/stolen-bases/index.html#module",
    "title": "Stolen Bases",
    "section": "Module",
    "text": "Module\nhttps://isle.stat.cmu.edu/SCORE/stolen-bases-module/",
    "crumbs": [
      "Home",
      "Baseball",
      "Stolen Bases"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SCORE Module Repository",
    "section": "",
    "text": "The SCORE Network Module Repository enables you to search for modules by either sport (along the left), or you can browse by statistics and data science topic. The modules listed in this repository have completed the required SCORE Network pedagogical and industry peer reviews to become a published module.\nInterested in submitting your own module? Click here to find out more information about the SCORE Network module submission process.\nYou can also access preprint modules from various SCORE Network affiliates below (note that these materials have not yet completed the SCORE Network review process):\n\nCarnegie Mellon University\nSt. Lawrence University\nBaylor University + Azusa Pacific University\nWest Point\n\nThe development of the SCORE with Data network is funded by the National Science Foundation (award 2142705)."
  },
  {
    "objectID": "baseball/mlb_injuries/index.html",
    "href": "baseball/mlb_injuries/index.html",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "",
    "text": "Facilitation notes\n\n\n\n\n\n\nThis module would be suitable for an in-class lab or take-home assignment in an intermediate statistics course.\nIt assumes a familiarity with the RStudio Environment and R programming language.\nStudents should be provided with the following data file (.csv) and Quarto document (.qmd) to produce visualizations and write up their answers to each exercise. Their final deliverable is to turn in an .html document produced by “Rendering” the .qmd.\n\nMonthly Injury Data\nTommy John Surgeries Data\nStudent Quarto template\n\nPosit Cloud (via an Instructor account) or Github classroom are good options for disseminating files to students, but simply uploading files to your university’s course management system works, too.\nThe data for the mlb_injuries_monthly.csv file was derived from data found on prosportstransactions.com. The original data from the site contained rows of observations showing each transaction that involved sending a player to the injured list or bringing a player off of the injured list. The data was then filtered to include only times when a player was sent to the injured list and aggregated by month. The 2000 season was originally included in the data, but was removed after incomplete data was found for that season.\nThe data for the tj_surgeries_mlb_milb.csv file was derived from data accumulated by @MLBPlayerAnalys over many years. The original data lists any reported Tommy John surgery for a player now playing in the MLB or MiLB. This data was aggregated by year to produce the data used in this module.",
    "crumbs": [
      "Home",
      "Baseball",
      "MLB Injuries - Introductory Time Series Analysis"
    ]
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#classical-decomposition",
    "href": "baseball/mlb_injuries/index.html#classical-decomposition",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "Classical Decomposition",
    "text": "Classical Decomposition\nThere are two common methods for decomposing time series data: classical decomposition and STL decomposition. In this module we will focus on classical decomposition.\nClassical decomposition (additive) has four main steps:\n\nComputing a trend-cycle component (\\(T_t\\)) using a moving average. A moving average is a technique for smoothing time series data by averaging the values of neighboring points. This helps to remove short-term fluctuations and highlight longer-term trends.\nComputing a series without the trend-cycle component (\\(y_t - T_t\\)).\nEstimating the seasonal component (\\(S_t\\)) by averaging the values from the detrended series for the season.\nComputing the remainder component (\\(R_t\\)) by subtracting the trend-cycle and seasonal components from the original series. \\(R_t = y_t - T_t - S_t\\)\n\nWe can use the classical_decomposition function inside of the model() function to decompose our time series data.\n\ninjuries |&gt; \n  model(classical_decomposition(Count)) |&gt;\n  components() |&gt;\n  autoplot() +\n  labs(title = \"Classical Additive Decomposition of Monthly MLB Injury Counts\")\n\n\n\n\n\n\n\n\nClassical multiplicative decomposition works similarly to the additive decomposition, but with a few key differences.\n\nThe detrended series is computed as \\(y_t / T_t\\) instead of \\(y_t - T_t\\).\nThe seasonal component is estimated as \\(S_t = y_t / T_t\\) instead of \\(y_t - T_t\\).\nThe remainder component is computed as \\(R_t = y_t / (T_t \\times S_t)\\) instead of \\(y_t - T_t - S_t\\).\n\n\n\nNOTE: Ideally, after doing a decomposition, the remainder component should be white noise.\nClassical multiplicative decomposition can be used by setting the type argument to \"multiplicative\" in the classical_decomposition() function.",
    "crumbs": [
      "Home",
      "Baseball",
      "MLB Injuries - Introductory Time Series Analysis"
    ]
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#stl-decomposition",
    "href": "baseball/mlb_injuries/index.html#stl-decomposition",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "STL Decomposition",
    "text": "STL Decomposition\nSeasonal and Trend decomposition using Loess (STL) is a more advanced method for decomposing time series data. It is more flexible than classical decomposition and can handle any type of seasonality, not just monthly or quarterly. It also allows the user to control the length of the smoothing window for the trend-cycle. Lastly, it is more robust to outliers so that they do not affect the trend and seasonal estimates as much.\nBelow is an example of how to use the STL() function to decompose the time series data.\n\ninjuries |&gt; \n  model(STL(Count ~ trend(window = 21)+ \n            season(window = \"periodic\"),\n            robust = TRUE)) |&gt;\n  components() |&gt;\n  autoplot() +\n  labs(title = \"STL Additive Decomposition of Monthly MLB Injury Counts\")\n\n\n\n\n\n\n\n\n\n\nTIP: The window argument in the trend() function controls the length of the smoothing window for the trend-cycle. The larger the window, the smoother the trend. This value should always be an odd number so that a central point can be used. trend(window = 21) is a common choice for monthly data. This relatively large window size helps to prevent the trend from being influenced by short-term fluctuations in just a single year, but rather capture long-term trends and cycles.\nTIP: The window argument in the season() function controls how many years the seasonal component should be estimated over. The default value is 11. When the seasonal window is set to periodic season(window = \"periodic\"), it is the equivalent setting the window to all of the data. When periodic is used, the seasonal component is assumed to be the same each year. The seasonal window argument should always be an odd number or “periodic”.\n\n\n\n\n\n\nExercise 3: Changing Decomposition Types\n\n\n\n\n\nCreate a classical multiplicative decomposition of the monthly MLB injury counts. How do the components differ from the additive decomposition? Which decomposition method’s remainder component looks more like white noise (classical additive or classical multiplicative)?\nCreate an STL decomposition of the monthly MLB injury counts with a shorter length for the trend smoothing window. How does the decomposition change with a shorter trend smoothing window? Particularly, how does the trend component change?\n\n\n\n\nNOTE: You can actually forecast with decomposition as well. If you’d like to learn more about this click here",
    "crumbs": [
      "Home",
      "Baseball",
      "MLB Injuries - Introductory Time Series Analysis"
    ]
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#the-mean-method",
    "href": "baseball/mlb_injuries/index.html#the-mean-method",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "The Mean Method",
    "text": "The Mean Method\nAn extremely simple method for forecasting is the mean method. This method forecasts the next observation as the average of all the observations in the training data. This method will produce a flat forecast that is equal to the mean of the training data. The mean method is useful when the data doesn’t have a trend or seasonality.",
    "crumbs": [
      "Home",
      "Baseball",
      "MLB Injuries - Introductory Time Series Analysis"
    ]
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#the-naive-method",
    "href": "baseball/mlb_injuries/index.html#the-naive-method",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "The Naive Method",
    "text": "The Naive Method\nThe naive method is another simple forecasting method. It forecasts the next observation as the value of the last observation in the training data. This method will produce a flat forecast that is equal to the last observation in the training data. The naive method is useful when the data appears to be random.",
    "crumbs": [
      "Home",
      "Baseball",
      "MLB Injuries - Introductory Time Series Analysis"
    ]
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#seasonal-naive-method",
    "href": "baseball/mlb_injuries/index.html#seasonal-naive-method",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "Seasonal Naive Method",
    "text": "Seasonal Naive Method\nThe seasonal naive method is similar to the naive method, but it forecasts the next observation as the value from the same season in the previous year. This method will produce a repeating pattern of forecasts that are equal to the observations from the same season in the previous year. (Basically forever repeating the last year’s pattern). The seasonal naive method is useful when the data has a strong seasonal pattern but no trend.",
    "crumbs": [
      "Home",
      "Baseball",
      "MLB Injuries - Introductory Time Series Analysis"
    ]
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#drift-method",
    "href": "baseball/mlb_injuries/index.html#drift-method",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "Drift Method",
    "text": "Drift Method\nThe drift method is a simple forecasting method that assumes a linear trend in the data. It forecasts the next observation as the value of the last observation plus the average change between observations. This method will produce a forecast that will continue on a linear trend from the first observation and through the last observation in the training data. The drift method is useful when the data has a linear trend but no seasonality.\n\n\n\n\n\n\nExercise 4: Basic Forecasting Methods\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich of the above time series plots would be best forecasted using the mean method?\nWhich of the above time series plots would be best forecasted using the seasonal naive method?\nWhich of the above time series plots would be best forecasted using the drift method?\n\nUse the following plots of the monthly MLB injury counts and forecasts to answer the following questions.\n\n\n\n\n\n\n\n\n\n\nAfter looking at the forecasts from the mean, naive, seasonal naive, and drift methods for the MLB injury data, which method appears to be the best forecast?\nWhich ones appear to be the worst forecasts?\nWhat is one major issue seen in all of the forecasts in regards to their prediction intervals?",
    "crumbs": [
      "Home",
      "Baseball",
      "MLB Injuries - Introductory Time Series Analysis"
    ]
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#residuals",
    "href": "baseball/mlb_injuries/index.html#residuals",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "Residuals",
    "text": "Residuals\nChecking the residuals of a model is one of the most effective ways to see how well the model is performing. Residuals are the differences between the observed values and the values predicted by the model. \\(e_t = y_t - \\hat{y}_t\\)\n\n\nIt is important to note that when we are talking about residuals in this module that we are referring to the innovation residuals. Most of the time innovation residuals are the same as regular residuals, such as with our seasonal naive model. Innovation residuals are the residuals that are left over after accounting for changes made to the data such as transformations or differencing. These residuals are the ones that are used to check the model assumptions and to evaluate the model’s performance.\nThere are 3 main things to look at when evaluating residuals:\n\nDo the residuals appear to be white noise? Remember that white noise has a mean of 0, constant variance, and shows no obvious patterns. This can be checked by looking at a time plot of the residuals.\nAre the residuals normally distributed? This can be checked by looking at a histogram of the residuals or by using a normal probability plot.\nAre the residuals uncorrelated? This can be checked by looking at the ACF plot of the residuals. There are also statistical tests that can be used to check for autocorrelation in the residuals such as the Ljung-Box test. We can use the Box.test() function in R to perform this test.\n\n\n\nThe formula for the test statistic for a Ljung-Box test is:\n\\[Q^{*} = T(T+2) \\sum_{k=1}^{l} \\frac{r^2_k}{T-k}\\]\nwhere:\n\n\\(T\\) is the number of observations\n\\(l\\) is the max number of lags\n\\(r_k\\) is the sample autocorrelation at lag \\(k\\)\n\nThe null hypothesis for the Ljung-Box test is that the data is not distinguishable from white noise. If the p-value is less than the significance level, we reject the null hypothesis and conclude that the data is autocorrelated.\nThankfully there is a very easy way to check all of these at once in R using the gg_tsresiduals() function.\n\n\n\n\n\n\nExercise 5: Residuals Analysis\n\n\n\nBelow is code that will create a time plot, histogram, and ACF plot of the residuals from the seasonal naive method.\n\nsnaive_mod &lt;- injuries |&gt;\n  model(SNAIVE(Count ~ lag('year')))\n  \nsnaive_mod |&gt; \n  gg_tsresiduals()\n\n\n\n\n\n\n\n\n\nDo the residuals appear to be white noise?\nWhat stands out about the time plot of the residuals?\nDoes the histogram of the residuals appear to be normally distributed?\nAre the residuals uncorrelated? What lag(s) show the most significant autocorrelation and what could this mean for the model?",
    "crumbs": [
      "Home",
      "Baseball",
      "MLB Injuries - Introductory Time Series Analysis"
    ]
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#testing-and-training-for-point-estimate-evaluations",
    "href": "baseball/mlb_injuries/index.html#testing-and-training-for-point-estimate-evaluations",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "Testing and Training for Point Estimate Evaluations",
    "text": "Testing and Training for Point Estimate Evaluations\nIf you want to evaluate the point estimates of a model, you can use a testing and training split. This involves training the model on the first part of the data and then testing the model on the second part of the data. This allows you to see how well the model can forecast future observations.\nFor this example, we will split the data into a training set that contains the first 75% of the data and a testing set that contains the last 25% of the data. This means we will train the models on the data from January 2001 to December 2017 and test the models on the data from January 2018 to December 2023.\n\n\nTIP: 75-25 is a common split for training and testing data, but you can use any split that makes sense for your data. Generally the more data you have, the less percentage you need for testing. Other common splits are 70-30 or 80-20.\n\ntraining &lt;- injuries |&gt; filter(year(Month) &lt; 2018)\ntesting &lt;- injuries |&gt; filter(year(Month) &gt;= 2018)\n\nNow that we have our training and testing data, we can fit the models to the training data and then forecast the testing data.\n\ninjury_fit &lt;- training |&gt; \n  model(mean = MEAN(Count),\n        naive = NAIVE(Count),\n        snaive = SNAIVE(Count ~ lag('year')),\n        drift = RW(Count ~ drift()))\n\ninjury_forecasts &lt;- injury_fit |&gt; \n  forecast(new_data = testing)\n\n\n\nTIP: You can fit multiple models at once by using the model() function as seen in the code to the left. The values to the left of the = are the names we are giving to the models and the values to the right of the = are the models we are fitting to the data.\nTIP: When using the forecast() function, you can specify the new data you want to forecast by using the new_data argument. In this case, we are forecasting for the testing data.\nLet’s visualize the forecasts from the training data and compare them to the testing data.\n\ninjury_forecasts |&gt;\n  autoplot(injuries, level = NULL) +\n  labs(title = \"Forecasting Methods for Monthly MLB Injury Counts\")+\n  guides(color = guide_legend(title = \"Forecast\"))\n\n\n\n\n\n\n\n\nThe seasonal naive method certainly appears to be the best forecast.\nWe can also evaluate these models using the accuracy() function. This function calculates a variety of accuracy measures for the forecasts, including the mean absolute error, root mean squared error, mean absolute percentage error, and more.\n\nThe mean absolute error (MAE) is the average of the absolute errors between the forecasts and the actual values. It is a measure of the average magnitude of the errors in the forecasts. We want this value to be as close to 0 as possible.\n\n\nThe formula for the mean absolute error is: \\[\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} | y_i - \\hat{y}_i |\\] where \\(y_i\\) is the actual value and \\(\\hat{y}_i\\) is the forecasted value.\n\nThe root mean squared error (RMSE) is the square root of the average of the squared errors between the forecasts and the actual values. It is a measure of the standard deviation of the errors in the forecasts. We want this value to be as close to 0 as possible.\n\n\nThe formula for the root mean squared error is: \\[\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\\] Where \\(y_i\\) is the actual value and \\(\\hat{y}_i\\) is the forecasted value.\n\nThe mean absolute percentage error (MAPE) is the average of the absolute percentage errors between the forecasts and the actual values. It is a measure of the accuracy of the forecasts. We want this value to be as close to 0 as possible.\n\n\nThe formula for the mean absolute percentage error is: \\[\\text{MAPE} = \\frac{100}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right|\\] where \\(y_i\\) is the actual value and \\(\\hat{y}_i\\) is the forecasted value.\n\nThe code below displays the accuracy measures for the forecasts.\n\naccuracy(injury_forecasts, testing)\n\n# A tibble: 4 × 10\n  .model .type    ME  RMSE   MAE   MPE  MAPE  MASE RMSSE  ACF1\n  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 drift  Test   86.0 122.   86.0   Inf   Inf   NaN   NaN 0.698\n2 mean   Test   36.2  93.9  79.2  -Inf   Inf   NaN   NaN 0.698\n3 naive  Test   86.0 122.   86.0   100   100   NaN   NaN 0.698\n4 snaive Test   14.3  58.4  34.5  -Inf   Inf   NaN   NaN 0.424\n\n\nThis confirms that the seasonal naive method is the best forecast, as it has the lowest MAE and RMSE values. The MAPE is shown at -Inf to Inf because some of the actual values are 0, which causes the percentage error to be infinite.",
    "crumbs": [
      "Home",
      "Baseball",
      "MLB Injuries - Introductory Time Series Analysis"
    ]
  },
  {
    "objectID": "baseball/unbreakable-records/index.html",
    "href": "baseball/unbreakable-records/index.html",
    "title": "Unbreakable Records in Baseball",
    "section": "",
    "text": "This lesson introduces students to the Bernoulli trial and Binomial Experiments to understand the probability of breaking one of the longest lasting records in baseball. We also explain how to execute a Chi-Square test using baseball data on handedness (right or left-handed) of batters versus pitchers.",
    "crumbs": [
      "Home",
      "Baseball",
      "Unbreakable Records in Baseball"
    ]
  },
  {
    "objectID": "baseball/unbreakable-records/index.html#motivation",
    "href": "baseball/unbreakable-records/index.html#motivation",
    "title": "Unbreakable Records in Baseball",
    "section": "",
    "text": "This lesson introduces students to the Bernoulli trial and Binomial Experiments to understand the probability of breaking one of the longest lasting records in baseball. We also explain how to execute a Chi-Square test using baseball data on handedness (right or left-handed) of batters versus pitchers.",
    "crumbs": [
      "Home",
      "Baseball",
      "Unbreakable Records in Baseball"
    ]
  },
  {
    "objectID": "baseball/unbreakable-records/index.html#module",
    "href": "baseball/unbreakable-records/index.html#module",
    "title": "Unbreakable Records in Baseball",
    "section": "Module",
    "text": "Module\nhttps://isle.stat.cmu.edu/SCORE/Unbreakable_Records_Baseball_Hits/",
    "crumbs": [
      "Home",
      "Baseball",
      "Unbreakable Records in Baseball"
    ]
  },
  {
    "objectID": "hockey/nhl-shooting-percentage-ventura/index.html",
    "href": "hockey/nhl-shooting-percentage-ventura/index.html",
    "title": "Predicting NHL Shooting Percentages",
    "section": "",
    "text": "https://isle.stat.cmu.edu/SCORE/NHLShots/",
    "crumbs": [
      "Home",
      "Hockey",
      "Predicting NHL Shooting Percentages"
    ]
  },
  {
    "objectID": "hockey/nhl-shooting-percentage-ventura/index.html#module",
    "href": "hockey/nhl-shooting-percentage-ventura/index.html#module",
    "title": "Predicting NHL Shooting Percentages",
    "section": "",
    "text": "https://isle.stat.cmu.edu/SCORE/NHLShots/",
    "crumbs": [
      "Home",
      "Hockey",
      "Predicting NHL Shooting Percentages"
    ]
  },
  {
    "objectID": "lacrosse/lacrosse_pll_vs_nll/index.html",
    "href": "lacrosse/lacrosse_pll_vs_nll/index.html",
    "title": "Lacrosse PLL vs. NLL",
    "section": "",
    "text": "Please note that these material have not yet completed the required pedagogical and industry peer-reviews to become a published module on the SCORE Network. However, instructors are still welcome to use these materials if they are so inclined.\n\n\nThis module examines the goals and shots in two prominent lacrosse leagues: the Premier Lacrosse League (PLL) and the National Lacrosse League (NLL). The PLL and NLL are highly regarded professional lacrosse leagues that feature top-tier athletes from around the world.\nThe PLL is played in an outdoor setting, following the field lacrosse format. This style of lacrosse is characterized by its larger field size, typically 110 yards by 60 yards. Field lacrosse involves 10 players per team and promotes a style of play that emphasizes long passes, intricate plays, and individual skills. Founded in 2019, the PLL operates with a touring model where teams travel to different cities each weekend, bringing the sport to a wide audience and fostering a festival-like atmosphere at each event. The league’s modernized approach includes a strong emphasis on media presence and player engagement. The PLL features approximately 200 athletes across its teams.\nIn contrast, the NLL follows the box lacrosse format, which is played indoors on a smaller, enclosed field, generally 200 feet by 85 feet. Box lacrosse involves 6 players per team, and the gameplay is marked by frequent physical interactions, quick ball movements, and high-intensity transitions. Established in 1986, the NLL has a traditional franchise model with teams based in specific cities across the United States and Canada. This structure has cultivated strong local fan bases and deep community ties, contributing to the league’s longevity. The NLL features approximately 500 athletes, many of which also play in the PLL.\nThese data, from the 2021-2022 seasons, allow for an analysis of goal-scoring within these leagues to identify differences between indoor (NLL) and outdoor (PLL) play. By examining goals and shots, we aim to understand how the environment and format of the game influence offensive strategies and overall scoring trends in professional lacrosse. It’s important to recognize that outdoor field lacrosse and indoor box lacrosse are distinct sports, each with its own unique dynamics, rules, and playing styles. By acknowledging these nuances, we can may gain a deeper understanding of how various playing conditions and league structures impact statistical outcomes.\n\n\n\n\n\n\nActivity Length\n\n\n\n\n\nThis activity would be suitable for an in-class example or can be modified to be a quiz or part of an exam.\n\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nThe learning goals associated with this module are:\n\nStudents will be able to test for a difference in means between two groups.\nStudents will be able to find a confidence interval for a difference in means between two groups.\n\n\n\n\n\n\n\n\n\n\nMethods\n\n\n\n\n\nThis module requires students use a two-sample test and confidence interval (e.g., t-test or randomization) to compare the means of two groups.\nStudents are expected to have access any equations and/or lecture notes to complete the activity.\nTechnology requirement:\n\nThe provided handout assumes that students can use technology such as calculators to perform a two-sample t-test and interval for a difference in means between two groups.\nThe raw data is provided to allow instructors to customize the handout to incorporate other forms of technology. e.g., Students can use software such as Minitab to calculate test statistics and p-values for a t-test or StatKey for simulation based inference.\n\n\n\n\n\n\n\nThe data set has 162 rows with 9 columns. Each row represents a single lacrosse match either in the Premier Lacrosse League or the National Lacrosse League during the 2021-2022 season.\nDownload data: lacrosse_pll_nll_2021-2022.csv\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nLeague\nThe Premier Lacrosse league and the National Lacrosse League\n\n\nAway_team\nThe traveling team\n\n\nHome_team\nThe hosting team\n\n\nAway_shots\nHow many shots the Away_team had on net\n\n\nHome_shots\nHow many shots the Home_team had on net\n\n\nAway_goals\nHow many goals the Away_team had on net\n\n\nHome_goals\nHow many goals the Home_team had on net\n\n\nGoals\nThe total amount of goals scored each game\n\n\nGoals_per_48\nThe average amount of goals for the first 48 minutes of a game\n\n\n\n\n\n\nPremier Lacrosse League stats. Premier Lacrosse League Stats. (n.d.). https://stats.premierlacrosseleague.com/\nPlayer stats. NLL. (2023, January 26). https://www.nll.com/stats/all-player-stats/\n\n\n\n\nThe data and worksheet associated with this module are available for download through the following links.\n\nlacrosse_pll_nll_2012-2022.csv - Dataset with game-by-game shots and goals scored for both leagues in the 2021-2022 season..\nlacrosse_pll_vs_nll_t-test_worksheet.docx- Activity worksheet to compare scoring and shots between indoor and outdoor leagues using t-distributions.\nlacrosse_pll_vs_nll_randomization_worksheet.docx- Activity worksheet to compare scoring and shots between indoor and outdoor leagues using randomization tests implements via StatKey.\n\nSample solutions to the worksheets\n\nlacrosse_pll_vs_nll_t-test_worksheet_key.docx - Activity worksheet using t-distributions with sample solutions.\nlacrosse_pll_vs_nll_randomization_worksheet_key.docx - Activity worksheet using randomization tests with sample solutions.\n\n\n\n\n\n\n\nConclusion\n\n\n\n\n\nStudents should notice that while no discernible difference between average goals per game was discovered, after adjusting for the length of the game, PLL (i.e., the 48-minute outdoor league) has the higher rate per 48-minutes.\nFurther, students will ideally see how a confidence interval can be used to supplement the conclusion of a hypothesis test by bringing effect sizes into the interpretation in addition to statistical significance.",
    "crumbs": [
      "Home",
      "Lacrosse",
      "Lacrosse PLL vs. NLL"
    ]
  },
  {
    "objectID": "lacrosse/lacrosse_pll_vs_nll/index.html#module",
    "href": "lacrosse/lacrosse_pll_vs_nll/index.html#module",
    "title": "Lacrosse PLL vs. NLL",
    "section": "",
    "text": "Please note that these material have not yet completed the required pedagogical and industry peer-reviews to become a published module on the SCORE Network. However, instructors are still welcome to use these materials if they are so inclined.\n\n\nThis module examines the goals and shots in two prominent lacrosse leagues: the Premier Lacrosse League (PLL) and the National Lacrosse League (NLL). The PLL and NLL are highly regarded professional lacrosse leagues that feature top-tier athletes from around the world.\nThe PLL is played in an outdoor setting, following the field lacrosse format. This style of lacrosse is characterized by its larger field size, typically 110 yards by 60 yards. Field lacrosse involves 10 players per team and promotes a style of play that emphasizes long passes, intricate plays, and individual skills. Founded in 2019, the PLL operates with a touring model where teams travel to different cities each weekend, bringing the sport to a wide audience and fostering a festival-like atmosphere at each event. The league’s modernized approach includes a strong emphasis on media presence and player engagement. The PLL features approximately 200 athletes across its teams.\nIn contrast, the NLL follows the box lacrosse format, which is played indoors on a smaller, enclosed field, generally 200 feet by 85 feet. Box lacrosse involves 6 players per team, and the gameplay is marked by frequent physical interactions, quick ball movements, and high-intensity transitions. Established in 1986, the NLL has a traditional franchise model with teams based in specific cities across the United States and Canada. This structure has cultivated strong local fan bases and deep community ties, contributing to the league’s longevity. The NLL features approximately 500 athletes, many of which also play in the PLL.\nThese data, from the 2021-2022 seasons, allow for an analysis of goal-scoring within these leagues to identify differences between indoor (NLL) and outdoor (PLL) play. By examining goals and shots, we aim to understand how the environment and format of the game influence offensive strategies and overall scoring trends in professional lacrosse. It’s important to recognize that outdoor field lacrosse and indoor box lacrosse are distinct sports, each with its own unique dynamics, rules, and playing styles. By acknowledging these nuances, we can may gain a deeper understanding of how various playing conditions and league structures impact statistical outcomes.\n\n\n\n\n\n\nActivity Length\n\n\n\n\n\nThis activity would be suitable for an in-class example or can be modified to be a quiz or part of an exam.\n\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nThe learning goals associated with this module are:\n\nStudents will be able to test for a difference in means between two groups.\nStudents will be able to find a confidence interval for a difference in means between two groups.\n\n\n\n\n\n\n\n\n\n\nMethods\n\n\n\n\n\nThis module requires students use a two-sample test and confidence interval (e.g., t-test or randomization) to compare the means of two groups.\nStudents are expected to have access any equations and/or lecture notes to complete the activity.\nTechnology requirement:\n\nThe provided handout assumes that students can use technology such as calculators to perform a two-sample t-test and interval for a difference in means between two groups.\nThe raw data is provided to allow instructors to customize the handout to incorporate other forms of technology. e.g., Students can use software such as Minitab to calculate test statistics and p-values for a t-test or StatKey for simulation based inference.\n\n\n\n\n\n\n\nThe data set has 162 rows with 9 columns. Each row represents a single lacrosse match either in the Premier Lacrosse League or the National Lacrosse League during the 2021-2022 season.\nDownload data: lacrosse_pll_nll_2021-2022.csv\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nLeague\nThe Premier Lacrosse league and the National Lacrosse League\n\n\nAway_team\nThe traveling team\n\n\nHome_team\nThe hosting team\n\n\nAway_shots\nHow many shots the Away_team had on net\n\n\nHome_shots\nHow many shots the Home_team had on net\n\n\nAway_goals\nHow many goals the Away_team had on net\n\n\nHome_goals\nHow many goals the Home_team had on net\n\n\nGoals\nThe total amount of goals scored each game\n\n\nGoals_per_48\nThe average amount of goals for the first 48 minutes of a game\n\n\n\n\n\n\nPremier Lacrosse League stats. Premier Lacrosse League Stats. (n.d.). https://stats.premierlacrosseleague.com/\nPlayer stats. NLL. (2023, January 26). https://www.nll.com/stats/all-player-stats/\n\n\n\n\nThe data and worksheet associated with this module are available for download through the following links.\n\nlacrosse_pll_nll_2012-2022.csv - Dataset with game-by-game shots and goals scored for both leagues in the 2021-2022 season..\nlacrosse_pll_vs_nll_t-test_worksheet.docx- Activity worksheet to compare scoring and shots between indoor and outdoor leagues using t-distributions.\nlacrosse_pll_vs_nll_randomization_worksheet.docx- Activity worksheet to compare scoring and shots between indoor and outdoor leagues using randomization tests implements via StatKey.\n\nSample solutions to the worksheets\n\nlacrosse_pll_vs_nll_t-test_worksheet_key.docx - Activity worksheet using t-distributions with sample solutions.\nlacrosse_pll_vs_nll_randomization_worksheet_key.docx - Activity worksheet using randomization tests with sample solutions.\n\n\n\n\n\n\n\nConclusion\n\n\n\n\n\nStudents should notice that while no discernible difference between average goals per game was discovered, after adjusting for the length of the game, PLL (i.e., the 48-minute outdoor league) has the higher rate per 48-minutes.\nFurther, students will ideally see how a confidence interval can be used to supplement the conclusion of a hypothesis test by bringing effect sizes into the interpretation in addition to statistical significance.",
    "crumbs": [
      "Home",
      "Lacrosse",
      "Lacrosse PLL vs. NLL"
    ]
  },
  {
    "objectID": "marathons/marathon-records/index.html",
    "href": "marathons/marathon-records/index.html",
    "title": "Marathon Record-Setting Over Time",
    "section": "",
    "text": "https://isle.stat.cmu.edu/SCORE/Marathons_SCORE_Template/",
    "crumbs": [
      "Home",
      "Marathons",
      "Marathon Record-Setting Over Time"
    ]
  },
  {
    "objectID": "marathons/marathon-records/index.html#module",
    "href": "marathons/marathon-records/index.html#module",
    "title": "Marathon Record-Setting Over Time",
    "section": "",
    "text": "https://isle.stat.cmu.edu/SCORE/Marathons_SCORE_Template/",
    "crumbs": [
      "Home",
      "Marathons",
      "Marathon Record-Setting Over Time"
    ]
  },
  {
    "objectID": "robotics/FIRST_Robotics_Competition/index.html",
    "href": "robotics/FIRST_Robotics_Competition/index.html",
    "title": "FIRST Robotics Competition - Winning Chances",
    "section": "",
    "text": "Introduction\nThe FIRST Robotics Competition (FRC) is a high school level robotics competition, in which “[u]nder strict rules and limited time and resources, teams of high school students are challenged to build industrial-size robots to play a difficult field game in alliance with other teams.” It combines “the excitement of sport with the rigors of science and technology”.\nOne of the key features of FRC is that robot/team competes not individually, but in alliance with other teams. So, it is important for teams to “scout” other teams as potential alliance partners. Various methodologies/models to evaluate each team’s potential contribution were developed. One of the popular models is called Expected Points Added (EPA) model.\nDetailed algorithm of the EPA model can be found at here (https://www.statbotics.io/blog/epa). Briefly, the EPA model builds upon the Elo rating system which is a “well-known method for ranking chess players, and has been adapted to many other domains.” It produces predicted probabilities of winning for the alliance based on the past performances of each team in the alliance, as well as teams in the opposition alliance. As such, there is a desire/need to assess how good the EPA model prediction is.\nBrier score originated with weather forecast research. It was designed to evaluate the predicted probabilities against the actual outcomes and is straight forward to calculate. While it is not widely used outside specific use cases, it is one of many approaches for the important step of evaluating models based on their predictions. Since the EPA model provides the predicted winning probabilities, Brier score is useful for evaluating its performance by comparing its predicted winning probabilities to the actual FRC outcomes.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nBy the end of this activity, you will be able to:\n\nCalculate Brier score.\nInterpret Brier score.\n\n\n\n\n\n\nData\nIn this lesson, we will use the EPA data and competition outcomes calculated and compiled by the website statbotics.io.\n\ndtAll &lt;- read.csv(\"matches.csv\")\ndtUse &lt;- subset(dtAll, status==\"Completed\" & offseason==\"f\", \n                select=c(year, event, playoff, comp_level, winner, epa_win_prob))\n\nBelow is a description of the variables:\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nyear\nthe year/season of the FRC event\n\n\nevent\nunique identifier for each FRC event\n\n\nplayoff\n“t” for playoff match; “f” for qualifying match\n\n\ncomp_level\n“qm” for qualifying match; “sf” for semifinals match; “f” for finals match\n\n\nwinner\nwinning alliance (“red” or “blue”) of the match\n\n\nepa_win_prob\npredicted winning probability for the Red Alliance by EPA model\n\n\n\n\nThe data covers the competition seasons from 2002 to 2023, except for 2021 due to the COVID pandemic.\n\nunique(dtUse$year)\n\n [1] 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016\n[16] 2017 2018 2019 2020 2022 2023\n\n\nFor our example, we will use a particular event, the Hopper Division competition at the 2023 FRC World Championship in Houston, to illustrate the calculation of Brier score.\n\ndt2023hop &lt;- subset(dtUse, event==\"2023hop\")\n\nBelow is what the raw data looks like. Each row is a match between a Red alliance and a Blue alliance. Each alliance consists of three robots/teams.\n\n\n\n\n\n\n\n\nBrier Score\nFor match \\(i\\), let \\(f_i\\) denote the probability forecast. In our case, it is the predicted winning probability for the Red Alliance by EPA model, i.e., the variable epa_win_prob. Let \\(o_i\\) denote the match outcome: \\(o_i=1\\) when the Red alliance won and \\(o_i=0\\) when the Blue alliance won. The Brier score for match \\(i\\) is calculated as \\((f_i - o_i)^2\\). For example, suppose it is predicted that the Red alliance will win with 80% probability, i.e., \\(f_i=0.8\\), if the actual outcome is that the Red alliance won, the Brier score is \\((0.8-1)^2=0.04\\). If the actual outcome is that the Blue alliance won, the Brier score is \\((0.8-0)^2=0.64\\).\nBrier score is a quantity bounded by \\(0\\) and \\(1\\). Brier score of \\(0\\) means correctly predicting the outcome with 100% certainty. 50:50 random guess would give a Brier score of \\(0.25\\). The overall Brier score for all the matches during a competition event or season is simply the average of individual match scores: \\[\\frac{1}{N} \\sum_{i=1}^N (f_i - o_i)^2\\] The following table shows the calculation for each match.\n\n\n\n\n\n\nThe overall Brier score for the 2023 Hopper Division event is 0.1366352, which is slightly worse than the half way between perfect \\(0\\) and random guess \\(0.25\\).\n\n\nYour Turn\nNow, it’s your turn. Please use the data from the Turing Division competition at the 2022 FRC World Championship to calculate the average Brier score for the event. You should find the data in the file dt2022tur.csv.\n\ndt2022tur &lt;- subset(dtUse, event==\"2022tur\")\nwrite.csv(dt2022tur, \"dt2022tur.csv\")\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe average Brier score for the 2022 Turing Division event is 0.1160236, which is better than the Brier score for the 2023 Hopper Division event.\n\n\n\n\n\nOver the Years\nSince we have the data for more than 20 years, we could answer an interesting question: did the predictive ability of the EPA model change over the years?\nWe build two simple functions to do the calculations.\n\n\nTwo Functions\n\nThe first function calculates the Brier score for a given data set. Occasionally, a game can end in a draw. We assign the value of \\(0.5\\) to \\(o_i\\) for a draw.\n\ncalcBS &lt;- function(dt){\n  n &lt;- nrow(dt)\n  outcome &lt;- rep(NA, n)\n  outcome[dt$winner==\"red\"] &lt;- 1\n  outcome[dt$winner==\"draw\"] &lt;- 0.5\n  outcome[dt$winner==\"blue\"] &lt;- 0\n  diff &lt;- dt$epa_win_prob - outcome\n  Brier &lt;- mean(diff^2)\n  c(n=n, Brier=Brier)\n}\n\nThe second function separates the data by year and does the calculation for each year.\n\nbyYear &lt;- function(dt=dtUse) {\n  yrs &lt;- unique(dt$year)\n  m &lt;- length(yrs)\n  size &lt;- Brier &lt;- rep(NA, m)\n  for (i in 1:m) {\n    dat &lt;- subset(dt, year==yrs[i])\n    res &lt;- calcBS(dt=dat)\n    size[i] &lt;- res[1]\n    Brier[i] &lt;- res[2]\n  }\n  data.frame(year=yrs, n=size, Brier=Brier)\n}\nFRC &lt;- byYear()\n\n\nBelow are the Brier scores from 2002 to 2023.\n\nknitr::kable(FRC)\n\n\n\n\nyear\nn\nBrier\n\n\n\n\n2002\n2197\n0.2351889\n\n\n2003\n3173\n0.2225493\n\n\n2004\n3198\n0.2069319\n\n\n2005\n2059\n0.2043631\n\n\n2006\n3283\n0.1997467\n\n\n2007\n3563\n0.2099309\n\n\n2008\n4036\n0.1892942\n\n\n2009\n4567\n0.1961946\n\n\n2010\n5564\n0.1691369\n\n\n2011\n6224\n0.1621286\n\n\n2012\n7707\n0.1841302\n\n\n2013\n8242\n0.1704309\n\n\n2014\n10663\n0.1906669\n\n\n2015\n11810\n0.1841460\n\n\n2016\n13286\n0.1794790\n\n\n2017\n15429\n0.2043697\n\n\n2018\n16930\n0.1750251\n\n\n2019\n18022\n0.1758972\n\n\n2020\n4634\n0.1817734\n\n\n2022\n14645\n0.1480655\n\n\n2023\n16319\n0.1604984\n\n\n\n\n\nIt is interesting to note that the predictive ability of the EPA model has improved for the past 20 years. Since the model has not changed, I believe the improvement comes from established teams becoming more consistent and predictable. Meanwhile, the pool of newer, less experienced teams has stayed healthy.\n\nplot(FRC$year, FRC$Brier)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusion\n\n\n\n\n\nIn conclusion, Brier score is a simple statistic that assesses the probability prediction against actual outcome. A smaller Brier score corresponds to better prediction.\nThe EPA model has been getting better at predicting FRC match outcome.\n\n\n\n\n\nAuthor\nCreated by Jake Tan (Wissahickon High School). Jake is a subsystem leader at FRC Team 341, Miss Daisy. Team 341 competed at FRC World Championship in the Turing Division in 2022 and Hopper Division in 2023.",
    "crumbs": [
      "Home",
      "Robotics",
      "FIRST Robotics Competition - Winning Chances"
    ]
  },
  {
    "objectID": "triathlons/ironman-lakeplacid-mlr/index.html",
    "href": "triathlons/ironman-lakeplacid-mlr/index.html",
    "title": "Ironman Triathlon (Canadian Females) - Multiple Linear Regression",
    "section": "",
    "text": "Welcome video\n\n\n\nIntroduction\nAn Ironman triathlon is one of the most grueling endurance events in the world, consisting of three sequential races: a 2.4-mile (3.86 km) swim, a 112-mile (180.25 km) bike ride, and a marathon 26.2-mile (42.20 km) run. The event tests the physical and mental limits of athletes, requiring months or even years of dedicated training. Originating in 1978 in Hawaii, the Ironman triathlon has become a global phenomenon, symbolizing the ultimate challenge in long-distance triathlon competitions.\nThe Triathlon Multiple Linear Regression module focuses on analyzing the relationships and predictive capabilities of multiple variables in the context of triathlon performance. Specifically, we will explore the prediction of run times (the last stage) using swim times and bike times as predictors. By employing simple regression models, checking conditions with residuals plots, and conducting hypothesis tests and confidence intervals, we aim to identify significant predictors and understand their contextual implications. The dataset used for this analysis comprises the 2022 Canadian finishers of the Lake Placid Ironman.\n\n\n\n\n\n\nActivity Length\n\n\n\n\n\nThis activity would be suitable for an long in-class example (of approximately 30 - 60 minutes) or out-of-class assignment.\n\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\n\nUnderstand the concept of multiple linear regression and its application in analyzing sports data.\nRecognize the importance of checking assumptions, such as assessing residuals vs. fits plots, for validating the regression models.\nGain insights into the significance of predictors through hypothesis tests and confidence intervals.\nComprehend the interpretation and contextual implications of the findings from multiple linear regression analyses in the triathlon setting.\n\n\n\n\n\n\n\n\n\n\nMethods\n\n\n\n\n\nTo successfully complete this worksheet, students should have prior knowledge of the following statistical concepts:\n\nFamiliarity with simple linear regression, including interpretation of slopes, intercepts, and residuals.\nUnderstanding of hypothesis testing and confidence intervals, particularly in the context of regression analysis.\nKnowledge of residual analysis, including the interpretation of residual plots.\nBasic understanding of the triathlon sport and its components (swimming, biking, and running) to better grasp the contextual implications of the statistical analyses.\n\nTechnology requirement:\n\nThe “no tech” version of the activity uses Minitab to display the output and requires only simple hand calculations (for predictions and residuals).\nThe provided “tech required” handout assumes that students can use technology (such as Minitab) to perform a multiple linear regression analysis.\n\n\n\n\n\n\nData\nThe data set has 64 rows with 17 columns. Each row represents a Canadian female who has participated in the 2022 Lake Placid Ironman. Note that this data set includes more variables than what are needed to complete the activity. Students are welcome to further explore the data using these additional variables.\nDownload data: ironman_lake_placid_female_2022_canadian.csv\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nBib\nregistration number of each runner used for identification\n\n\nName\nThe participant’s name\n\n\nCountry\nWhat country the participant is from\n\n\nGender\nThe participant’s gender\n\n\nDivision\nThe age range or membership a runner is\n\n\nDivision.Rank\nWithin the divisions, the place each runner has obtained over all races\n\n\nOverall.Time\nThe total time it took to complete the Ironman in minutes\n\n\nOverall.Rank\nThe runner’s finishing place for that particular triathlon\n\n\nSwim.Time\nThe time in minutes it took to complete the swimming portion\n\n\nSwim.Rank\nThe place the runner finished for the swim portion\n\n\nBike.Time\nThe time in minutes it took to complete the biking portion\n\n\nBike.Rank\nThe place the runner finished for the bike portion\n\n\nRun.Time\nThe time in minutes it took to complete the running portion\n\n\nRun.Rank\nThe place the runner finished for the running portion\n\n\nFinish.Status\nStates whether someone completed the Ironman successfully\n\n\nLocation\nWhere the Ironman takes place\n\n\nYear\nThey year when the mentioned participant ran\n\n\n\n\n\n\nMaterials\nClass handout - requires technology\nClass handout - no technology required\nClass handout - with solutions\n\n\n\n\n\n\nConclusion\n\n\n\n\n\nIn conclusion, the Triathlon Multiple Linear Regression worksheet has provided valuable insights into the predictors of run times in the Lake Placid 2022 Ironman Canadian Finishers dataset. Initially, both Swim Time and Bike Time demonstrated significance as individual predictors, supported by hypothesis tests and confidence intervals. The hypothesis test for the slope term of Bike Time yielded a p-value of 0, indicating its significance. Additionally, the 95% confidence interval for the slope term of Swim Time excluded 0, further affirming its significance.\nHowever, when both Swim Times and Bike Times were included as predictors in the multiple linear regression model, we observed a change in significance. Swim Times were no longer a significant predictor. This finding suggests that when considering both predictors simultaneously, the predictive power of Swim Times in estimating run times diminishes. These findings emphasize the importance of evaluating multiple predictors in regression analysis and understanding how their inclusion can impact the significance and interpretation of individual predictors.",
    "crumbs": [
      "Home",
      "Triathlons",
      "Ironman Triathlon (Canadian Females) - Multiple Linear Regression"
    ]
  },
  {
    "objectID": "preprints.html",
    "href": "preprints.html",
    "title": "Preprint Access",
    "section": "",
    "text": "This page links to preprint repositories maintained by members of the SCORE Network. These preprint repositories are created and maintained by faculty and students from the respective institutions. Please note that these materials have not yet completed the required pedagogical and industry peer reviews to become a published module on the SCORE Network. However, instructors are still welcome to use these materials if they are so inclined. The following are actively maintained preprint repositories:\n\nCarnegie Mellon University\nSt. Lawrence University\nBaylor University + Azusa Pacific University\nWest Point",
    "crumbs": [
      "Home",
      "Preprint Access"
    ]
  }
]