[
  {
    "objectID": "by-statsds-topic.html",
    "href": "by-statsds-topic.html",
    "title": "üìä Modules by Statistics and Data Science Topic",
    "section": "",
    "text": "MMA Inter-rater Reliability Data Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStrength Ratios - Flat Dumbbell Press to Barbell Bench Press\n\n\n\nData Collection Methods\n\nConfidence Intervals\n\n\n\nCritically exploring self reported strength ratios.\n\n\n\n\n\nFeb 24, 2026\n\n\nVivian Johnson, Ivan Ramler\n\n\n\n\n\n\n\n\n\n\n\n\nPickleball: Estimating the proportion of dependable DUPR ratings\n\n\n\nConfidence Intervals\n\nProportions\n\n\n\nEstimate the proportion of collegiate pickleball players who have a dependable overall DUPR reliability score (of 60% or higher).\n\n\n\n\n\nDec 4, 2025\n\n\nFaith Rhinehart, Ivan Ramler, Jessica Chapman\n\n\n\n\n\n\n\n\n\n\n\n\nPGA - Drive for Show, Putt for Dough?\n\n\n\nCorrelation\n\n\n\nUsing tournament data for professional golfers to see if driving or putting are more strongly related to success.\n\n\n\n\n\nOct 23, 2025\n\n\nAlyssa Bigness, Robin Lock\n\n\n\n\n\n\n\n\n\n\n\n\nPGA - Drive for Show, Putt for Dough?\n\n\n\nCorrelation\n\n\n\nUsing tournament data for professional golfers to see if driving or putting are more strongly related to success.\n\n\n\n\n\nOct 23, 2025\n\n\nAlyssa Bigness, Robin Lock\n\n\n\n\n\n\n\n\n\n\n\n\nBlackJack Logistic Regression\n\n\n\nLogistic Regression\n\nBinary Data\n\n\n\nAn Introduction to Logistic Regression Using BlackJack\n\n\n\n\n\nJul 23, 2025\n\n\nAndrew Tran, Nicholas Clark\n\n\n\n\n\n\n\n\n\n\n\n\nOlympic Rowing Medals Between 1900 and 2022 - Data Wrangling\n\n\n\ndplyr\n\nfiltering\n\ngrouping and summarizing\n\nmutating\n\n\n\nArranging data to analyze the total number of medals and the weighted points for nations competing in rowing events in the Summer Olympic Games between 1900 and 2022.\n\n\n\n\n\nJun 5, 2025\n\n\nAbigail Smith, Robin Lock, Ivan Ramler\n\n\n\n\n\n\n\n\n\n\n\n\nOlympic Rowing Medals Between 1900 and 2022 - Summary Statistics\n\n\n\ndistribution and skewness\n\noutlier detection\n\nsummary statistics\n\nconfounding variable\n\n\n\nThe total number of medals and the weighted points for nations competing in rowing events in the Summer Olympic Games between 1900 and 2022.\n\n\n\n\n\nJun 5, 2025\n\n\nAbigail Smith, Ivan Ramler, Robin Lock\n\n\n\n\n\n\n\n\n\n\n\n\nAmerican Ninja Warrior - Kaplan-Meier Survival Analysis\n\n\n\nKaplan-Meier\n\nLog Rank test\n\nNonparametric tests\n\n\n\nExploring Survival Analysis using the Kaplan-Meier method with American Ninja Warrior data.\n\n\n\n\n\nJun 4, 2025\n\n\nJonathan Lieb, Rodney X. Sturdivant\n\n\n\n\n\n\n\n\n\n\n\n\nExpected Goals in Soccer\n\n\n\nLogistic Regression\n\nFeature Engineering\n\nUnder Sampling\n\n\n\nAn Introduction to Expected Goals Using Soccer\n\n\n\n\n\nMay 19, 2025\n\n\nColman Kim, Andrew Lee\n\n\n\n\n\n\n\n\n\n\n\n\nMLS - Types of Decision Errors\n\n\n\nDecision Errors\n\n\n\nExploring types of decision errors\n\n\n\n\n\nMar 30, 2025\n\n\nJonathan Lieb\n\n\n\n\n\n\n\n\n\n\n\n\nPGA - Scoring Average Confidence Intervals\n\n\n\nSingle Mean Confidence Intervals\n\nSingle Mean Hypothesis Testing\n\n\n\nExploring Single Mean Confidence Intervals with Golf Data\n\n\n\n\n\nJan 25, 2025\n\n\nJonathan Lieb\n\n\n\n\n\n\n\n\n\n\n\n\nPGA - Scoring Average Confidence Intervals (No R)\n\n\n\nSingle Mean Confidence Intervals\n\nSingle Mean Hypothesis Testing\n\n\n\nExploring Single Mean Confidence Intervals with Golf Data\n\n\n\n\n\nJan 25, 2025\n\n\nJonathan Lieb\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface\n\n\n\nANOVA\n\nTennis\n\n\n\nUsing tennis to teach ANOVA and linear regression with categorical variables\n\n\n\n\n\nJan 22, 2025\n\n\nZachary O. Binney, Heyi Yang\n\n\n\n\n\n\n\n\n\n\n\n\nMLB Injuries - Introductory Time Series Analysis\n\n\n\nTime series plots\n\nTime series decomposition\n\nResidual analysis\n\nSimple forecasting\n\n\n\nExploring MLB injury data through time series analysis and forecasting.\n\n\n\n\n\nNov 26, 2024\n\n\nJonathan Lieb\n\n\n\n\n\n\n\n\n\n\n\n\nWhat‚Äôs the prime age of an MLB player?\n\n\n\nData wrangling\n\nDplyr basics\n\nTidyverse\n\n\n\nUse data wrangling skills to explore the prime age of MLB players\n\n\n\n\n\nJul 26, 2024\n\n\nJazmine Gurrola, Joseph Hsieh, Dat Tran, Katie Fitzgerald\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Elo ratings\n\n\n\nElo ratings\n\nBrier score\n\nprediction\n\n\n\nAn introduction to Elo ratings using NFL game outcomes.\n\n\n\n\n\nJul 9, 2024\n\n\nRon Yurko\n\n\n\n\n\n\n\n\n\n\n\n\n2023 Boston Marathon - Variability in Finish Times\n\n\n\nhistograms\n\nsummary statistics\n\nbimodal data\n\n\n\nDescribing finish time for runners in the 2023 Boston Marathon\n\n\n\n\n\nMay 13, 2024\n\n\nIvan Ramler, Jack Fay\n\n\n\n\n\n\n\n\n\n\n\n\nFIRST Robotics Competition - Winning Chances\n\n\n\nBrier score\n\nprediction assessment\n\n\n\nEvaluating the predicted winning probabilities against the actual outcomes.\n\n\n\n\n\nMar 5, 2024\n\n\nJake Tan\n\n\n\n\n\n\n\n\n\n\n\n\nLeague of Legends - Buffing and Nerfing\n\n\n\noutliers\n\nsummary statistics\n\n\n\nInvestigating game play statistics for League of Legends champions in two different patches.\n\n\n\n\n\nFeb 21, 2024\n\n\nIvan Ramler, George Charalambous, A.J. Dykstra\n\n\n\n\n\n\n\n\n\n\n\n\nLacrosse Faceoff Proportions\n\n\n\nHypothesis testing\n\nSingle proportion\n\n\n\nUsing data from NCAA Div I lacrosse teams to explore the importance of winning faceoffs\n\n\n\n\n\nFeb 5, 2024\n\n\nJack Fay, Ivan Ramler, A.J. Dykstra\n\n\n\n\n\n\n\n\n\n\n\n\nLacrosse PLL vs.¬†NLL\n\n\n\nDifference in two means\n\n\n\nComparing scoring rates between indoor and outdoor profesional lacrosse leagues.\n\n\n\n\n\nFeb 5, 2024\n\n\nJack Cowan, Ivan Ramler, A.J. Dykstra, Robin Lock\n\n\n\n\n\n\n\n\n\n\n\n\nNASCAR Transformation Module\n\n\n\nLinear regression\n\nTransformations\n\nPolynomial regression\n\n\n\nUsing NASCAR driver rating data to explore a series of transformations to improve linearity in regression.\n\n\n\n\n\nFeb 5, 2024\n\n\nAlyssa Bigness, Ivan Ramler, Jack Fay\n\n\n\n\n\n\n\n\n\n\n\n\nIronman Triathlon (Canadian Females) - Multiple Linear Regression\n\n\n\nLinear regression\n\n\n\nUsing Lake Placid Ironman triathlon results for female Canadian finishers to predict run times for participants based on both swim and bike times.\n\n\n\n\n\nFeb 5, 2024\n\n\nA.J. Dykstra, Ivan Ramler\n\n\n\n\n\n\n\n\n\n\n\n\nStolen Bases\n\n\n\nNormality tests\n\n\n\n\n\n\n\n\n\nJul 23, 2023\n\n\nAndrew Lee and Jacob Hurtubise\n\n\n\n\n\n\n\n\n\n\n\n\nUnbreakable Records in Baseball\n\n\n\nBernoulli distribution\n\nBinomial distribution\n\nChi-Square Test\n\n\n\n\n\n\n\n\n\nJul 23, 2023\n\n\nAndrew Lee and Fr Gabriel Costa\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting NHL Shooting Percentages\n\n\n\nlinear regression\n\n\n\nAn Introduction to Simple Linear Regression\n\n\n\n\n\nJul 23, 2023\n\n\nSam Ventura\n\n\n\n\n\n\n\n\n\n\n\n\nMarathon Record-Setting Over Time\n\n\n\nExponential distribution\n\nPoisson process\n\n\n\nDetermining whether the setting of world records in the marathon is historically a Poisson process.\n\n\n\n\n\nJul 23, 2023\n\n\nNicholas Clark, Rodney Sturdivant, and Kate Sanborn\n\n\n\n\n\n\n\n\n\n\n\n\nIronman Triathlete Performance\n\n\n\nScatterplots\n\nCorrelation\n\n\n\nGaining insight into the performance patterns of triathletes by exploring the relationships between swimming, biking, and running times.\n\n\n\n\n\nJul 23, 2023\n\n\nMichael Schuckers, Matt Abell, AJ Dykstra, Sarah Weaver, Ivan Ramler, and Robin Lock\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "üìä Modules by Statistics and Data Science Topic"
    ]
  },
  {
    "objectID": "triathlons/ironman-lakeplacid-mlr/index.html#how-to-cite",
    "href": "triathlons/ironman-lakeplacid-mlr/index.html#how-to-cite",
    "title": "Ironman Triathlon (Canadian Females) - Multiple Linear Regression",
    "section": "How to Cite",
    "text": "How to Cite\nIf you use this module in your work, please cite it as follows:\nDykstra, A. J. & Ramler, I (2024, August 14). Ironman Triathlon (Canadian Females) Multiple Regression. ‚ÄúThe SCORE Network.‚Äù https://doi.org/10.17605/OSF.IO/4MRBT.\nYou can include this citation directly in your references or bibliography.",
    "crumbs": [
      "Home",
      "Triathlons",
      "Ironman Triathlon (Canadian Females) - Multiple Linear Regression"
    ]
  },
  {
    "objectID": "tennis/index.html",
    "href": "tennis/index.html",
    "title": "Tennis",
    "section": "",
    "text": "These modules use tennis data to teach topics in statistics and data science.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface\n\n\n\nANOVA\n\nTennis\n\n\n\nUsing tennis to teach ANOVA and linear regression with categorical variables\n\n\n\n\n\nJan 22, 2025\n\n\nZachary O. Binney, Heyi Yang\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Tennis"
    ]
  },
  {
    "objectID": "soccer/mls_decision_errors/index.html",
    "href": "soccer/mls_decision_errors/index.html",
    "title": "MLS - Types of Decision Errors",
    "section": "",
    "text": "NoteFacilitation notes\n\n\n\n\n\n\nThis module would be suitable for an in-class lab in an introductory statistics course or for supplemental self-study.\nThis module is designed for students just learning about hypothesis testing and is meant to bolster their understanding of errors that can be made in hypothesis testing. It assumes a basic knowledge of what hypothesis testing is.\nThis module requires no programming knowledge.\nThe data used to create this module can be downloaded here\nMuch of the data for the module was derived from an MLS article and fbref.com. The rest of the data concerning the opening of training facilities was individually researched.",
    "crumbs": [
      "Home",
      "Soccer",
      "MLS - Types of Decision Errors"
    ]
  },
  {
    "objectID": "soccer/mls_decision_errors/index.html#whats-worse-type-i-or-type-ii-errors",
    "href": "soccer/mls_decision_errors/index.html#whats-worse-type-i-or-type-ii-errors",
    "title": "MLS - Types of Decision Errors",
    "section": "What‚Äôs Worse: Type I or Type II Errors?",
    "text": "What‚Äôs Worse: Type I or Type II Errors?\nThe answer to this question depends on the context of the hypothesis test. In some cases, a Type I error is more serious than a Type II error. In other cases, a Type II error is more serious than a Type I error. Below are two examples related to injuries in sports.\n\nExample where Type I error is more serious than Type II error\nAssume that a new recovery method is being tested for athletes with muscle injuries. This method claims to help athletes recover faster from their injuries. The null hypothesis is that the new recovery method takes the same amount of time as traditional recovery methods. The alternative hypothesis is that the new recovery method is faster than traditional recovery methods.\nA Type I error in this context would claim that the new recovery method is effective, when in reality it is not. This could result in athletes using the new recovery method and potentially worsening their injuries by returning to play too soon. A Type II error in this context would be when the new recovery method is assumed to be ineffective when it is actually effective. This would result in athletes continuing to recover following traditional recovery timelines.\nThe risk of more serious injuries and more setbacks in the injury recovery process appears to be more serious than the risk of athletes continuing to follow standard recovery timelines. Therefore, in this context, a Type I error is more serious than a Type II error.\n\n\nExample where Type II error is more serious than Type I error\nAn example of a Type II error being more serious than a Type I error can be seen in the context of concussion protocols in sports. A player takes a hard hit to the head during a game. Team physicians run quick tests on the player. The null hypothesis is that the player performs equal on the test to an uninjured player and therefore does not have a concussion, while the alternative hypothesis is that the player performs worse and therefore has a concussion.\nA Type I error would assume a player has a concussion when they do not. This could result in the player being taken out of the game unnecessarily, but would not have long term consequences for their health and safety. A Type II error in this context would incorrectly assume a player was fine after a head injury, when they in fact have a concussion. This could result in the player continuing to play when they should not, potentially leading to serious long-term brain damage.\nThe risk of serious long term brain damage from a concussion is much greater concern than the risk of a player being taken out of a game unnecessarily. Therefore, in this context, a Type II error is more serious than a Type I error.\n\n\n\n\n\n\nNoteExercise 4: Consequences of Errors\n\n\n\nAnswer the following questions in the context of the new training facilities:\n\nWhich type of error would be more likely to hurt/fail to help the performance of the team?\n\nType I errorType II error\n\nWhich type of error would be more costly financially?\n\nType I errorType II error\n\nWhich type of error do you think is more serious in this context: Type I or Type II?",
    "crumbs": [
      "Home",
      "Soccer",
      "MLS - Types of Decision Errors"
    ]
  },
  {
    "objectID": "soccer/expected_goals/index.html",
    "href": "soccer/expected_goals/index.html",
    "title": "Expected Goals in Soccer",
    "section": "",
    "text": "This module introduces students to Logistic Regression, Feature Engineering, and Undersampling using a soccer-specific Expected Goals Model. We explain how to create a logistic regression model, using data gathered by Statsbomb from the 2022 World Cup.\nThis module is available on the ISLE platform Expected Goals in Soccer Module",
    "crumbs": [
      "Home",
      "Soccer",
      "Expected Goals in Soccer"
    ]
  },
  {
    "objectID": "soccer/expected_goals/index.html#module",
    "href": "soccer/expected_goals/index.html#module",
    "title": "Expected Goals in Soccer",
    "section": "",
    "text": "This module introduces students to Logistic Regression, Feature Engineering, and Undersampling using a soccer-specific Expected Goals Model. We explain how to create a logistic regression model, using data gathered by Statsbomb from the 2022 World Cup.\nThis module is available on the ISLE platform Expected Goals in Soccer Module",
    "crumbs": [
      "Home",
      "Soccer",
      "Expected Goals in Soccer"
    ]
  },
  {
    "objectID": "soccer/expected_goals/index.html#how-to-cite",
    "href": "soccer/expected_goals/index.html#how-to-cite",
    "title": "Expected Goals in Soccer",
    "section": "How to Cite",
    "text": "How to Cite\nIf you use this module in your work, please cite it as follows:\nKim, C., & Lee, A. (2025, May 19). Soccer - Expected Goals. ‚ÄúThe SCORE Network.‚Äù https://doi.org/10.17605/OSF.IO/953BP\nYou can include this citation directly in your references or bibliography.",
    "crumbs": [
      "Home",
      "Soccer",
      "Expected Goals in Soccer"
    ]
  },
  {
    "objectID": "rowing/olympic_rowing_datawrangling/index.html",
    "href": "rowing/olympic_rowing_datawrangling/index.html",
    "title": "Olympic Rowing Medals Between 1900 and 2022 - Data Wrangling",
    "section": "",
    "text": "Introduction to Rowing\nIf you are unfamiliar with the sport of rowing, we encourage you to watch the following video from World Rowing\n\n\n\n\nIntroduction to Module\nThis activity organizes the data in Olympic rowing between 1900 and 2022 so that the total medals and points for countries can be analyzed.\nThe Summer Olympic Games are an international athletics event held every four years and hosted in different countries around the world. Rowing was added to the Olympics in 1896 but was cancelled due to weather. It has been in every Summer Olympics since 1900. Rowing races in the Olympic context are regatta style, meaning that there are multiple boats racing head to head against each other in multiple lanes. Since 1912, the standard distance for Olympic regattas has been 2000m, but before then there had been a range in distances. The boat that is first to cross the finish line is awarded a gold medal, the second a silver medal, and the third a bronze.\nOver the course of its time as an Olympic sport there have been 25 different event entries. These events are separated by gender and range with the number of rowers in the boat (1, 2, 4, 6, 8, 17), the rigging (inrigged, outrigged), sculling, sweeping, and whether or not they are coxed. An inrigged shell means the riggers (where the oar is attached to the boat) are on the inside of the boat, outrigged shells mean the riggers are on the outside. Sculling is where the rowers have an oar on each side and sweeping is when each rower only has one oar on one side. The coxswain steers the boat and guides the rowers, some events have coxed boats whereas some others do not.\nThe original data set (athlete_events.csv) is not just about rowing but for every Olympic sport since 1896. It will be important to adjust the data so that it is just rowing. Moreover, medals are given to each athlete in the boat. In Olympic scoring however, the medals should be counted as one towards the entire boat. It is important to make sure this is the case when arranging the data.\nIn looking at the total medals and total points for each nation, it is interesting to see which nations dominate in Olympic rowing. Additionally, looking at the overall distribution of the medals for all countries provides insight on just how lop-sided medaling can be in rowing at the Olympic level. This effect could likely be attributed towards how much funding nations are placing towards their rowing teams.\n\n\n\n\n\n\nNoteActivity Length\n\n\n\n\n\nThis activity could be used as an example or a short take home assessment. Best to use as a predecessor to the introductory statistics rowing module associated with this module.\n\n\n\n\n\n\n\n\n\nNoteLearning Objectives\n\n\n\n\n\nBy the end of the activity, students will be able to:\n\nFilter a dataset so there are no NA values\nGroup data to procure summarized data\nMutate data to create new variables\n\n\n\n\n\n\n\n\n\n\nNoteMethods\n\n\n\n\n\nStudents will use basic dplyr skills to structure data to match the descriptions listed in the questions. Basic ggplot2 functions are also needed to visualize the data.\n\n\n\n\n\n\n\n\n\nNoteTechnology Requirements\n\n\n\n\n\nAll the questions will require the use of R or a similar programming language.\n\n\n\n\n\nData\nIn the data set there are 271,106 athletes from 230 countries competing in 66 sports at 35 different Olympics. Each row represents an individual athlete competing in an event at an Olympics. There are 15 variables in the dataset.\n\nDownload data:\n\nathlete_events.csv\n\n\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nID\nThe ID number assigned to the athlete.\n\n\nName\nThe name of the athlete in first name last name order.\n\n\nSex\nThe sex of the athlete.\n\n\nAge\nThe age of the athlete at the time of competing.\n\n\nHeight\nThe height of the athlete at the time of competing. Measured in cm.\n\n\nWeight\nThe weight of the athlete at the time of competing. Measured in kg.\n\n\nTeam\nThe team the athlete is a member of. In some cases this is different than theNOC.\n\n\nNOC\nThe nation the athlete is representing, usually the best variable to choose to analyze nations.\n\n\nGames\nThe Olympic Games the case is from. e.g.¬†‚Äú1992 Summer Olympics‚Äù.\n\n\nYear\nThe year the athlete was competing.\n\n\nSeason\nThe season the athlete was competing, ‚ÄúSummer‚Äù or ‚ÄúWinter‚Äù.\n\n\nCity\nThe city the Olympics were hosted in the case.\n\n\nSport\nThe sport the athlete competed in.\n\n\nEvent\nThe event the athlete competed in.\n\n\nMedal\nIf the athlete medaled, which medal they won.\n\n\n\nData Source\nKaggle: 120 years of Olympic history: athletes and results\n\n\n\nMaterials\nWe provide editable qmd files along with their solutions.\nWorksheet\nWorksheet Answers\n\n\n\n\n\n\nNoteConclusion\n\n\n\n\n\nThis dataprep worksheet helps students familiarize themselves with the use of basic dplyr tools to structure data in a way that is easier to analyze. In doing so, it enables to students to draw conclusions about different patterns in rowing as an Olympic sport when it comes to medaling.\n\n\n\n\n\nHow to Cite\nIf you use this module in your work, please cite it as follows:\nSmith, A., Lock, R., & Ramler, I. (2025, June 12). Olympic Rowing - Data Wrangling. ‚ÄúThe SCORE Network.‚Äù https://doi.org/10.17605/OSF.IO/XRYTQ\nYou can include this citation directly in your references or bibliography.",
    "crumbs": [
      "Home",
      "Rowing",
      "Olympic Rowing Medals Between 1900 and 2022 - Data Wrangling"
    ]
  },
  {
    "objectID": "robotics/index.html",
    "href": "robotics/index.html",
    "title": "Robotics",
    "section": "",
    "text": "These modules use robotics data to teach topics in statistics and data science.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFIRST Robotics Competition - Winning Chances\n\n\n\nBrier score\n\nprediction assessment\n\n\n\nEvaluating the predicted winning probabilities against the actual outcomes.\n\n\n\n\n\nMar 5, 2024\n\n\nJake Tan\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Robotics"
    ]
  },
  {
    "objectID": "pickleball/pickleball_one-proportion-ci/index.html",
    "href": "pickleball/pickleball_one-proportion-ci/index.html",
    "title": "Pickleball: Estimating the proportion of dependable DUPR ratings",
    "section": "",
    "text": "If you are unfamiliar with Pickleball, please watch this video:",
    "crumbs": [
      "Home",
      "Pickleball",
      "Pickleball: Estimating the proportion of dependable DUPR ratings"
    ]
  },
  {
    "objectID": "pickleball/pickleball_one-proportion-ci/index.html#background-to-pickleball-video",
    "href": "pickleball/pickleball_one-proportion-ci/index.html#background-to-pickleball-video",
    "title": "Pickleball: Estimating the proportion of dependable DUPR ratings",
    "section": "",
    "text": "If you are unfamiliar with Pickleball, please watch this video:",
    "crumbs": [
      "Home",
      "Pickleball",
      "Pickleball: Estimating the proportion of dependable DUPR ratings"
    ]
  },
  {
    "objectID": "pickleball/pickleball_one-proportion-ci/index.html#introduction-to-module",
    "href": "pickleball/pickleball_one-proportion-ci/index.html#introduction-to-module",
    "title": "Pickleball: Estimating the proportion of dependable DUPR ratings",
    "section": "Introduction to Module",
    "text": "Introduction to Module\nPickleball is a fast-paced sport blending tennis, badminton, and ping-pong, played with paddles and a plastic ball on a small court. College tournaments feature singles, doubles, and mixed doubles in a best-of-three format, emphasizing strategy, teamwork, and precision. Player performance is often tracked using the DUPR (Dynamic Universal Pickleball Rating) system, which rates skill from 2.000 to 8.000 based on match results, opponent strength, and recent play. A Reliability Score (1‚Äì100%) reflects how accurate a rating is, with 60%+ considered dependable.\nThe pickleBall dataset features data from the NCPA (National Collegiate Pickleball Association) of male and female players in 2024. Variables include characteristics such as name and sex. Other variables included the overrall, singles, doubles, and mixed rating and the overall, singles, doubles, and mixed reliability score. The relevant variables for this specific module are doubles reliability and singles reliability.\n\n\n\n\n\n\nNoteActivity Length\n\n\n\n\n\nThis could serve as an in class activity and should take roughly a half an hour to complete.\n\n\n\n\n\n\n\n\n\nNoteLearning Objectives\n\n\n\n\n\nStudents will gain experience performing single proportion confidence intervals.\n\n\n\n\n\n\n\n\n\nNoteMethods\n\n\n\n\n\nStudents are expected to have been exposed to the concept confidence intervals for a single proportion. (Although this is intended to be used as an earlier example for the topic.)",
    "crumbs": [
      "Home",
      "Pickleball",
      "Pickleball: Estimating the proportion of dependable DUPR ratings"
    ]
  },
  {
    "objectID": "pickleball/pickleball_one-proportion-ci/index.html#data",
    "href": "pickleball/pickleball_one-proportion-ci/index.html#data",
    "title": "Pickleball: Estimating the proportion of dependable DUPR ratings",
    "section": "Data",
    "text": "Data\nThis module used the pickleBall data from the SCORE Data Repository\nHowever, students do not need access to the data to complete the module as it is summarized in the provided worksheet.",
    "crumbs": [
      "Home",
      "Pickleball",
      "Pickleball: Estimating the proportion of dependable DUPR ratings"
    ]
  },
  {
    "objectID": "pickleball/pickleball_one-proportion-ci/index.html#materials",
    "href": "pickleball/pickleball_one-proportion-ci/index.html#materials",
    "title": "Pickleball: Estimating the proportion of dependable DUPR ratings",
    "section": "Materials",
    "text": "Materials\n\nClass handout (MS Word)\nClass handout - with solutions\n\n\n\n\n\n\n\nNoteConclusion\n\n\n\n\n\nStudents should gain an increased understanding of constructing and interpreting single-proportion confidence intervals.\n\n\n\n\nHow to Cite\nIf you use this module in your work, please cite it as follows:\nRhinehart, F., Ramler, I., & Chapman, J. (2025, December 4). Pickleball: Estimating the proportion of dependable DUPR ratings. ‚ÄúThe SCORE Network.‚Äù https://doi.org/10.17605/OSF.IO/H7ZPC\nYou can include this citation directly in your references or bibliography.",
    "crumbs": [
      "Home",
      "Pickleball",
      "Pickleball: Estimating the proportion of dependable DUPR ratings"
    ]
  },
  {
    "objectID": "motor_sports/nascar_regression_transformation/index.html",
    "href": "motor_sports/nascar_regression_transformation/index.html",
    "title": "NASCAR Transformation Module",
    "section": "",
    "text": "Introduction\nIn NASCAR, driver rating is a metric used to evaluate the performance of drivers in races. It provides a comprehensive assessment of a driver‚Äôs overall competitiveness, efficiency, and consistency during a race. The driver rating is based on several key performance factors and is designed to offer a more objective view of a driver‚Äôs abilities. For this activity, you will be exploring the relationship between average position a driver finishes per lap over a season and their corresponding driver rating. Using data transformations techniques and polynomial regression to create different variations of linear models, you will enhance the capabilities of your models to make them more effective and accurate.\n\n\n\n\n\n\nNoteMore about NASCAR\n\n\n\n\n\nNASCAR, the National Association for Stock Car Auto Racing, is a preeminent motorsports organization in the United States, distinguished by its high-speed competitions and fervent fanbase. Established in 1948 by Bill France Sr., NASCAR has evolved into a leading racing series that encompasses three national divisions: the NASCAR Cup Series, the Xfinity Series, and the Truck Series. The Cup Series is the most prestigious, showcasing the elite drivers and teams as they compete in events that span a variety of track types, typically oval in shape, and ranging from short tracks (0.24 miles) to expansive superspeedways (over 3 miles).\nThe races take place on diverse tracks across the nation, including renowned venues such as Daytona International Speedway and Talladega Superspeedway, where vehicles frequently exceed speeds of 200 miles per hour. NASCAR races serve as a complex interplay of driver skill, team strategy, and vehicle performance. Key metrics such as lap times, pit stop efficiency, and vehicle dynamics offer insights into the determinants of success in the sport. This not only enhances the understanding of race outcomes but also contributes to the development of strategies that optimize performance.\n\n\n\n\n\n\n\n\n\nNoteHow NASCAR uses data\n\n\n\n\n\nThe video linked below is a panel discussion from the 2023 Sloan Sports Analytics Conference entitled Start Your Engines: How Data is Fueling NASCAR‚Äôs Strategy to Engage an Evolving Customer Base.\nAlthough the panel discussion is not directly related to this module, Justin Marks discusses how data is used in NASCAR during the 26:53 to 30:44 minute interval.\n\n\n\n\n\n\n\n\n\n\n\nNoteActivity Length\n\n\n\n\n\nThis activity would be suitable for an in-class activity lasting approximately one class period or as an out-of-class assignment.\n\n\n\n\n\n\n\n\n\nNoteLearning Objectives\n\n\n\n\n\nBy the end of this activity, students will have reinforced the following skills:\n\nAssessing the effectiveness of simple linear regression models\nChecking regression model assumptions\nUsing log transformations to improve linear regression model fit\nApplying polynomial regression to model curved relationships\n\n\n\n\n\n\n\n\n\n\nNoteMethods\n\n\n\n\n\nFor this activity, students will need to use software to create scatterplots and plots of residual vs fitted values of models they will create. They will also need to create polynomial models and mutate the data by applying mathematic functions to columns.\nIt is assumed that students are already exposed to these concepts as the activity is intended to reinforce the skills instead of introduce them.\n\n\n\n\n\nData\nThe data comes from the NASCAR website and shows the season statistics from 2007-2022. Each row displays the metrics of a racer for that specific year. The data frame contains 1111 rows of observations and 20 variables.\nDownload data: nascar_driver_statistics.csv\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nWins\nThe sum of the driver‚Äôs victories\n\n\nAvgStart\nThe sum of the driver‚Äôs starting positions divided by the number of races\n\n\nAvgMidRace\nThe sum of the driver‚Äôs mid race positions divided by the number of races\n\n\nAvgFinish\nThe sum of the driver‚Äôs finishing positions divided by the number of races\n\n\nAvgPos\nThe sum of the driver‚Äôs position each lap divided by the number of laps\n\n\nPassDiff\nThe sum of green flag passes minus green times passed\n\n\nGreenFlagPasses\nNumber of green flag passes performed by the driver\n\n\nGreenFlagPassed\nNumber of times driver is passed during green flag\n\n\nQualityPasses\nNumber of passes in the top 15 while under green flag conditions by driver\n\n\nPercentQualityPasses\nThe sum of quality passes divided by green flag passes\n\n\nNumFastestLaps\nNumber of where the driver had the fastest speed on the lap\n\n\nLapsInTop15\nNumber of laps completed while running in a top 15 position\n\n\nPercentLapsInTop15\nThe sum of the laps run in the top 15 divided by total laps completed\n\n\nLapsLed\nThe sum of the laps led in a race\n\n\nPercentLapsLed\nThe sum of the laps led in the race\n\n\nTotalLaps\nThe sum of the laps completed by a driver that year\n\n\nDriverRating\nFormula combining wins, finish, top15-finish, average running position while on lead lap, average speed under green, fastest lap, led most laps, and lead lap finish with a maximum rating of 150 points\n\n\n\n\n\n\nMaterials\nClass handout\nClass handout - with solutions: MS Word\nClass handout - with solutions: Quarto\n\n\n\n\n\n\nNoteConclusion\n\n\n\n\n\nIn conclusion, the Transforming NASCAR Driver Data worksheet offers valuable insights into the relationship between average position and driver rating in NASCAR. Through the transformation of the average position variable, the worksheet enables students to enhance linearity in their models, thereby improving the accuracy of their predictions and analysis. The identification of curvature in the variable relationship also allows for the model using quadratic regression to be highly effective. The model can be conceptually compared to the model using a square root transformation to capture the same curve. Assessing the effectiveness of each model, students can critically evaluate the different approaches and determine the most suitable model for the data. This exercise equips students with essential data transformation and modeling skills, empowering them to make informed decisions and gain a deeper understanding of the factors influencing a driver‚Äôs performance.\n\n\n\n\n\nHow to Cite\nIf you use this module in your work, please cite it as follows:\nBigness, A., Ramler, I & Fay, J. (2024, August 14). Transforming NASCAR Driver Data. ‚ÄúThe SCORE Network.‚Äù https://doi.org/10.17605/OSF.IO/UQA2D\nYou can include this citation directly in your references or bibliography.",
    "crumbs": [
      "Home",
      "Motor Sports",
      "NASCAR Transformation Module"
    ]
  },
  {
    "objectID": "mixed_martial_arts/mma_interrater_reliability/index.html#learning-objectives",
    "href": "mixed_martial_arts/mma_interrater_reliability/index.html#learning-objectives",
    "title": "MMA Inter-rater Reliability Data Analysis",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this module you will be able to:\n\nunderstand the basic concepts of inter-rater reliability.\nunderstand the purposes of inter-rater reliability\nunderstand and interpret measures of reliability:\n\nPercent agreement\nCohen‚Äôs Kappa\nWeighted Kappa\nFleiss‚Äô Kappa\n\napply reliability measures to judging of MMA fight data",
    "crumbs": [
      "Home",
      "Mixed Martial Arts",
      "MMA Inter-rater Reliability Data Analysis"
    ]
  },
  {
    "objectID": "mixed_martial_arts/mma_interrater_reliability/index.html#percent-agreement",
    "href": "mixed_martial_arts/mma_interrater_reliability/index.html#percent-agreement",
    "title": "MMA Inter-rater Reliability Data Analysis",
    "section": "Percent Agreement",
    "text": "Percent Agreement\nWe‚Äôll begin by evaluating the inter-rater-reliability between two judges. One simple way is to calculate the percentage of fights in which they agree on the outcome. This is appropriately termed percent agreement. This reduces each fight into a simple outcome: agree or disagree, so we lose any information on the scores or margin of the fight (we will return to the original scores later.) Another limitation of percent agreement is that it ignores the possibility of judges arriving at agreement through chance. Still, it provides a foundation for the widely used Cohen‚Äôs kappa. There will be more on this later, but for now, let‚Äôs calculate the percent agreement for some judges.\nBelow, we see a table of the ten most frequently-appearing judges. The table includes the number of fights that they judged. D‚ÄôAmato, Lee, and Cleary are the most experienced judges in our data set.\n\n\njudgefightsD'Amato846Lee534Cleary532Cartlidge413Weeks384Bell383Col√≥n372Crosby368Rosales321Lethaby305\n\n\n\nExample: D‚ÄôAmato and Lee\nLet‚Äôs take the top two judges: D‚ÄôAmato and Lee, and compare their rulings on fights where they both judged the same fight. We‚Äôll look specifically at the outcome categorical variable: judge_out.\nHere is a contingency table that summarizes all the fights that D‚ÄôAmato and Lee judged together. Contingency tables are used to assess the association between two paired categorical variables. They tabulate the distribution of each variable and compare their results.\n\n\nLeeD'Amatofighter1drawfighter2totalfighter16201274draw4206fighter21005262total76264142\n\n\nThis table helps us to organize and compare D‚ÄôAmato‚Äôs and Lee‚Äôs ratings. Each row consists of D‚ÄôAmato‚Äôs votes and each column consists of Lee‚Äôs corresponding votes. Thus, the table forms a downward sloping diagonal where the judges agree. These are called concordant responses.\nThey both selected fighter 1 as the victor 62 times, a draw twice, and fighter 2 as victor 52 times. However, they selected opposing fighters 22 times (10 + 12), and D‚ÄôAmato voted for a draw four times where Lee disagreed.\nOur total concordant values is 62 + 2 + 52 = 116. We can calculate the simple percent agreement by adding up all the concordant values, and dividing it by the total number of results. Another word for this is the proportion of observed agreement (\\(p_{o}\\)). This term will be important later.\n\n\nAgreeDisagreePercent Agreement1162681.69%\n\n\nD‚ÄôAmato and Lee‚Äôs simple agreement (proportion of observed agreement) is only 81.69%. This means they disagreed on almost 1/5 of their rulings.\n\n\nExercise 1: Percent Agreement for D‚ÄôAmato and Cleary\nLet‚Äôs compare D‚ÄôAmato and Lee‚Äôs consistency with that of D‚ÄôAmato and Cleary. Below is the contingency table of D‚ÄôAmato and Cleary.\n\n\nClearyD'Amatofighter1drawfighter2totalfighter1650469draw1315fighter2707178total73376152\n\n\n1.1. Can you identify D‚ÄôAmato and Cleary‚Äôs concordant responses?\n\n 139 152 65 71 73\n\n1.2. Use the concordant responses to calculate D‚ÄôAmato and Cleary‚Äôs percent agreement yourself. Remember, the formula for percent agreement \\((p_{o})\\) = concordant values/total.\n\n 48% 46.7% 91.4% 89%\n\n\n\nAgreeDisagreePercent Agreement\n\n\n1.3. We‚Äôve left the table above empty for you. Fill it in with the appropriate values (Agree/Disagree/Percent Agreement).\n\n 139/13/91.4% 65/8/89% 73/79/48% 139/152/91.4%\n\n1.4. How do the results compare with D‚ÄôAmato and Lee?\n\n D'Amato and Lee agree the same amount D'Amato and Lee tend to agree less often D'Amato and Lee tend to agree more often\n\n\n\nExercise 2: Cartlidge and Lethaby\nWe next consider an example using the top judge combination Cartlidge and Lethaby.\n\n\nLethabyCartlidgefighter1drawfighter2totalfighter188016104draw2136fighter27096103total971115213\n\n\n2.1. Assess the table. How well do the judges tend to agree?\n\n Reasonable agreement (less than 30 times disagree) They agree very rarely (&lt;50%, 88 times) Excellent, they only disagree 1 time\n\n\n\nAgreeDisagreePercent Agreement\n\n\n2.2. Once again, we‚Äôve left an empty percent agreement table for you (above). Fill in the cells with the appropriate values and calculate the percent agreement.\n\n 186/28/86.8% 115/98/54% 186/213/86.8% 88/16/84.6%\n\n2.3. How do Cartlidge and Lethaby‚Äôs results compare to the other judges we assessed?\n\n They have slightly better agreement than D'Amato and Lee but not as strong as D'Amato and Cleary They have agreement about as strong as D'Amato and Cleary They have much worse agreement than either of the previous examples. They have much better agreement than either of the previous examples.\n\n\n\nExercise 3 - Advanced (Optional): Other Judges\nUse R code (or the package you use for data analysis) to select any two judges to compare from the list below.\n\n\njudge1judge2fights_judgedCartlidgeLethaby213ClearyD'Amato152D'AmatoLee142CollettLethaby116CartlidgeCollett113ClearyLee98Col√≥nTirelli90BellMccarthy78D'AmatoKamijo75D'AmatoWeeks74CrosbyD'Amato70BellD'Amato69BellCleary67KamijoWeeks64Col√≥nD'Amato63\n\n\n\n\nLimitations of Percent Agreement\nPercent agreement is helpful, because it gives us a general understanding of the judges‚Äô reliability, but it is limited. In particular, it cannot account for the judges‚Äô arriving at similar conclusions via chance.\nSo, how likely is it for judges to arrive at similar conclusions via chance even if they do not necessarily judge consistently?\nLet‚Äôs look at a simple simulation. Here, we have two hypothetical judges - we‚Äôll call them Jimmy and Mateo - rating 1000 fights. Except, instead of watching and analyzing the fights before carefully determining a winner, both Jimmy and Mateo slept through all 1000 fights. Luckily for them, they remembered the historical voting trends of MMA judges. Both of them, independently, decided to randomly select a winner for each of the 1000 fights in a way that was consistent with the likelihoods of the historical rulings.\nThe historical rulings are below. In the 1000 fights, they selected fighter 1 and fighter 2 about 49% of the time and a draw about 2% of the time.\n\n\n\n\n\n\n\n\n\nThey made these ratings without consulting each other or watching the fights. Below are the first 15 observations of the data set we created.\n\n\nfightJimmyMateo1fighter1fighter22fighter2fighter23fighter2fighter24fighter2fighter15fighter1fighter26fighter1fighter17fighter1fighter18fighter1fighter19fighter1fighter110fighter2fighter211fighter2fighter112fighter2fighter213fighter1fighter214fighter2fighter215fighter2fighter1\n\n\nAnd here is their contingency table.\n\n\nMateoJimmyfighter1drawfighter2totalfighter124112248501draw901322fighter22246247477total474185081,000\n\n\nTheir simple agreement numbers come out like this:\n\n\nAgreeDisagreePercent Agreement48851248.80%\n\n\nWoah! Jimmy and Mateo agreed 48.80% of the time. This certainly is not a good rate of agreement, but it does suggest caution in interpreting percent agreement rates of our real judges D‚ÄôAmato, Lee, and Cleary. Their percent agreements fell in the 80-90% range, but we can get over half that agreement with just random chance.\nWe next consider metrics that account for the fact that some agreement is likely just due to chance.",
    "crumbs": [
      "Home",
      "Mixed Martial Arts",
      "MMA Inter-rater Reliability Data Analysis"
    ]
  },
  {
    "objectID": "mixed_martial_arts/mma_interrater_reliability/index.html#cohens-kappa",
    "href": "mixed_martial_arts/mma_interrater_reliability/index.html#cohens-kappa",
    "title": "MMA Inter-rater Reliability Data Analysis",
    "section": "Cohen‚Äôs Kappa",
    "text": "Cohen‚Äôs Kappa\nCohen‚Äôs kappa is a second, more rigorous method, that assesses the agreement between two judges. Like percent agreement, it measures the reproducibility of repeated assessments of the same event. It was developed by Jacob Cohen in the 1960s as an alternative agreement method that accounts for the possibility of chance agreement.\nCohen‚Äôs kappa makes a few assumptions about the data:\n\nThe same two individuals must rate each event.\nThe principle of independence. The judges rate the same events without consultation or communication. This means the judges‚Äô results are paired.\nThe judgments are made between the same defined categories. In our context, the judges categorize the fight result as win/lose/draw for each fighter.\n\nAll three of these assumptions are met by our data. We will filter our data to ensure the same two judges score each event. Judges in MMA fights are kept in separate areas around the fight. All our judges vote for fighter 1, fighter 2, or a draw.\nLike percent agreement, Cohen‚Äôs kappa works with any categorical variable.\nCohen‚Äôs kappa isolates the judges‚Äô real agreement from their chance agreement. It produces a correlation coefficient kappa (\\(\\kappa\\)) that assesses the agreement between the two judges and ranges from -1 to 1.\n\nAt \\(\\kappa\\) = -1, the two judges produced exactly opposite assessments of the event.\nAt \\(\\kappa\\) = 0, the agreement between the two judges is tantamount to an agreement entirely produced by chance.\nAt \\(\\kappa\\) = 1, the two judges have perfect agreement. Their assessments of the events are identical.\n\n\nExample: D‚ÄôAmato and Lee\nAs we walk through the methodology of Cohen‚Äôs kappa, let‚Äôs revisit our example of Lee and D‚ÄôAmato.\nAgain, we begin with a contingency table.\nLeeD'Amatofighter1drawfighter2totalfighter16201274draw4206fighter21005262total76264142\nEarlier, we found the proportion of observed agreement for this table is 81.69%. If we‚Äôre going to account for chance, we need to estimate what the agreement rate would be if the results were completely randomized.\nWe can estimate these random results by producing theoretical estimates. This is called the expected value. We calculate the expected value of each cell by multiplying together three values. The first judge‚Äôs probability of producing a result, the second judge‚Äôs probability of producing the corresponding result independent of the first judge, and the total number of fights.\nFor example, to find the expected value in the draw-draw concordant cell. We can multiply D‚ÄôAmato‚Äôs draw rate of \\(\\frac{6}{142}\\) by Lee‚Äôs draw rate of \\(\\frac{2}{142}\\) and by the total number of fights: 142. We end up with 0.085.\nIn other words, If D‚ÄôAmato and Lee were to judge a new set of 142 fights and randomly pick their results from a hat containing their historical results together, we‚Äôd expect them to both pick a draw 0.085 times.\nWe created a table full of the expected values.\n\n\nLeeD'Amatofighter1drawfighter2totalfighter139.611.0433.3574draw3.210.082.706fighter233.180.8727.9462total76.002.0064.00142\n\n\nWith the table, we sum up the three concordant cells: 67.63, and divide by the total number of fights: 142. This gives us the proportion of expected agreement (\\(p_{e}\\)). A value of 47.63%.\nNow, with both the proportion of observed agreement and the proportion of expected agreement, we can calculate kappa using the formula:\n\n\\(kappa(\\kappa) = \\displaystyle \\frac{p_{o} - p_{e}}{1 - p_{e}}\\)\n\nWhen we plug in those values and solve for kappa, we find that the kappa between D‚ÄôAmato and Lee is 0.65.\nWe‚Äôre past all the calculations and math, but what does our kappa mean?\n\n\nInterpreting Kappa\nThe kappa value represents the percentage of the two judges‚Äô results that agree with one another over and above what we would expect from chance. Conversely, the complement of kappa represents the percentage of the two judge‚Äôs results that result from chance or straight-up disagreement.\nThus, the magnitude of the agreement is important. A higher kappa is always better, because it suggests a higher reproducibility in the measurement system. Unlike some statistical tests, the kappa statistic is not evaluated by passing a threshold. Instead, the exact assessment of a kappa often depends on a myriad of factors.\nThis contextual nature of kappa makes interpretation difficult. There is a lot of disagreement over the interpretations for different kappa values, and the guidelines typically vary with the field of study. For example, health related studies demand a stronger reliability than fields that have less widespread influence over the population‚Äôs well-being like, say, judging MMA fights.\nBelow is one evaluation method, that has been generally agreed upon by several prominent statisticians:\n\n\\(\\kappa &gt; 0.75\\) Excellent reproducibility\n\\(0.4 \\le \\kappa \\le 0.75\\) Good reproducibility\n\\(0 \\le \\kappa &lt; 0.4\\) Marginal reproducibility\n\nNote that a negative kappa value is possible, and would represent agreement that is worse than that expected by chance. Clearly such a value would indicate very poor reproducibility.\nApplying this method to our judges, D‚ÄôAmato and Lee‚Äôs kappa of 0.65 indidcates the judges have ‚Äúgood‚Äù reproducibility in their ratings. They have decent consistency and interchangeability. We should be careful, because an estimated 35% of their relationship is comprised of chance agreement or disagreement. However, we cannot speak to D‚ÄôAmato and Lee‚Äôs accuracy or validity in their ratings. We cannot assess if their judgments were correct.\n\nAdvanced (optional): Confidence intervals and tests for Kappa\n\n\nWe can produce a confidence interval and hypothesis test for our kappa.\nWith a large enough sample size, kappa is normally distributed with a standard error (se).\n\n\\(se(\\kappa) = \\sqrt{\\displaystyle \\frac{1}{n(1 - p_{e})^{n}} * [p_{e} + p_{e}^{2} - \\sum_{i=1}^{c}{(a_{i}b_{i}(a_{i} + b_{i}))}]}\\)\n\nUsing this standard error, we can calculate the 95% confidence interval by:\n\n\\(\\kappa = \\pm 1.96 * se(\\kappa)\\)\n\nA 95% confidence interval produces an estimated range for the true value of kappa. We can say with 95% confidence that the interval includes the true kappa. Like all confidence intervals, a larger sample size reduces this interval.\nThe confidence interval is important, because it helps us to see how much we can trust our kappa. A high kappa statistic that has a large confidence interval is far from ideal.\nOur 95% confidence interval for the kappa of D‚ÄôAmato and Lee is 0.529 to 0.772. Thus, we can say with 95% confidence that the interval (0.529, 0.772) includes the true value of kappa. In other words, we would not be surprised if the true kappa is as low as 0.529.\nWe can also create a hypothesis test for our kappa. We‚Äôre looking to test that there is at least some non-random association between the judges.\nOur kappa test has a null and alternative hypothesis of:\n\n\\(H_{o}: \\kappa = 0\\)\n\\(H_{a}: \\kappa &gt; 0\\)\n\nWe will hold to our null hypothesis unless we have significant evidence to reject it. This evidence is held in a p-value. If our p-value is less than our \\(\\alpha\\) of 0.05, then we have sufficient evidence to reject our null hypothesis and agree with our alternative. Moreover, if our confidence interval does not include 0 within its range, then we can reject the null hypothesis without checking for the p-value.\nThe hypothesis test can be misleading, however, because a small kappa value can reject the null hypothesis despite indicating only poor agreement. For this reason, confidence intervals are preferable.\nOur p-value for D‚ÄôAmato and Lee is 2.22^{-16}. We can thoroughly reject the null hypothesis that there is no association in the decisions of D‚ÄôAmato and Lee.\n\n\n\nExercise 4: D‚ÄôAmato and Cleary\nNow that we‚Äôve analyzed D‚ÄôAmato and Lee. We‚Äôd like to give you the opportunity to describe the agreement between D‚ÄôAmato and Cleary. We‚Äôll produce the results and you produce the analysis.\n\n\nClearyD'Amatofighter1drawfighter2totalfighter1650469draw1315fighter2707178total73376152\n\n\nClearyD'Amatofighter1drawfighter2totalfighter133.141.3634.569draw2.400.102.55fighter237.461.5439.078total73.003.0076.0152\n\n\n4.1. Recall our percent agreement assessment from earlier. Do D‚ÄôAmato and Cleary tend to agree?\n\n Very good agreement (less than 15 times disagree) Perfect agreement (0 values in the draw column) They agree very rarely (&lt;50%, 71 times)\n\n4.2. Compare the two tables. Do the expected values for the cells surprise you?\n\n Yes, it shows too much disagreement No.¬†Expected values are based on random selections so we would expect about 50 percent agreement\n\n4.3. After some calculations, we find that D‚ÄôAmato and Cleary‚Äôs kappa is 0.84. Using the guidelines demonstrated above, interpret the value.\n\n This value suggests excellent reproducibility This value suggests marginal reproducibility This value suggests good reproducibility\n\n4.4. (Optional - Advanced) We can run the confidence interval and hypothesis test through our program:\n\n\n\n    Estimate Cohen's kappa statistics and test the null hypothesis that the\n    extent of agreement is same as random (kappa=0)\n\ndata:  .\nZ = 10.844, p-value &lt; 2.2e-16\n95 percent confidence interval:\n 0.7522942 0.9217408\nsample estimates:\n[1] 0.8370175\n\n\nAnalyze the results. Produce explanations of the confidence interval and hypothesis test, and provide your own assessment of the association between D‚ÄôAmato and Cleary. Try to use the wording and phrases that we explained earlier.\n\n\nExercise 5: Cleary and Lee\nNext, we have selected to analyze Cleary and Lee. Note: the lower the sample size, the wider our confidence interval and the less we can trust our kappa value.\n5.1. How does the sample size between these two judges likely impact the kappa estimate?\n\n The sample size is larger than previous examples so we will have LESS confidence in the kappa estimate The sample size is smaller than previous examples so we will have MORE confidence in the kappa estimate The sample size is larger than previous examples so we will have MORE confidence in the kappa estimate The sample size is smaller than previous examples so we will have LESS confidence in the kappa estimate\n\n\n\nLeeClearyfighter1drawfighter2totalfighter14411156draw2002fighter2703340total5314498\n\n\nLeeClearyfighter1drawfighter2totalfighter130.290.5725.1456draw1.080.020.902fighter221.630.4117.9640total53.001.0044.0098\n\n\n5.2. Assess the two tables. Do the judges appear to agree?\n\n The agree almost perfectly They almost never They do disagree a fair amount relative to the sample size, but still near 80 percent\n\n5.3. Now, compare the two tables. Do the expected values for the cells surprise you? How similar are they to the observed values?\n\n Very surprising. Expected agreement is as high as the observed. Expected agreement is roughly 50 percent so not surprising.\n\n5.4. After solving for kappa, our value is 0.21. Using the guidelines demonstrated previously, interpret the value. How does it compare to previous judge-pairings kappas?\n\n This value suggests excellent reproducibility similar to previous examples This value suggests good reproducibility, somewhat lower than previous examples This value suggests marginal reproducibility, much worse than previous examples\n\n\n\n\n\n    Estimate Cohen's kappa statistics and test the null hypothesis that the\n    extent of agreement is same as random (kappa=0)\n\ndata:  .\nZ = 4.1376, p-value = 1.755e-05\n95 percent confidence interval:\n 0.08973446 0.32838582\nsample estimates:\n[1] 0.2090601\n\n\n5.5. (Optional advanced) Interpret the confidence interval and p-value. What does they mean for the relationship between the two judges?\n\n The small sample leads to a wide interval so the reproducibility has high probability of still being excellent The interval provides strong evidence of marginal reproducibility",
    "crumbs": [
      "Home",
      "Mixed Martial Arts",
      "MMA Inter-rater Reliability Data Analysis"
    ]
  },
  {
    "objectID": "mixed_martial_arts/mma_interrater_reliability/index.html#weighted-kappa",
    "href": "mixed_martial_arts/mma_interrater_reliability/index.html#weighted-kappa",
    "title": "MMA Inter-rater Reliability Data Analysis",
    "section": "Weighted Kappa",
    "text": "Weighted Kappa\nGreat. We‚Äôve analyzed our data and produced a kappa value that assesses the true agreement between judges while accounting for random chance.\nStill, we are losing some information. Our judges supply score cards with point values for each fighter. They don‚Äôt just assign a winner. When we reduce each judge‚Äôs ruling to win, lose, or draw, we miss out on the degree of these victories. Instead of looking at the outcome variable, let‚Äôs analyze the margin variable.\nIn this case, the margin variable is an ordinal variable. Ordinal variables are a type of categorical variable that has a similar function to nominal variables, except that there is a clear ordering in the results. Height, for example, can be divided into ordinal categories like ‚Äúvery tall‚Äù, ‚Äútall‚Äù, ‚Äúnormal‚Äù, ‚Äúshort‚Äù, and ‚Äúvery short‚Äù.\nThis clear ordering of the categories allows for partial agreement. Partial agreement affords some credit to close responses. The judge‚Äôs responses may not be identical, but they could be close. Short is a lot closer to very short than very tall. Partial agreement takes this into account.\nWeighted kappa is a variant of Cohen‚Äôs kappa (also created by Jacob Cohen) that permits this partial agreement between responses. Like Cohen‚Äôs kappa, it accounts for any chance agreement, but it also takes into account the proximity of the judges‚Äô results. A large disparity in the two judge‚Äôs margin will lower the agreement much more than smaller disparities. The unweighted Cohen‚Äôs kappa, however, treats all disparities equally.\nThe weighted kappa makes assumptions that are similar to Cohen‚Äôs kappa about the data:\n\nThe same two individuals must rate each event.\nThe principle of independence. The judges rate the same events without consultation or communication. This means the judges‚Äô results are paired.\nThe judgments are made between the same ordinal categories.\n\n\nExercise 6: D‚ÄôAmato and Lee\nWeighted kappa begins like the Cohen‚Äôs kappa with a contingency table. To simplify the analysis for this example, we only kept the fights that went three rounds.\n\n\nLeeD'Amato-5-4-3-2-1012345Total-5010000000001-4040000000004-310130400000018-2001210000004-1002114090100270000002300005100107018150032200000012000330010006011202040000000003035000000001102Total151832623731860119\n\n\n6.1. Take a look at the contingency table. Trace your eyes along the diagonal of concordant values. How often do the judges completely agree?\n\n 69 times, 58 percent 97 times, 82 percent 8 times, 7 percent\n\n6.2. In the first column, there is a single observation (a 1 in row three). What does this value represent?\n\n Agreement about the fight outcome (winner) but Lee had a wider margin in scores Agreement about the fight outcome (winner) but D'Amato had a wider margin in scores Disagreement about the fight outcome with the judges picking different winners\n\n6.3. What are the most common frequencies? Why?\n\n 0. The judges disagree too much. 18. Score differences of three are very likely since fights are three rounds. 0. There are many possible score combinations for the size of the sample.\n\n\n\nLeeD'Amato-5-4-3-2-1012345Total-50.00.00.20.00.20.00.30.00.20.101-40.00.20.60.10.90.11.20.10.60.204-30.20.82.70.53.90.35.60.52.70.9018-20.00.20.60.10.90.11.20.10.60.204-10.21.14.10.75.90.58.40.74.11.402700.00.20.80.11.10.11.60.10.80.30510.31.34.80.87.00.59.90.84.81.603220.00.10.50.10.70.10.90.10.50.20330.20.83.00.54.40.36.20.53.01.002040.00.10.50.10.70.10.90.10.50.20350.00.10.30.10.40.00.60.10.30.102Total1.05.018.03.026.02.037.03.018.06.00119\n\n\n6.4. Now look at the expected values. What values are the largest? Is this surprising?\n\n Large differences between judges (5 and -5). The judges should be expected to disagree by a lot. All diagnonal values. The judges are expected to agree often. All less than 10. There are many possible score combinations for the size of the sample.\n\n6.5. Does the observed agreement surpass the expected agreement? By how much?\n\n No 7 observed vs 58 percent expected. No.¬†58 observed vs 78 percent expected. Yes. 58 observed vs 18 percent expected. Yes. 82 observed vs 58 percent expected.\n\n\n\nWeights\nLike Cohen‚Äôs kappa, the weighted kappa calculates the proportion of observed agreement and the proportion of expected agreement by using the concordant values along the diagonal of our contingency tables. However, the calculation of these two agreements becomes more complex, because we can allow for partial agreement for close matches. The formulas are the same as Cohen‚Äôs kappa, except for the addition of the weights:\n\n\\(p_{o} = \\sum_{i}\\sum_{j} W_{ij} P_{ij}\\)\n\\(p_{e} = \\sum_{i}\\sum_{j} W_{i+} P_{+j}\\)\n\nwhere W is the weight for each cell and P is the proportion of each cells frequency.\nThe weights W are proportions between 0 and 1 that reflect the level of agreement. All concordant values have complete agreement, so their weight is 1. Values to the left and right of the diagonal have proprtions slightly less than 1 and so on. In the standard unweighted Cohen‚Äôs kappa, all the diagonal values have weights of 1 and the non-diagonal values have weights of 0.\nThere are many different ways to calculate the weights and selecting them generally depends on the size of the table and the distribution of the variables. Two common methods are linear and quadratic weighting.\nLinear weights, formally known as Cicchetti-Allison weights, create equal distance between the weights. A cell‚Äôs weight is directly proportional to its distance from the concordant value.\nThe formula for the linear weights are:\n\n\\(W_{ij} = 1 - (|i - j|)/(R - 1)\\)\n\nR is the total number of categories and |i - j| is the distance between the two cells.\nLet‚Äôs calculate the weights of the first few cells using the the formula. We‚Äôll begin with the (-5, -5) cell and move right on the table.\n\n\\(W_{-5,-5} = 1 - (|0|/(11-1))\\) = 1\n\\(W_{-5,-4} = 1 - (|1|/(11-1))\\) = .9\n\\(W_{-5,-3} = 1 - (|2|/(11-1))\\) = .8\n\\(W_{-5,-2} = 1 - (|3|/(11-1))\\) = .7\n\\(W_{-5,-1} = 1 - (|4|/(11-1))\\) = .6\n\n\n\nJudge2Judge1-5-4-3-2-1012345-51.00.90.80.70.60.50.40.30.20.10.0-40.91.00.90.80.70.60.50.40.30.20.1-30.80.91.00.90.80.70.60.50.40.30.2-20.70.80.91.00.90.80.70.60.50.40.3-10.60.70.80.91.00.90.80.70.60.50.400.50.60.70.80.91.00.90.80.70.60.510.40.50.60.70.80.91.00.90.80.70.620.30.40.50.60.70.80.91.00.90.80.730.20.30.40.50.60.70.80.91.00.90.840.10.20.30.40.50.60.70.80.91.00.950.00.10.20.30.40.50.60.70.80.91.0\n\n\nNotice that each concordant value is 1 and all values next to it are 0.9. This creates a cascade effect for the weights.\nQuadratic weights, formally known as Fleiss-Cohen weights, use quadratic distancing between the weights. A cell‚Äôs weight is quadratically related to its distance from the concordant value.\nThe formula for the quadratic weights are:\n\n\\(W_{ij} = 1 - (|i - j|)^{2}/(R - 1)^{2}\\)\n\nAgain, let‚Äôs calculate the weights of the first few cells using the the formula. We‚Äôll begin with the (-5, -5) cell and move right on the table.\n\n\\(W_{-5,-5} = 1 - (|0|^{2}/(11-1)^{2})\\) = 1\n\\(W_{-5,-4} = 1 - (|1|^{2}/(11-1)^{2})\\) = .99\n\\(W_{-5,-3} = 1 - (|2|^{2}/(11-1)^{2})\\) = .96\n\\(W_{-5,-2} = 1 - (|3|^{2}/(11-1)^{2})\\) = .91\n\\(W_{-5,-1} = 1 - (|4|^{2}/(11-1)^{2})\\) = .84\n\n\n\nJudge2Judge1-5-4-3-2-1012345-51.000.990.960.910.840.750.640.510.360.190.00-40.991.000.990.960.910.840.750.640.510.360.19-30.960.991.000.990.960.910.840.750.640.510.36-20.910.960.991.000.990.960.910.840.750.640.51-10.840.910.960.991.000.990.960.910.840.750.6400.750.840.910.960.991.000.990.960.910.840.7510.640.750.840.910.960.991.000.990.960.910.8420.510.640.750.840.910.960.991.000.990.960.9130.360.510.640.750.840.910.960.991.000.990.9640.190.360.510.640.750.840.910.960.991.000.9950.000.190.360.510.640.750.840.910.960.991.00\n\n\nAgain, notice how each concordant value is 1 and all values next to it are .99. This creates a steeper cascade than the linear weighting as the differences in judging increase.\nAssess the two weighting methods for yourself. What are the advantages and disadvantages of each? Can you imagine any problems arising for either? Which would you choose for our MMA data and why?\nLinear weighting values the distance between the fourth and fifth category the same as the distance between the first and second category. If this constant effect fits the data, then it‚Äôs best to choose linear weighting.\nQuadratic weighting determines that the distance between the first and second category is much less than the distance between the fourth and fifth category. As the categories get furthered removed from the concordant value, the difference becomes more egregious.\nFor the MMA data, we tend to think the quadratic weighting method works best. Generally, egregious misses are the errors that cast doubt on the judging system. The difference in a 3 point and 2 point win is basically none. Still, we need to be careful. Under the quadratic weighting method, a 1 point win for fighter 1 and a 1 point win for fighter 2 are essentially in agreement (w = .96).\n\n\nCalculating and Interpreting Weighted Kappa\nThe calculation and interpretation of the weighted kappa \\(\\kappa\\) are the same as Cohen‚Äôs kappa. If you need a refresher, read through our explanation in the previous tab.\nOur weighted kappa \\((\\kappa)\\) is calculated once again by \\(kappa(\\kappa) = \\displaystyle \\frac{p_{o} - p_{e}}{1 - p_{e}}\\).\nwith weights:\n\n\\(p_{o} = \\sum_{i}\\sum_{j} W_{ij} P_{ij}\\)\n\\(p_{e} = \\sum_{i}\\sum_{j} W_{i+} P_{+j}\\).\n\nUsing quadratic weights, the observed proportion of agreement is 0.982. This is extremely high, because we have so many partial agreements. If you‚Äôre curious, look again through our contingency table.\nHowever, the expected proportion of agreement is also very high at 0.9. The weights may inflate our observed agreement levels by adding in partial agreement, but they also inflate our expected agreement.\nAfter solving for D‚ÄôAmato and Lee‚Äôs weighted kappa, we find it at 0.82. This is higher than the unweighted kappa (for the outcome variable) of 0.65, likely because a lot of judges disagree marginally.\nOur interpretation for the weighted kappa is identical to that of Cohen‚Äôs kappa. Below is a reminder:\n\n\\(\\kappa &gt; 0.75\\) Excellent reproducibility\n\\(0.4 \\le \\kappa \\le 0.75\\) Good reproducibility\n\\(0 \\le \\kappa &lt; 0.4\\) Marginal reproducibility\n\nUsing quadratic weights, we can say there is excellent reproducibility in the scoring margins between D‚ÄôAmato and Lee. This means the judges are generally consistent in their scores and it is possible to replace one with the other and expect similar results. Remember, as with the other measures, this kappa does not mean that the judges are accurate in their assessments.\nWe calculate the confidence interval the same way as before, and our confidence interval is from 0.581 to 1.059. Thus, with 95% confidence, the interval includes the true kappa value for these judges.\nThis should give us pause. The lower end of our confidence interval is 0.581. This means the true kappa could be this low. This would drop our verdict to ‚Äúgood reproducibility‚Äù and change our overall assessment of the relationship.\nLike Cohen‚Äôs kappa, our kappa test has a null and alternative hypothesis of:\n\n\\(H_{o}: \\kappa = 0\\)\n\\(H_{a}: \\kappa &gt; 0\\)\n\nThe confidence interval doesn‚Äôt include 0, so we have sufficient evidence to reject the null hypothesis that there is no association between the judges‚Äô scores.\n\n\nComparing Kappa and the Weighted Kappa\nLet‚Äôs compare our results with the linear weights. The observed proportion of agreement is 0.919 and the expected proportion of agreement is 0.747. The linear-weighted kappa is 0.68.\nThis drops our interpretation to only ‚Äúgood reproducibility‚Äù. We can be reasonably confident in the judges‚Äô reproducibility, but it‚Äôs also feasible that swapping D‚ÄôAmato for Lee could lead to a different result. An estimated 32% of the data is composed of chance agreement or disagreement. Once again, this would not indicate that D‚ÄôAmato or Lee are somehow less accurate than before, it only speaks to their consistency and reproducibility.\nWe can say with 95% confidence that 0.486 and 0.874 contains the true kappa value. Once again, the interval does not include 0, so we have sufficient evidence to reject our null hypothesis that there is no association between the judges‚Äô rulings. The lower end of the interval is 0.486. This would indicate ‚Äúgood reproducibility‚Äù. As with the quadratic weighting, this should lower our assessment of the relationship between the two judges.\n\n\nExercise 7: D‚ÄôAmato and Cleary\nNow that we‚Äôve walked through an example of weighted kappa on the consistency of D‚ÄôAmato‚Äôs and Lee‚Äôs scoring margins, let‚Äôs look at the scoring margins of D‚ÄôAmato and Cleary. We‚Äôll present the data and the findings to you, and you can reproduce the analysis. Feel free to look at our earlier phrasings and points.\nOnce again, we filtered the data to only include three rounds. We displayed all of the scoring margins by the judges in a contingency table below.\n\n\nClearyD'Amato-5-4-3-2-1012345Total-5210000000003-4031000000004-301170300000021-2000010000001-1003120050000290000003000003100002020021025200000001010230000007014002140000000212165000000000112Total252112633231752117\n\n\n7.1. How often do D‚ÄôAmato and Cleary completely agree?\n\n 71 times (61 percent) 98 times (84 percent) 83 times (71 percent)\n\n7.2. Do D‚ÄôAmato and Cleary seem to agree on the fight often but not on the score?\n\n No.¬†There are many values in the off diagnonal. Yes. There are only 7 times one is positive and the other negative. Yes. There are few values in the off diagnonal.\n\n7.3. How does D‚ÄôAmato and Cleary agreement compare to D‚ÄôAmato and Lee (previous exercise)?\n\n They agree more. (71 vs 58 percent) They agree much less (28 vs 71 percent) They agree less (58 vs 71 percent) They agree much more (92 vs 58 percent)\n\n\n\nClearyD'Amato-5-4-3-2-1012345Total-50.10.10.50.00.70.10.80.10.40.10.13-40.10.20.70.00.90.11.10.10.60.20.14-30.40.93.80.24.70.55.70.53.10.90.421-20.00.00.20.00.20.00.30.00.10.00.01-10.51.25.20.26.40.77.90.74.21.20.52900.10.10.50.00.70.10.80.10.40.10.1310.41.14.50.25.60.66.80.63.61.10.42520.00.10.40.00.40.10.50.10.30.10.0230.40.93.80.24.70.55.70.53.10.90.42140.10.31.10.11.30.21.60.20.90.30.1650.00.10.40.00.40.10.50.10.30.10.02Total2.05.021.01.026.03.032.03.017.05.02.0117\n\n\n7.4. Look through the expected value table. Do the highly expected values also occur frequently in the observed table?\n\n No.¬†The highest observed values of 20 are 0 in the expected table. Yes. The highest expected values of 5 and above are all observed at least 10 times. Somewhat. The highest observed values have relatively high exepected, but some high expected values are not observed often.\n\nQuadratic Weights:\n\nProportion of Observed Agreement: 0.99\nProportion of Expected Agreement: 0.878\nWeighted kappa with Quadratic Weights: 0.92\n95% Confidence Interval for kappa: 0.772 and 1.068\n\n7.5. Using quadratic weights do D‚ÄôAmato and Cleary have good reproducibility?\n\n Yes, the kappa value is over 0.9. No.¬†The observed agreement is not much better than the expected.\n\n7.6. (Optional - Advanced) Assess the confidence interval based on quadratic weights. What can you conclude?\n\n We have evidence to support excellent reproducibility for these two judges. We have too much variability so there is not enough evidence to conclude reproducability is excellent.\n\nLinear Weights:\n\nProportion of Observed Agreement: 0.948\nProportion of Expected Agreement: 0.722\nWeighted kappa with Linear Weights: 0.81\n95% Confidence Interval for kappa: 0.665 and 0.955\n\n7.7. Using linear weights do D‚ÄôAmato and Cleary have good reproducibility?\n\n No.¬†The observed agreement is not much better than the expected. Yes, the kappa value is over 0.9.\n\n7.8. (Optional - Advanced) Assess the confidence interval based on linear weights. What can you conclude?\n\n We have evidence to support excellent reproducibility for these two judges. We have too much variability so there is not enough evidence to conclude reproducability is excellent.\n\n7.9. (Optional Advanced) How large is the difference between the quadratic and linear weights?\n\n A large enough difference to matter when drawing inference from the confidence interval. Hardly any difference so the choice does not matter here.",
    "crumbs": [
      "Home",
      "Mixed Martial Arts",
      "MMA Inter-rater Reliability Data Analysis"
    ]
  },
  {
    "objectID": "mixed_martial_arts/mma_interrater_reliability/index.html#fleiss-kappa",
    "href": "mixed_martial_arts/mma_interrater_reliability/index.html#fleiss-kappa",
    "title": "MMA Inter-rater Reliability Data Analysis",
    "section": "Fleiss‚Äô Kappa",
    "text": "Fleiss‚Äô Kappa\nThus far, we‚Äôve assessed the inter-rater reliability within data sets of two judges, but what about three or more judges? MMA fights are evaluated by three judges, and in both the weighted and unweighted variations of Cohen‚Äôs kappa, we completely ignore the third judge. This ignorance becomes even more egregious if we have larger quantities of judges.\nSeveral different methodologies have been created to account for this. Light‚Äôs kappa, for example, takes the average of every combination of Cohen‚Äôs kappa within the pool of raters. We‚Äôll turn to a slightly more complex version. Fleiss‚Äô kappa is a variation of Cohen‚Äôs kappa that allows for three or more judges. It measures the level of agreement or consistency within the group of judges. A high Fleiss‚Äô kappa would indicate a high rate of reliability between the group of judges.\nFleiss‚Äô kappa works with nominal variables. It does not give weight to partial agreement like weighted kappa. There are methods that work with ordinal variables and partial agreement with three or more judges, but they extend beyond the scope of this module. Search for Kendall‚Äôs Coefficient of Concordance if you are interested.\nLike all other kappa values, Fleiss‚Äô kappa removes chance agreement. Because the method is unweighted and gives out no partial agreement, we‚Äôll use the outcome variable for our analysis.\nFleiss‚Äôs kappa makes a few assumptions about the data. They are similar to the assumptions made by weighted kappa and Cohen‚Äôs kappa, but not exactly the same.\n\nEach of the raters are independent.\nThe raters are selecting from the same defined categories of a categorical variable.\n\nWe‚Äôve selected a new set of three judges from our data set that judged lots of fights together. Cartlidge, Collett, and Lethaby judged 96 fights that went to a decision together.\nWith three or more judges, it becomes difficult to observe the data using a contingency table.\nBelow is a table of each judge‚Äôs verdict for the 96 fights. We‚Äôve created three columns on the right to help summarize the judge‚Äôs votes. They sum up the total number of verdicts of that type for each fight.\n\n\nfightCartlidgeCollettLethabyfighter2drawfighter11fighter2fighter2fighter23002fighter1fighter1fighter10033fighter1fighter1fighter10034fighter2fighter2fighter23005fighter2fighter2fighter23006fighter1fighter1fighter10037fighter1fighter1fighter10038fighter1fighter1fighter10039fighter1fighter1fighter100310fighter1fighter2fighter220111fighter1fighter1fighter100312fighter2fighter2fighter230013fighter1fighter1fighter100314fighter2fighter2fighter230015fighter1fighter1fighter1003\n\n\n\nExercise 8: Cartlidge, Collett, and Lethaby\n8.1. Take a look at the table. How often do the judges agree?\n\n All but one fight About half of the fights Very rarely (less than 20 percent)\n\n\n\nCalculating and Interpreting Fleiss‚Äô Kappa\nAs with the other kappas, we begin by calculating the the proportion of observed agreement \\((p_{o})\\) and proportion of expected agreement \\((p_{e})\\). However, for Fleiss‚Äô kappa, they are calculated in more complex ways.\nThis makes sense. As we add more judges, we have so many more levels of agreement. For example, if Collett and Lethaby agree, but Cartlidge disagrees (like fight 10 in our data above), this is still better agreement than if all three judges give different verdicts. These options are only magnified if we were to consider sets of four or more judges or events with four of more different outcomes.\nThe proportion of observed agreement is calculated by a long formula:\n\n\\(p_{o} = \\displaystyle \\frac{1}{N * n * (n - 1)} (\\sum_{i=1}^{N} \\sum_{j=1}^{k}n^{2}_{ij} - N * n)\\)\n\nwhere N is the number of observations and n is the number of raters.\nFor our example, N = 96 and n = 3.\nYou won‚Äôt have to calculate it by hand, and in this case intuition for the formula is not easy and beyond the scope of the module.\nWe show the calculation using this formula for the proportion of observed agreement for our set of Cartlidge, Collett, and Lethaby:\n\n\\(p_{o} = \\displaystyle \\frac{1}{96 * 3 * (3 - 1)} (3^{2} + 0^{2} + 0^{2} + ... + 1^{2} - 96 * 3)\\)\n\n(Optional) Take a moment if you are interested to see how we entered the values into the formula.\nAfter evaluating, we end up with a \\(p_{o}\\) = 0.882. This is our total observed agreement. It includes both real agreement and chance agreement.\nThe proportion of expected agreement is computed by a much less complex formula.\n\n\\(p_{e} = \\sum p_{j}^{2}\\)\n\nWe calculate the frequency (or expected rate) for each of the three categories \\((p_{j})\\), square them, and add them all together. This is like finding the concordant values with two judges. We‚Äôre finding the probability that the selections appear together randomly.\n\n\\(p_{fighter1}\\) = 0.524\n\\(p_{draw}\\) = 0.017\n\\(p_{fighter2}\\) = 0.458\n\nIf we square these frequencies and sum them up, we‚Äôll find that \\(p_{e}\\) = 0.485.\nThis means that if the three judges were to issue random verdicts without watching the fights or consulting with each other, we‚Äôd expect the three of them to agree about 48.5% of the time.\nWe can solve for Fleiss‚Äô kappa \\((\\kappa)\\) with the same formula as the weighted and unweighted kappa values.\n\n\\(kappa(\\kappa) = \\displaystyle \\frac{p_{o} - p_{e}}{1 - p_{e}}\\).\n\nFleiss‚Äô kappa for Cartlidge, Collett, and Lethaby is 0.771.\nOur interpretation for the Fleiss‚Äô kappa is identical to that of the weighted and unweighted kappa. Below is a reminder:\n\n\\(\\kappa &gt; 0.75\\) Excellent reproducibility\n\\(0.4 \\le \\kappa \\le 0.75\\) Good reproducibility\n\\(0 \\le \\kappa &lt; 0.4\\) Marginal reproducibility\n\nWe can claim that Cartlidge, Collett, and Lethaby have excellent reproducibility in their judgments. This means they are likely to evaluate the fights in similar ways, and if we substituted one for another, we would not expect exceedingly different results. About 23% of the data is a result of chance agreement or disagreement. Once again, this cannot prove that the three of them are good at selecting the correct victor, only that they are likely to select similar victors.\nAs with the other kappa values, we can calculate a confidence interval. Our 95% confidence interval for Fleiss‚Äô kappa is 0.661 to 0.881. Thus, with 95% confidence, we can claim that the interval includes the true value of Fleiss‚Äô kappa. This interval does not include 0, so we can conclude with at least 95% confidence that there is some real association between the three judges. The lower end of the confidence interval is 0.661, which would be in the upper portion of the ‚Äúgood reproducibility‚Äù bracket.\nFleiss‚Äô kappa does afford us an extra piece of analysis. We can look at the individual kappas for each of the categories to assess the level of agreement across their verdicts. This can help us to break down our kappa into simpler results that assess raters reliability on only one category.\nThis can be especially helpful for certain tests of reliability. For example, a survey evaluating the inter-rater-reliability of several doctors prescribing or diagnosing patients would immensely benefit by seeing which prescriptions or diagnoses the doctors are most and least consistent in the ratings.\nFor our data, we‚Äôll look at the individual kappas for the categories: fighter1, draw, and fighter2.\n\n\nCategoryKappazp.valuedraw0.1863.1540.002fighter10.79113.4270.000fighter20.79013.4100.000\n\n\nFighter 1 and fighter 2 are arbitrary assignments, so it is fitting that their values are almost identical. Their difference would not tell us anything meaningful regardless. However, the individual kappa of the draw category is much smaller than the others. This demonstrates that the judges have a much lower level of agreement when issuing draws than when they select a fighter.\nThis makes contextual sense. Draws are unlikely and less desirable. Collett, Cartlidge, and Lethaby never put forth a unanimous draw, and they rarely even had two of three vote draw.\n\n\nExercise 9: Other Judges\nWe‚Äôll provide a second example using the judge combination of Cartlidge, Sledge, and Lethaby. We‚Äôll ask you some general questions to help guide your analysis.\n\n\njudge1judge2judge3fightsCartlidgeCollettLethaby96CartlidgeLethabySledge36ChampionDivilbissGraham33ClearyD'AmatoLee24CrosbyD'AmatoValel22ClearyD'AmatoKamijo18CartlidgeLethabyOglesby14Col√≥nTirelliUrso14GuearyMillerSwanberg14MathisenSutherlandTurnage14\n\n\n\n\nfightCartlidgeLethabySledgefighter2drawfighter11fighter2fighter2fighter23002fighter2fighter2fighter23003fighter1fighter2fighter22014fighter2fighter2fighter23005fighter1fighter1fighter10036fighter2fighter2fighter23007fighter2fighter2fighter23008fighter1fighter1fighter10039fighter1fighter2fighter220110fighter1fighter1fighter100311fighter1fighter1fighter100312fighter1fighter1fighter100313fighter2fighter2fighter230014fighter2fighter2fighter230015fighter2fighter1fighter1102\n\n\n9.1. Take a look at the table. How often do the judges agree?\n\n Most fights (12 of 15) Very rarely (less than 20 percent) About half of the fights\n\n9.2. Does one judge tend to differ more?\n\n Yes, Cartlidge is the only one to disagree Yes, Sledge disagrees the most No, they all disagree at least one time\n\nResults\n\nProportion of Observed Agreement: 0.926\nProportion of Expected Agreement: 0.529\nFleiss‚Äô kappa: 0.843\n95% Confidence Interval for kappa: 0.843 and 0.654\n\nTable of Individual kappas:\nCategoryKappazp.valuefighter10.8438.7580fighter20.8438.7580\n9.3. Based on the results, what is the percent disagreement that is attributable to chance?\n\n Most (about 84 percent) About two thirds (65 percent) Just over half (53 percent)\n\n9.4. How good is the estimated reproducibility for the three judges?\n\n Good, higher than chance agreement (84 percent) Marginal, not much better than chance agreement (65 percent) Excellent, well above chance agreement (over 90 percent)\n\n9.5. (Optional advanced) Assess the confidence interval. Can you reject a null hypothesis of excellent reproducibility?\n\n Cannot determine from the information provided. Yes, we reject that kappa is equal to 0 No, the interval includes the possiblity of a value that is only good",
    "crumbs": [
      "Home",
      "Mixed Martial Arts",
      "MMA Inter-rater Reliability Data Analysis"
    ]
  },
  {
    "objectID": "marathons/marathon-records/index.html",
    "href": "marathons/marathon-records/index.html",
    "title": "Marathon Record-Setting Over Time",
    "section": "",
    "text": "https://isle.stat.cmu.edu/SCORE/Marathons_SCORE_Template/",
    "crumbs": [
      "Home",
      "Marathons",
      "Marathon Record-Setting Over Time"
    ]
  },
  {
    "objectID": "marathons/marathon-records/index.html#module",
    "href": "marathons/marathon-records/index.html#module",
    "title": "Marathon Record-Setting Over Time",
    "section": "",
    "text": "https://isle.stat.cmu.edu/SCORE/Marathons_SCORE_Template/",
    "crumbs": [
      "Home",
      "Marathons",
      "Marathon Record-Setting Over Time"
    ]
  },
  {
    "objectID": "marathons/boston-marathon-finish_times-2023/index.html",
    "href": "marathons/boston-marathon-finish_times-2023/index.html",
    "title": "2023 Boston Marathon - Variability in Finish Times",
    "section": "",
    "text": "Welcome video\n\n\n\n\nIntroduction\nFor this activity, you will be exploring the result times from female and male runners that finished the 2023 Boston Marathon.\nIn particular, you will examine both visualizations and summary statistics of result times to explore the variation in finish times as well as use comparative techniques, such as z-scores, to compare and contrast male and female participants.\nInvestigating these trends is useful for several reasons. Firstly, exploring these trends can help to deepen our understanding of how different factors, such as gender, impact marathon performances. Secondly, analyzing the distribution of finish times and the performance of top finishers against the masses provides insights into the competitive landscape of the marathon. It can identify outliers or exceptional performances and understand how elite athletes compare to average participants. Although not directly connected to this data, analyses like these can inform training strategies, highlight the effectiveness of different preparation methods, and inspire both new and experienced runners by showcasing the range of achievable performances.\n\n\n\n\n\n\nNoteActivity Length\n\n\n\n\n\nThis activity would be suitable for an in-class example or quiz.\n\n\n\n\n\n\n\n\n\nNoteLearning Objectives\n\n\n\n\n\nBy the end of the activity, you will be able to:\n\nAnalyze distributions using histograms\nIdentify potential confounding variables to explain bimodal data\nCompare and contrast distributions for a pair of groups\nCalculate and compare z-scores for individual cases\n\n\n\n\n\n\n\n\n\n\nNoteMethods\n\n\n\n\n\nFor this activity, students will primarily use basic concepts of histograms and summary statistics to analyze distributions. Students will also likely require knowledge of z-scores.\n\n\n\n\n\n\n\n\n\nNoteTechnology Requiremens\n\n\n\n\n\nThe provided worksheets do not require any specific statistical software. (Although they will likely require access to a calculator.)\nSince the data are provided, instructors are encouraged to modify the worksheets to have student construct visualizations and calculate summary statistics using whichever software they choose.\n\n\n\n\n\nData\nThe data set contains 26598 rows and 15 columns. Each row represents a runner who completed the Boston Marathon in 2023\nDownload data:\nAvailable on the SCORE Data Repository: boston_marathon_2023.csv\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nage_group\nage group of the runner\n\n\nplace_overall\nfinishing place of the runner out of all runners\n\n\nplace_gender\nfinishing place of runner among the same gender\n\n\nplace_division\nfinishing place of runner among runners of the same gender and age group\n\n\nname\nname of runner\n\n\ngender\ngender of runner\n\n\nteam\nteam the runner is affiliated with\n\n\nbib_number\nbib number of runner\n\n\nhalf_time\nhalf marathon time of runner\n\n\nfinish_net\nfinishing time timed from when they cross the starting gate\n\n\nfinish_gun\nfinishing time of runner timed from when the starter gun is fired\n\n\nhalf_time_sec\nhalf marathon time in seconds\n\n\nfinish_net_sec\nnet finish in seconds\n\n\nfinish_gun_sec\ngun finish in seconds\n\n\nfinish_net_minutes\nnet finish in minutes\n\n\n\nData Source\nBoston Athletic Association\n\n\n\nMaterials\n\nWe provide editable MS Word handouts along with their solutions.\n\nClass handout\n\n\nClass handout - with solutions\n\n\n\n\n\n\n\n\nNoteConclusion\n\n\n\n\n\nIn conclusion, the Boston Marathon Times worksheet provides valuable learning opportunities for students in several key areas. It allows them to understand reasons by variability might exist and to discover multimodal distributions can occur simply due to excluding an important explanatory variable that otherwise confounds the analysis. The calculation of z-scores or other similar measurement of relative location enables students to compare and contrast the remarkable achievements of the top female and male finishers, shedding light on their talent in their respective fields. Overall, this worksheet allows students to critically analyze the 2023 marathon result data and draw meaningful conclusions about the extraordinary performances of athletes in the race.",
    "crumbs": [
      "Home",
      "Marathons",
      "2023 Boston Marathon - Variability in Finish Times"
    ]
  },
  {
    "objectID": "lacrosse/index.html",
    "href": "lacrosse/index.html",
    "title": "Lacrosse",
    "section": "",
    "text": "These modules use lacrosse data to teach topics in statistics and data science.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLacrosse Faceoff Proportions\n\n\n\nHypothesis testing\n\nSingle proportion\n\n\n\nUsing data from NCAA Div I lacrosse teams to explore the importance of winning faceoffs\n\n\n\n\n\nFeb 5, 2024\n\n\nJack Fay, Ivan Ramler, A.J. Dykstra\n\n\n\n\n\n\n\n\n\n\n\n\nLacrosse PLL vs.¬†NLL\n\n\n\nDifference in two means\n\n\n\nComparing scoring rates between indoor and outdoor profesional lacrosse leagues.\n\n\n\n\n\nFeb 5, 2024\n\n\nJack Cowan, Ivan Ramler, A.J. Dykstra, Robin Lock\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Lacrosse"
    ]
  },
  {
    "objectID": "hockey/nhl-shooting-percentage-ventura/index.html",
    "href": "hockey/nhl-shooting-percentage-ventura/index.html",
    "title": "Predicting NHL Shooting Percentages",
    "section": "",
    "text": "https://isle.stat.cmu.edu/SCORE/NHLShots/",
    "crumbs": [
      "Home",
      "Hockey",
      "Predicting NHL Shooting Percentages"
    ]
  },
  {
    "objectID": "hockey/nhl-shooting-percentage-ventura/index.html#module",
    "href": "hockey/nhl-shooting-percentage-ventura/index.html#module",
    "title": "Predicting NHL Shooting Percentages",
    "section": "",
    "text": "https://isle.stat.cmu.edu/SCORE/NHLShots/",
    "crumbs": [
      "Home",
      "Hockey",
      "Predicting NHL Shooting Percentages"
    ]
  },
  {
    "objectID": "hockey/nhl-shooting-percentage-ventura/index.html#how-to-cite",
    "href": "hockey/nhl-shooting-percentage-ventura/index.html#how-to-cite",
    "title": "Predicting NHL Shooting Percentages",
    "section": "How to Cite",
    "text": "How to Cite\nIf you use this module in your work, please cite it as follows:\nSchuckers, M., Macdonald, B., & Ventura, S. (2025, April 30). Hockey Regression. ‚ÄúThe SCORE Network.‚Äù https://doi.org/10.17605/OSF.IO/YUX6T\nYou can include this citation directly in your references or bibliography.",
    "crumbs": [
      "Home",
      "Hockey",
      "Predicting NHL Shooting Percentages"
    ]
  },
  {
    "objectID": "golf/pga_masters_nor/index.html",
    "href": "golf/pga_masters_nor/index.html",
    "title": "PGA - Scoring Average Confidence Intervals (No R)",
    "section": "",
    "text": "NoteFacilitation notes\n\n\n\n\n\n\nThis module would be suitable for an in-class lab or take-home assignment in an introductory statistics course.\nThe data used to create this module and a student answer template can be downloaded from the following links:\n\nData\nStudent answer template\n\nThe data for this module is derived largely from the ESPN website. However, the tours were manually added to the data based on the what they played on in 2023 at the time of the Masters.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals (No R)"
    ]
  },
  {
    "objectID": "golf/pga_masters_nor/index.html#terms-to-know",
    "href": "golf/pga_masters_nor/index.html#terms-to-know",
    "title": "PGA - Scoring Average Confidence Intervals (No R)",
    "section": "Terms to know",
    "text": "Terms to know\nBefore proceeding with the analysis, let‚Äôs make sure we know some important golf terminology that will help us master this lab.\n\nGolf Terminology\n\nPar in golf is the amount of strokes that a good golfer is expected to take to get the ball in the hole.\n\nEach hole in golf has its own par. There are par 3 holes, par 4 holes, and par 5 holes.\nThere are 18 holes on a golf course and the pars of each of these holes sums to par for the course, also known the course par.\n\nA round in golf is when a golfer plays the full set of 18 holes on the course.\n\nIn most professional golf tournaments, all golfers play 2 rounds, the best golfers are selected and those golfers play 2 more rounds for a total of 4 rounds.\n\n\n\n\n\n\n\n\nImportantTypes of Golf Tours\n\n\n\n\nIn golf there are a few tours, better thought of as leagues, that golfers regularly compete in\n\nThe PGA Tour, or ‚ÄúProfessional Golf Association Tour‚Äù, has long been considered the preeminant golfing tour, hosting most tournaments and containing the most skilled members.\nThe LIV tour is a Saudi-backed alternative to the PGA that played its first season in 2022. LIV is the Roman numeral for 54 and is related to the fact that LIV tournaments only allow 54 players and only play 54 holes, compared to the normal PGA Tour 72 holes.\nThe PGA Tour Champions is a branch off of the PGA tour for players 50 or older. It used to be called the ‚ÄúSenior PGA Tour‚Äù until 2003, when it began being called the Champions tour\nAn amateur is a golfer who is not yet a professional. They are not allowed to win money in professional golf tournaments. Most amateurs are college golfers.\n\n\n\n\n\n\nPGA Tour vs.¬†LIV Golf\nClick here to read about LIV golf‚Äôs founding and its continued impact on the PGA tour.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals (No R)"
    ]
  },
  {
    "objectID": "golf/pga_masters_nor/index.html#t-interval-for-single-means",
    "href": "golf/pga_masters_nor/index.html#t-interval-for-single-means",
    "title": "PGA - Scoring Average Confidence Intervals (No R)",
    "section": "t-interval for single means",
    "text": "t-interval for single means\nA t-distribution is used for calculating a single mean confidence interval if the sample size is small (rule of thumb: less than 30) and the population standard deviation is unknown.\nThe formula for calculating a CI using this method is shown below: \\[CI = \\bar{x} \\pm t_{\\alpha/2, df} \\times \\frac{s}{\\sqrt{n}}\\]\n\n\nClick here to learn more about the t-distribution and play around with t-distribution graphs.\nWhere \\(\\bar{X}\\) is the sample mean,\n\\(t_{\\alpha/2, df}\\) is the critical value for the t-distribution with \\(df = n-1\\),\n\\(S\\) is the sample standard deviation,\nand \\(n\\) is the sample size.\n\n\nNOTE: The t-distribution changes based on the degrees of freedom, approaching the normal distribution as the degrees of freedom increase.\nThe critical value for the t-distribution is determined by the confidence level and the degrees of freedom. This is calculated by finding the value of \\(t_{\\alpha/2, df}\\) such that the area under the t-distribution curve (with that specific degrees of freedom) between \\(-t_{\\alpha/2, df}\\) and \\(t_{\\alpha/2, df}\\) is equal to the confidence level.\n\n\n\n\n\n\n\n\n\nThese critical values can be found using t-tables like the one below. Some important things to note about this specific t-table. The degrees of freedom for the test are on the left. The row cum. prob at the top shows us the \\(t_{\\alpha}\\) value. The one-tail row shows us the p-value for a one-tailed test and the row two-tail shows us the p-value for a two-tailed test. There is also a helpful row at the bottom called confidence level that shows the associated confidence level for confidence intervals. The values in the table show the t-test statistic values corresponding to the degrees of freedom and \\(t_{alpha}\\).\n\nFor a 95% confidence interval, our Type I error rate is \\(\\alpha = 0.05\\). Since confidence intervals are two-tailed, we split this \\(\\alpha\\) in half, half in the left tail and half in the right tail. This means that we are looking for \\(t_{.025, df}\\) or \\(t_{.975, df}\\).\n\n\n\n\n\n\nNoteExample 1: t-distribution confidence intervals\n\n\n\n\n\nWe would like to make a 90% confidence interval for the true mean of scoring for Jon Rahm at the Masters. His scores were as follows.\n\n\n\n\n\nGolfer\nRound\nScore\n\n\n\n\nJon Rahm\n1\n65\n\n\nJon Rahm\n2\n69\n\n\nJon Rahm\n3\n73\n\n\nJon Rahm\n4\n69\n\n\n\n\n\nWe start by calculating the the sample mean (\\(\\bar{x}\\)).\n\\[\n\\begin{align*}\n\\bar{x} &= \\frac{65 + 69 + 73 + 69}{4} \\\\\n        &= 69\n\\end{align*}\n\\]\nNext we calculate the sample standard deviation (\\(s\\)).\n\\[\n\\begin{align*}\ns &= {\\sqrt {\\frac {\\sum _{i=1}^{n}(x_{i}-{\\overline {x}})^{2}}{n-1}}} \\\\\n  &= {\\sqrt {\\frac{(65 - 69)^2 + (69 - 69)^2 + (73 - 69)^2 + (69 - 69)^2}{4-1}}} \\\\\n  &= {\\sqrt {\\frac{(-4)^2 + 0^2 + 4^2 + 0^2}{3}}} \\\\\n  &\\approx 3.266\n\\end{align*}\n\\]\nOur next step is to find the critical value for a 90% confidence interval with 3 degrees of freedom (\\(t_{\\alpha/2, df}\\). We‚Äôll use the t-table from earlier to find this.\n\nLastly we will use the confidence interval formula to find the bounds of our 90% confidence interval. \\[\n\\begin{align*}\nCI &= \\bar{x} \\pm t_{\\alpha/2, df} \\times \\frac{s}{\\sqrt{n}} \\\\\nCI &= 69 \\pm 2.353 \\times \\frac{3.266}{\\sqrt{4}} \\\\\nCI &= (67.07878, 70.92122)\n\\end{align*}\n\\] Finally we can interpret the confidence interval and say that we are 95% confident that the true mean of Jon Rahm‚Äôs scores at Augusta National is between 67.1 and 70.9.\n\n\n\n\n\n\n\n\n\nNoteExercise 1: t-distribution confidence intervals\n\n\n\nThe table below shows summary data for the first round of the 2023 Masters tournament for amateur golfers. The critical value is for a 90% confidence interval with 6 degrees of freedom.\n\n\n\n\n\n\n\n\n\n\n\n\nSample\nSample Mean\nSample Standard Deviation\nSample Size\nCritical Value\n\n\n\n\nAmateurs Round 1\n75\n3.21455\n7\n1.94318\n\n\n\n\n\nUse the formula for creating a confidence interval using the t-distribution to calculate the upper and lower limits of a confidence interval for the true mean of amateur scoring using the amateurs in the first round as our sample. Use a 90% confidence interval (\\(\\alpha = .1\\)).\n\nWhat is the lower bound of the confidence interval?\nWhat is the upper bound of the confidence interval?\nWhat is the interpretation of this confidence interval?\n\n\n\nRepeat this process with a 99% confidence interval (\\(\\alpha = .01\\)).\n\nWhat is the new lower bound of the confidence interval?\nWhat is the new upper bound of the confidence interval?\nIs this 99% confidence interval larger, smaller, or the same as 90% confidence interval?\nWhen thinking about your anwer to f, what do you think could explain this?\n\n\n\nTIP: When interpreting a confidence interval do not say ‚Äúthere is a 90% chance that the true mean is between the lower and upper bounds‚Äù. Instead, say ‚Äúwe are 90% confident that the true mean is between the lower and upper bounds‚Äù.\n\nTIP: The t-table from earlier in the lesson will need to be used to find the critical value for the t-distribution for a 99% confidence interval with 6 degrees of freedom.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals (No R)"
    ]
  },
  {
    "objectID": "golf/pga_masters_nor/index.html#z-interval-for-single-means",
    "href": "golf/pga_masters_nor/index.html#z-interval-for-single-means",
    "title": "PGA - Scoring Average Confidence Intervals (No R)",
    "section": "z-interval for single means",
    "text": "z-interval for single means\nA standard normal distribution (also known as a z-distribution) is used to calculate the confidence interval for a single mean if the sample size is large enough (greater than 30) or the population standard deviation is known. The first case is common as oftentimes samples are greater than 30. The second case is rare because it is uncommon to know the population standard deviation but not the population mean.\nThe formula for the confidence interval for a single mean using the z-distribution is very similar to that of the t-distribution\n\n\nClick here for more information about the standard normal distribution.\n\\(CI = \\bar{X} \\pm Z_{\\alpha/2} \\times \\frac{\\sigma}{\\sqrt{n}}\\)\nWhere \\(\\bar{X}\\) is once again the sample mean, \\(Z_{\\alpha/2}\\) is the critical value for the standard normal distribution at the specified confidence level, \\({\\sigma}\\) is the population standard deviation, and \\(n\\) is the sample size.\nThe reasoning behind why we can use the standard normal distribution when the sample is greater than 30 even if the population standard deviation is unknown is found in the Central Limit Theorem, which says that as the sample size increases, the sampling distribution of the sample mean approaches a normal distribution. This means that when the sample size is greater than 30 we can use the sample standard deviation to estimate the population standard deviation and create a confidence interval as seen below\n\\(CI = \\bar{X} \\pm Z_{\\alpha/2} \\times \\frac{s}{\\sqrt{n}}\\)\nOnce again the critical value for the z-distribution is the value of \\(Z_{\\alpha/2}\\) such that the area under the standard normal distribution curve between \\(-Z_{\\alpha/2}\\) and \\(Z_{\\alpha/2}\\) is equal to the confidence level.\nThe critical values for the z-distribution can be found using a z-table such as the one below. All values in the table are the area under the standard normal curve to the left of the associated z-score. The column on the left shows the associated ones and tenths value for the z-score. The row at the top shows the associated hundredths value for the z-score.\n\n\n\n\n\n\n\nNoteExample 2: z-distribution confidence intervals\n\n\n\n\n\nWe would like to make a 98% confidence interval for the true mean of scoring for PGA Tour golfers based off of a sample from the third round. The sample includes the following scores.\n\n\nRound 3 PGA Tour golfer scores: 73 76 71 75 70 73 71 72 71 74 73 68 74 67 73 73 70 72 74 74 69 73 74 74 72 78 74 76 74 72 77 70 74 77 72 73 74 74 77\n\n\nWhew! That‚Äôs 39 observations we have in our sample, which means we can use the z-distribution.\nWe start by calculating the the sample mean (\\(\\bar{x}\\)). This probably has to be done using a calculator or programming tool. For this example our sample mean is approximately 73.03.\nNext we calculate the sample standard deviation (\\(s\\)), which is our estimate for \\(\\sigma\\). Once again, with a sample this size a calculator or computer is probably necessary for computing this. Our sample standard deviation is approximately 2.44.\nOur next step is to find the critical value for a 98% confidence interval for a z-distribution (\\(Z_{\\alpha/2}\\)). We will use the z-table from earlier. Since we want alpha equal to .2, our area to the right should be .1 and we look for the value on the table of .99.\n\nNote that this value is not exact. The value is slightly less than 2.33 but .9901 is closer to .99 than .9898, so we went with 2.33.\nLastly we will use the confidence interval formula to find the bounds for our 98% confidence interval. \\[\n\\begin{align*}\nCI &= \\bar{X} \\pm Z_{\\alpha/2} \\times \\frac{s}{\\sqrt{n}} \\\\\nCI &= 73.03 \\pm 2.33 \\times \\frac{2.44}{\\sqrt{39}} \\\\\nCI &= (72.12, 73.94)\n\\end{align*}\n\\] Finally we can interpret the confidence interval and say that we are 98% confident that the true mean of PGA Tour golfers‚Äô scores at the 2023 Masters is between 72.12 and 73.94.\n\n\n\n\n\n\n\n\n\nNoteExercise 2: z-distribution confidence intervals\n\n\n\nThe table below shows summary data for the first round of the 2023 Masters Tournament for PGA professional golfers. The critical value is for a 95% confidence interval.\n\n\n\n\n\n\n\n\n\n\n\n\nSample\nSample Mean\nStandard Deviation Estimate\nSample Size\nCritical Value\n\n\n\n\nPGA Round 1\n71.54545\n3.023477\n55\n1.959964\n\n\n\n\n\nUse the formula for creating a confidence interval using the standard normal distribution to calculate the upper and lower limits of a confidence interval for the true mean of PGA professional scoring at Augusta using the PGA pros in the first round as our sample. Use a 95% confidence interval (\\(\\alpha = .05\\)).\n\nWhy can we use the standard normal distribution to calculate this confidence interval?\nWhat is the confidence interval for the true mean of scoring for PGA professionals at Augusta National using round 1 as our sample?\nWhat is the interpretation of this confidence interval?",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals (No R)"
    ]
  },
  {
    "objectID": "golf/pga_masters_nor/index.html#test-statistics",
    "href": "golf/pga_masters_nor/index.html#test-statistics",
    "title": "PGA - Scoring Average Confidence Intervals (No R)",
    "section": "Test Statistics",
    "text": "Test Statistics\nLike confidence intervals, we have two different tests for hypothesis testing for the population mean. Remember that if the population standard deviation is unknown and the sample size is less than 30, we use the t-distribution. If the population standard deviation is known or the sample size is greater than 30, we use the standard normal distribution.\nEach of these distributions have their own tests, the t-test and the z-test. This means that we have different test statistics to calculate depending on the situation.\n\nt-test\nThe t-test statistic is calculated using the formula:\n\\[t = \\frac{\\bar{x} - \\mu_0}{\\frac{s}{\\sqrt{n}}}\\]\nwhere \\(\\bar{x}\\) is the sample mean, \\(\\mu_0\\) is the hypothesized population mean, \\(s\\) is the sample standard deviation, and \\(n\\) is the sample size.\n\n\nz-test\nThe z-test statistic is calculated using the formula:\n\\[z = \\frac{\\bar{x} - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}\\]\nwhere \\(\\bar{x}\\) is the sample mean, \\(\\mu_0\\) is the hypothesized population mean, \\(\\sigma\\) is the population standard deviation, and \\(n\\) is the sample size.\nOnce again, if the sample size is over 30 and the population standard deviation is unknown, we use the sample standard deviation to approximate the population standard deviation.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals (No R)"
    ]
  },
  {
    "objectID": "golf/pga_masters_nor/index.html#to-reject-or-fail-to-reject",
    "href": "golf/pga_masters_nor/index.html#to-reject-or-fail-to-reject",
    "title": "PGA - Scoring Average Confidence Intervals (No R)",
    "section": "To Reject or Fail to Reject",
    "text": "To Reject or Fail to Reject\nThere are two ways to make a decision about the null hypothesis.\nMethod 1: Critical values, along with test statistics, can be used to determine if the hypothesized population mean is within the confidence interval for the true mean.\nA critical value is a value that separates the rejection region from the non-rejection region. The rejection region is the area where the null hypothesis is rejected. The non-rejection region is the area where the null hypothesis is not rejected. The critical value is determined by the significance level (\\(\\alpha\\)) and the degrees of freedom (if it is a t-test). The critical value is compared to the test statistic to determine if the null hypothesis should be rejected. If the test statistic is within the non-rejection region, the null hypothesis is not rejected. If the test statistic is within the rejection region, the null hypothesis is rejected and the alternative hypothesis is accepted.\nBelow is an example of using critical values and a test-statistic for a z-test with a 95% confidence level (two-sided). The critical value is 1.96. This means that if the test statistic is greater than 1.96 or less than -1.96, the null hypothesis is rejected. The blue represents the non-rejection region and the red the rejection region. Since the test statistic for this example is 1.1 (less than 1.96 and greater than -1.96), we fail to reject the null hypothesis.\n\n\n\n\n\n\n\n\n\nThis method corresponds directly to the related confidence intervals produced for the sample data.\nIf the hypothesized population mean is within the confidence interval, we fail to reject the null hypothesis. If the hypothesized population mean is not within the confidence interval, the null hypothesis is rejected and the alternative hypothesis is accepted.\n\n\nNote: We can say that there is significant evidence to accept the alternative hypothesis if the null hypothesis is rejected. However, it should never be said that we accept the null hypothesis. We can only fail to reject it.\nMethod 2: The second method is to use a p-value. The p-value is the probability of observing a test statistic as extreme as the one calculated from the sample data given that the null hypothesis is true. The p-value is compared to the significance level (\\(\\alpha\\)) to determine if the null hypothesis should be rejected. If the p-value is less than \\(\\alpha\\), the null hypothesis is rejected. If the p-value is greater than \\(\\alpha\\), the null hypothesis is not rejected.\n\n\nNOTE: Our alternative hypothesis determines whether we are looking for the probability that the test statistic is greater than or less than the observed value.\n\nFor a two-sided test, the p-value is the probability that the test statistic is greater than the observed value or less than the negative of the observed value. Find the area in one of the tails and double it.\nFor a left-tailed test (\\(H_a: \\mu &lt; \\mu_0\\)), the p-value is the probability that the test statistic is less than the observed value.\nFor a right-tailed test (\\(H_a: \\mu &gt; \\mu_0\\)), the p-value is the probability that the test statistic is greater than the observed value.\n\nFirst let‚Äôs visualize what a p-value is telling us.\n\n\n\n\n\n\n\n\n\nNow let‚Äôs see how we can calculate p-value for our example using a z-table.\nWe have a hypothetical z-test statistic of 1.1 and alpha of .05 for a two-tailed test. We start by finding the area under the curve to the left of a z-score of 1.1. However, we know that we want the area under the curve greater than 1.1 or less than -1.1. We can find the area under the curve to the right of 1.1 by subtracting our area value from 1. Additionally, we know that the z-distribution is symmetrical, implying that the area under the curve less than -1.1 is equal to that which is greater than 1.1. This means that we multiply our area value by 2 to get the p-value.\n\nThis means we can calculate p-value with the math below.\n\\[\n\\begin{align*}\np\\text{-}value &= 2 * (1 - .8643)\\\\\n               &= .2714\n\\end{align*}\n\\]\nSince the p-value is greater than our alpha value of .05 we fail to reject the null hypothesis.\n\n\n\n\n\n\nNoteExample 3: Hypothesis testing example\n\n\n\n\n\nSuppose we have a sample of data with the following values: 10, 20, 30, 40, 50, 60, 70, 80, 90, 100. We want to test if the true mean of the population is not equal to 50. Alpha is 0.05. Our hypotheses are as follows:\n\nNull Hypothesis (\\(H_0\\)): \\(\\mu = 50\\)\nAlternative Hypothesis (\\(H_1\\)): \\(\\mu \\neq 50\\)\n\nWe should use the t-test to test this hypothesis because the sample size is small and the population standard deviation is unknown.\nWe start by finding the sample mean, sample standard deviation, and degrees of freedom. Using a calculator we can find \\(\\bar{x} = 55\\), \\(s = 30.2765\\), and logically \\(df = 9\\).\nNow we need to find the critical value for a 95% two sided confidence t-interval with 9 degrees of freedom (\\(t_{\\alpha/2, df}\\)). Once again we will use the t-table from earlier.\n\nThe critical value is \\(\\pm 2.262\\).\nNext the test statistic can be calculated as follows:\n\\[t = \\frac{\\bar{x} - \\mu_0}{\\frac{s}{\\sqrt{n}}}\\] \\[t = \\frac{55 - 50}{\\frac{30.2765}{\\sqrt{10}}}\\] \\[t = 0.522\\]\nThis is visualized in the plot below.\n\n\n\n\n\n\n\n\n\nSince the test statistic is between the two critical values, there is not enough evidence to reject the null hypothesis and conclude that the true mean is not equal to 50.\nThe p-value for this example could be calculated by finding the area under the curve to the left of -0.522 and to the right of 0.522. The p-value would be the sum of these two areas.\nThis value can be estimated by looking at the t-table. Obviously .522 is not on the table, but is between 0 and .703. If we follow this up to the top of the table we can look at the two-tails column and see that the p-value would be between .5 and 1. This is significantly larger than .05. Therefore we fail to reject the null hypothesis.\n\n\n\n\n\n\nNOTE: Since this is a two-sided test you could simply find the probability that the test statistic is greater than 0.522 and multiply by 2. This would give you the p-value.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals (No R)"
    ]
  },
  {
    "objectID": "golf/pga_masters_nor/index.html#hypothesizing-par-as-the-population-mean",
    "href": "golf/pga_masters_nor/index.html#hypothesizing-par-as-the-population-mean",
    "title": "PGA - Scoring Average Confidence Intervals (No R)",
    "section": "Hypothesizing Par as the Population Mean",
    "text": "Hypothesizing Par as the Population Mean\n\n\nAugusta National is breathtakingly beautiful, but if golfers get distracted by the scenic views, tall pines, bunkers, water, and azaleas may catch their balls.\n\n\n\nAugusta Hole 13\n\n\nImage Source: Your Golf Travel, CC 4.0\nIn golf par is considered to be the number of strokes a good golfer is expected to take. The par for the course at Augusta National is 72. It is known that Augusta National is a tougher than usual course, but we would like to test if that is the case for different groups of golfers.\nOur null hypothesis will generally be that the true mean of the group is equal to 72.\n\n\n\n\n\n\nNoteExercise 3: t-test for single mean hypothesis testing\n\n\n\n\nAmateurs, who are not yet professional golfers, are generally expected to score higher than professionals. We would like to test if the mean of amateur scoring is above par at Augusta National\n\nWhat is the null hypothesis for this test?\nWhat is the alternative hypothesis for this test?\n\nPerform a single mean hypothesis test to see if the mean of amateur scoring is greater than 72 using the amateur round 1 score sample from earlier in the lesson. The summary of the sample is as follows:\n\n\n\n\n\nSample\nSample Mean\nSample Standard Deviation\nSample Size\n\n\n\n\nAmateurs Round 1\n75\n3.21455\n7\n\n\n\n\n\n\nWhat is the critical value for this test?\nWhat is the test statistic?\nWhat is the p-value?\nBased on the p-value and the test statistic to critical value comparison, is there statistically significant evidence that the mean of amateur scoring at Augusta National is greater than 72?\n\n\n\n\nAmateurs generally struggle in the Masters, but in 2023 Sam Bennett, a Texas A&M student, made the cut and finished 16th. However, due to his amateur status, he was not eligible to win money and missed out on $261,000.\n\n\n\n\n\n\nNoteExercise 4: z-test for single mean hypothesis testing\n\n\n\nPGA professionals would generally average somewhere around par. We would like to test if the mean of PGA professional scoring is not equal to 72 at Augusta National.\n\nWhat is the null hypothesis for this test?\nWhat is the alternative hypothesis for this test?\n\nPerform a single mean hypothesis test using the PGA Round 1 sample from earlier. The summary of that sample is displayed below.\n\n\n\n\n\nSample\nSample Mean\nSample Standard Deviation\nSample Size\n\n\n\n\nPGA Round 1\n71.54545\n3.023477\n55\n\n\n\n\n\n\nWhat is the critical value for this test?\nWhat is the test statistic?\nWhat is the p-value?\nWhat is your conclusion?\nExplain your answer to part d.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals (No R)"
    ]
  },
  {
    "objectID": "golf/pga_drive_show_putt_dough/index.html",
    "href": "golf/pga_drive_show_putt_dough/index.html",
    "title": "PGA - Drive for Show, Putt for Dough?",
    "section": "",
    "text": "Please note that these material have not yet completed the required pedagogical and industry peer-reviews to become a published module on the SCORE Network. However, instructors are still welcome to use these materials if they are so inclined. (Revised October 2025)\n\n\nA common expression among golfers is ‚ÄúDrive for show, putt for dough.‚Äù This implies that the long initial tee shots (drives) on each hole are impressive, but the real key to success is the final strokes rolling the ball along the green into the hole (putts). Do data support this adage?\nThe dataset for this activity was obtained from the PGA Statistics Website. Cases include all golfers who made the cut in each of 19 PGA tournaments in 2022. The dataset includes variables for driving ability, putting ability, and measuring success in the tournament. The ‚Äúdriving‚Äù variables include average driving distance (avgDriveDist), driving accuracy percentage (drivePct), and strokes gained off the tee (driveSG). The ‚Äúputting‚Äù variables are average putts per round (avgPuttsPerRound), one putt percentage (onePuttPct), and strokes gained putting (puttsSG). The variables to measure ‚Äúsuccess‚Äù are scoring average (avgScore), official money won (Money), and Fedex Cup points (Points).\nExample of ‚Äústrokes gained‚Äù: Suppose that a golfer is on the green with the ball 10 feet from the hole and PGA players use 1.6 putts, on average, to finish the hole from this distance. If the golfer makes the 10-foot putt, the strokes gained is 1.6-1=0.6, but a miss (and two putts) gives 1.6-2=-0.4 strokes ‚Äúgained‚Äù (which is is actually a ‚Äúloss‚Äù).\n\n\n\n\n\n\nNoteActivity Length\n\n\n\n\n\nThe ‚ÄúNo Tech‚Äù version of this activity (where the correlation matrix is provided) should take less than 15 minutes to complete in class. Time might go up to half an hour if students are using technology to compute correlations and draw graphs.\n\n\n\n\n\n\n\n\n\nNoteLearning Objectives\n\n\n\n\n\nThe learning objectives associated with this module are:\n\nStudents will be able to use correlation to measure the strength association between quantitative variables.\nStudents will be able to check regression model conditions.\nStudents will be able to compare correlations to assess which variables may be more strongly related.\n\n\n\n\n\n\n\n\n\n\nNoteMethods\n\n\n\n\n\nThis module requires students compare correlations between pairs of variables.\nTechnology requirement: Two handout activities accommodate different levels of available technology.\n\nThe ‚ÄúNo Tech version‚Äù provides a correlation matrix for students to use to find the required correlations to compare.\nThe ‚ÄúWith Tech‚Äù version provides the dataset and asks students to use technology to compute the needed correlations.\n\n\n\n\n\n\n\nEach row of data gives the measures for one golfer in one tournament. The dataset covers 19 PGA tournaments from the 2022 season with 1376 cases in all. Each tournament consists of four rounds of golf. Some golfers are eliminated after the first two (or sometimes three) rounds. Only players who competed in all four rounds (i.e.¬†those that made the ‚Äúcut‚Äù) are included in this dataset. Some golfers (for example amateurs) may be missing values if they are not eligible for money won or Fedex Cup points.\nDownload data: PGA2022.csv\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nplayerName\nName of the player\n\n\ncountry\nThe country where the player is from\n\n\navgDriveDist\nAverage driving distance (in yards)\n\n\ndrivePct\nPercentage of times a tee shot comes to rest in the fairway\n\n\ndriveSG\nStrokes gained off the tee measures player performance off the tee on all par 4s and par 5s of how much better or worse a player‚Äôs drive is than the PGA Tour average. Values are average strokes gained per round.\n\n\navgPuttsPerRound\nAverage number of total putts per round\n\n\nonePuttPct\nPercentage of times it took one putt to get the ball into the hole\n\n\nputtsSG\nStrokes gained putting measures how many strokes a player gains or loses on the greens (on average per round) compared to average performance for PGA Tour golfers.\n\n\navgScore\nThe scoring average is the total strokes divided by the total rounds\n\n\nMoney\nThe official money is the prize money award to the PGA members\n\n\nPoints\nFedexCup Points awarded based on finish position in each tournament\n\n\nTournament\nThe tournament where the PGA Tour is taking place\n\n\n\nData Source\nThe data were obtained from the PGA Statistics website\n\n\n\n\nThe data and worksheet associated with this module are available for download through the following links.\nPGA2022.csv - Dataset with driving, putting, and success measures for individual golfer in 19 PGA tournaments in 2022.\nPGACorrelationsNoTech.docx- ‚ÄúNo Tech‚Äù version of the activity worksheet provides a correlation matrix for students to use in answering the questions.\nPGACorrelationsWithTech.docx - ‚ÄúWith Tech‚Äù version of the activity worksheet assumes students have technology to compute any needed correlations from the provided dataset.\nSample solutions to the worksheets\nPGACorrelationsNoTech-Ans.docx- Sample solutions to the ‚ÄúNo Tech‚Äù version of the activity worksheet.\nPGACorrelationsWithTech-Ans.docx - Sample solutions to the ‚ÄúWith Tech‚Äù version of the activity worksheet.\n\n\n\n\n\n\nNoteConclusion\n\n\n\n\n\nStudents should find evidence to support the claim that. in general, putting statistics tend to be better predictors of tournament success than driving statistics. So golfers really do ‚Äúdrive for show, but putt for dough‚Äù.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Drive for Show, Putt for Dough?"
    ]
  },
  {
    "objectID": "golf/pga_drive_show_putt_dough/index.html#module",
    "href": "golf/pga_drive_show_putt_dough/index.html#module",
    "title": "PGA - Drive for Show, Putt for Dough?",
    "section": "",
    "text": "Please note that these material have not yet completed the required pedagogical and industry peer-reviews to become a published module on the SCORE Network. However, instructors are still welcome to use these materials if they are so inclined. (Revised October 2025)\n\n\nA common expression among golfers is ‚ÄúDrive for show, putt for dough.‚Äù This implies that the long initial tee shots (drives) on each hole are impressive, but the real key to success is the final strokes rolling the ball along the green into the hole (putts). Do data support this adage?\nThe dataset for this activity was obtained from the PGA Statistics Website. Cases include all golfers who made the cut in each of 19 PGA tournaments in 2022. The dataset includes variables for driving ability, putting ability, and measuring success in the tournament. The ‚Äúdriving‚Äù variables include average driving distance (avgDriveDist), driving accuracy percentage (drivePct), and strokes gained off the tee (driveSG). The ‚Äúputting‚Äù variables are average putts per round (avgPuttsPerRound), one putt percentage (onePuttPct), and strokes gained putting (puttsSG). The variables to measure ‚Äúsuccess‚Äù are scoring average (avgScore), official money won (Money), and Fedex Cup points (Points).\nExample of ‚Äústrokes gained‚Äù: Suppose that a golfer is on the green with the ball 10 feet from the hole and PGA players use 1.6 putts, on average, to finish the hole from this distance. If the golfer makes the 10-foot putt, the strokes gained is 1.6-1=0.6, but a miss (and two putts) gives 1.6-2=-0.4 strokes ‚Äúgained‚Äù (which is is actually a ‚Äúloss‚Äù).\n\n\n\n\n\n\nNoteActivity Length\n\n\n\n\n\nThe ‚ÄúNo Tech‚Äù version of this activity (where the correlation matrix is provided) should take less than 15 minutes to complete in class. Time might go up to half an hour if students are using technology to compute correlations and draw graphs.\n\n\n\n\n\n\n\n\n\nNoteLearning Objectives\n\n\n\n\n\nThe learning objectives associated with this module are:\n\nStudents will be able to use correlation to measure the strength association between quantitative variables.\nStudents will be able to check regression model conditions.\nStudents will be able to compare correlations to assess which variables may be more strongly related.\n\n\n\n\n\n\n\n\n\n\nNoteMethods\n\n\n\n\n\nThis module requires students compare correlations between pairs of variables.\nTechnology requirement: Two handout activities accommodate different levels of available technology.\n\nThe ‚ÄúNo Tech version‚Äù provides a correlation matrix for students to use to find the required correlations to compare.\nThe ‚ÄúWith Tech‚Äù version provides the dataset and asks students to use technology to compute the needed correlations.\n\n\n\n\n\n\n\nEach row of data gives the measures for one golfer in one tournament. The dataset covers 19 PGA tournaments from the 2022 season with 1376 cases in all. Each tournament consists of four rounds of golf. Some golfers are eliminated after the first two (or sometimes three) rounds. Only players who competed in all four rounds (i.e.¬†those that made the ‚Äúcut‚Äù) are included in this dataset. Some golfers (for example amateurs) may be missing values if they are not eligible for money won or Fedex Cup points.\nDownload data: PGA2022.csv\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nplayerName\nName of the player\n\n\ncountry\nThe country where the player is from\n\n\navgDriveDist\nAverage driving distance (in yards)\n\n\ndrivePct\nPercentage of times a tee shot comes to rest in the fairway\n\n\ndriveSG\nStrokes gained off the tee measures player performance off the tee on all par 4s and par 5s of how much better or worse a player‚Äôs drive is than the PGA Tour average. Values are average strokes gained per round.\n\n\navgPuttsPerRound\nAverage number of total putts per round\n\n\nonePuttPct\nPercentage of times it took one putt to get the ball into the hole\n\n\nputtsSG\nStrokes gained putting measures how many strokes a player gains or loses on the greens (on average per round) compared to average performance for PGA Tour golfers.\n\n\navgScore\nThe scoring average is the total strokes divided by the total rounds\n\n\nMoney\nThe official money is the prize money award to the PGA members\n\n\nPoints\nFedexCup Points awarded based on finish position in each tournament\n\n\nTournament\nThe tournament where the PGA Tour is taking place\n\n\n\nData Source\nThe data were obtained from the PGA Statistics website\n\n\n\n\nThe data and worksheet associated with this module are available for download through the following links.\nPGA2022.csv - Dataset with driving, putting, and success measures for individual golfer in 19 PGA tournaments in 2022.\nPGACorrelationsNoTech.docx- ‚ÄúNo Tech‚Äù version of the activity worksheet provides a correlation matrix for students to use in answering the questions.\nPGACorrelationsWithTech.docx - ‚ÄúWith Tech‚Äù version of the activity worksheet assumes students have technology to compute any needed correlations from the provided dataset.\nSample solutions to the worksheets\nPGACorrelationsNoTech-Ans.docx- Sample solutions to the ‚ÄúNo Tech‚Äù version of the activity worksheet.\nPGACorrelationsWithTech-Ans.docx - Sample solutions to the ‚ÄúWith Tech‚Äù version of the activity worksheet.\n\n\n\n\n\n\nNoteConclusion\n\n\n\n\n\nStudents should find evidence to support the claim that. in general, putting statistics tend to be better predictors of tournament success than driving statistics. So golfers really do ‚Äúdrive for show, but putt for dough‚Äù.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Drive for Show, Putt for Dough?"
    ]
  },
  {
    "objectID": "golf/Drive_for_show_putt_for_dough/index.html",
    "href": "golf/Drive_for_show_putt_for_dough/index.html",
    "title": "PGA - Drive for Show, Putt for Dough?",
    "section": "",
    "text": "Please note that these material have not yet completed the required pedagogical and industry peer-reviews to become a published module on the SCORE Network. However, instructors are still welcome to use these materials if they are so inclined. (Revised October 2025)\n\n\nA common expression among golfers is ‚ÄúDrive for show, putt for dough.‚Äù This implies that the long initial tee shots (drives) on each hole are impressive, but the real key to success is the final strokes rolling the ball along the green into the hole (putts). Do data support this adage?\nThe dataset for this activity was obtained from the PGA Statistics Website. Cases include all golfers who made the cut in each of 19 PGA tournaments in 2022. The dataset includes variables for driving ability, putting ability, and measuring success in the tournament. The ‚Äúdriving‚Äù variables include average driving distance (avgDriveDist), driving accuracy percentage (drivePct), and strokes gained off the tee (driveSG). The ‚Äúputting‚Äù variables are average putts per round (avgPuttsPerRound), one putt percentage (onePuttPct), and strokes gained putting (puttsSG). The variables to measure ‚Äúsuccess‚Äù are scoring average (avgScore), official money won (Money), and Fedex Cup points (Points).\nExample of ‚Äústrokes gained‚Äù: Suppose that a golfer is on the green with the ball 10 feet from the hole and PGA players use 1.6 putts, on average, to finish the hole from this distance. If the golfer makes the 10-foot putt, the strokes gained is 1.6-1=0.6, but a miss (and two putts) gives 1.6-2=-0.4 strokes ‚Äúgained‚Äù (which is is actually a ‚Äúloss‚Äù).\n\n\n\n\n\n\nNoteActivity Length\n\n\n\n\n\nThe ‚ÄúNo Tech‚Äù version of this activity (where the correlation matrix is provided) should take less than 15 minutes to complete in class. Time might go up to half an hour if students are using technology to compute correlations and draw graphs.\n\n\n\n\n\n\n\n\n\nNoteLearning Objectives\n\n\n\n\n\nThe learning objectives associated with this module are:\n\nStudents will be able to use correlation to measure the strength association between quantitative variables.\nStudents will be able to check regression model conditions.\nStudents will be able to compare correlations to assess which variables may be more strongly related.\n\n\n\n\n\n\n\n\n\n\nNoteMethods\n\n\n\n\n\nThis module requires students compare correlations between pairs of variables.\nTechnology requirement: Two handout activities accommodate different levels of available technology.\n\nThe ‚ÄúNo Tech version‚Äù provides a correlation matrix for students to use to find the required correlations to compare.\nThe ‚ÄúWith Tech‚Äù version provides the dataset and asks students to use technology to compute the needed correlations.\n\n\n\n\n\n\n\nEach row of data gives the measures for one golfer in one tournament. The dataset covers 19 PGA tournaments from the 2022 season with 1376 cases in all. Each tournament consists of four rounds of golf. Some golfers are eliminated after the first two (or sometimes three) rounds. Only players who competed in all four rounds (i.e.¬†those that made the ‚Äúcut‚Äù) are included in this dataset. Some golfers (for example amateurs) may be missing values if they are not eligible for money won or Fedex Cup points.\nDownload data: PGA2022.csv\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nplayerName\nName of the player\n\n\ncountry\nThe country where the player is from\n\n\navgDriveDist\nAverage driving distance (in yards)\n\n\ndrivePct\nPercentage of times a tee shot comes to rest in the fairway\n\n\ndriveSG\nStrokes gained off the tee measures player performance off the tee on all par 4s and par 5s of how much better or worse a player‚Äôs drive is than the PGA Tour average. Values are average strokes gained per round.\n\n\navgPuttsPerRound\nAverage number of total putts per round\n\n\nonePuttPct\nPercentage of times it took one putt to get the ball into the hole\n\n\nputtsSG\nStrokes gained putting measures how many strokes a player gains or loses on the greens (on average per round) compared to average performance for PGA Tour golfers.\n\n\navgScore\nThe scoring average is the total strokes divided by the total rounds\n\n\nMoney\nThe official money is the prize money award to the PGA members\n\n\nPoints\nFedexCup Points awarded based on finish position in each tournament\n\n\nTournament\nThe tournament where the PGA Tour is taking place\n\n\n\nData Source\nThe data were obtained from the PGA Statistics website\n\n\n\n\nThe data and worksheet associated with this module are available for download through the following links.\nPGA2022.csv - Dataset with driving, putting, and success measures for individual golfer in 19 PGA tournaments in 2022.\nPGACorrelationsNoTech.docx- ‚ÄúNo Tech‚Äù version of the activity worksheet provides a correlation matrix for students to use in answering the questions.\nPGACorrelationsWithTech.docx - ‚ÄúWith Tech‚Äù version of the activity worksheet assumes students have technology to compute any needed correlations from the provided dataset.\nSample solutions to the worksheets\nPGACorrelationsNoTech-Ans.docx- Sample solutions to the ‚ÄúNo Tech‚Äù version of the activity worksheet.\nPGACorrelationsWithTech-Ans.docx - Sample solutions to the ‚ÄúWith Tech‚Äù version of the activity worksheet.\n\n\n\n\n\n\nNoteConclusion\n\n\n\n\n\nStudents should find evidence to support the claim that. in general, putting statistics tend to be better predictors of tournament success than driving statistics. So golfers really do ‚Äúdrive for show, but putt for dough‚Äù.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Drive for Show, Putt for Dough?"
    ]
  },
  {
    "objectID": "golf/Drive_for_show_putt_for_dough/index.html#module",
    "href": "golf/Drive_for_show_putt_for_dough/index.html#module",
    "title": "PGA - Drive for Show, Putt for Dough?",
    "section": "",
    "text": "Please note that these material have not yet completed the required pedagogical and industry peer-reviews to become a published module on the SCORE Network. However, instructors are still welcome to use these materials if they are so inclined. (Revised October 2025)\n\n\nA common expression among golfers is ‚ÄúDrive for show, putt for dough.‚Äù This implies that the long initial tee shots (drives) on each hole are impressive, but the real key to success is the final strokes rolling the ball along the green into the hole (putts). Do data support this adage?\nThe dataset for this activity was obtained from the PGA Statistics Website. Cases include all golfers who made the cut in each of 19 PGA tournaments in 2022. The dataset includes variables for driving ability, putting ability, and measuring success in the tournament. The ‚Äúdriving‚Äù variables include average driving distance (avgDriveDist), driving accuracy percentage (drivePct), and strokes gained off the tee (driveSG). The ‚Äúputting‚Äù variables are average putts per round (avgPuttsPerRound), one putt percentage (onePuttPct), and strokes gained putting (puttsSG). The variables to measure ‚Äúsuccess‚Äù are scoring average (avgScore), official money won (Money), and Fedex Cup points (Points).\nExample of ‚Äústrokes gained‚Äù: Suppose that a golfer is on the green with the ball 10 feet from the hole and PGA players use 1.6 putts, on average, to finish the hole from this distance. If the golfer makes the 10-foot putt, the strokes gained is 1.6-1=0.6, but a miss (and two putts) gives 1.6-2=-0.4 strokes ‚Äúgained‚Äù (which is is actually a ‚Äúloss‚Äù).\n\n\n\n\n\n\nNoteActivity Length\n\n\n\n\n\nThe ‚ÄúNo Tech‚Äù version of this activity (where the correlation matrix is provided) should take less than 15 minutes to complete in class. Time might go up to half an hour if students are using technology to compute correlations and draw graphs.\n\n\n\n\n\n\n\n\n\nNoteLearning Objectives\n\n\n\n\n\nThe learning objectives associated with this module are:\n\nStudents will be able to use correlation to measure the strength association between quantitative variables.\nStudents will be able to check regression model conditions.\nStudents will be able to compare correlations to assess which variables may be more strongly related.\n\n\n\n\n\n\n\n\n\n\nNoteMethods\n\n\n\n\n\nThis module requires students compare correlations between pairs of variables.\nTechnology requirement: Two handout activities accommodate different levels of available technology.\n\nThe ‚ÄúNo Tech version‚Äù provides a correlation matrix for students to use to find the required correlations to compare.\nThe ‚ÄúWith Tech‚Äù version provides the dataset and asks students to use technology to compute the needed correlations.\n\n\n\n\n\n\n\nEach row of data gives the measures for one golfer in one tournament. The dataset covers 19 PGA tournaments from the 2022 season with 1376 cases in all. Each tournament consists of four rounds of golf. Some golfers are eliminated after the first two (or sometimes three) rounds. Only players who competed in all four rounds (i.e.¬†those that made the ‚Äúcut‚Äù) are included in this dataset. Some golfers (for example amateurs) may be missing values if they are not eligible for money won or Fedex Cup points.\nDownload data: PGA2022.csv\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nplayerName\nName of the player\n\n\ncountry\nThe country where the player is from\n\n\navgDriveDist\nAverage driving distance (in yards)\n\n\ndrivePct\nPercentage of times a tee shot comes to rest in the fairway\n\n\ndriveSG\nStrokes gained off the tee measures player performance off the tee on all par 4s and par 5s of how much better or worse a player‚Äôs drive is than the PGA Tour average. Values are average strokes gained per round.\n\n\navgPuttsPerRound\nAverage number of total putts per round\n\n\nonePuttPct\nPercentage of times it took one putt to get the ball into the hole\n\n\nputtsSG\nStrokes gained putting measures how many strokes a player gains or loses on the greens (on average per round) compared to average performance for PGA Tour golfers.\n\n\navgScore\nThe scoring average is the total strokes divided by the total rounds\n\n\nMoney\nThe official money is the prize money award to the PGA members\n\n\nPoints\nFedexCup Points awarded based on finish position in each tournament\n\n\nTournament\nThe tournament where the PGA Tour is taking place\n\n\n\nData Source\nThe data were obtained from the PGA Statistics website\n\n\n\n\nThe data and worksheet associated with this module are available for download through the following links.\nPGA2022.csv - Dataset with driving, putting, and success measures for individual golfer in 19 PGA tournaments in 2022.\nPGACorrelationsNoTech.docx- ‚ÄúNo Tech‚Äù version of the activity worksheet provides a correlation matrix for students to use in answering the questions.\nPGACorrelationsWithTech.docx - ‚ÄúWith Tech‚Äù version of the activity worksheet assumes students have technology to compute any needed correlations from the provided dataset.\nSample solutions to the worksheets\nPGACorrelationsNoTech-Ans.docx- Sample solutions to the ‚ÄúNo Tech‚Äù version of the activity worksheet.\nPGACorrelationsWithTech-Ans.docx - Sample solutions to the ‚ÄúWith Tech‚Äù version of the activity worksheet.\n\n\n\n\n\n\nNoteConclusion\n\n\n\n\n\nStudents should find evidence to support the claim that. in general, putting statistics tend to be better predictors of tournament success than driving statistics. So golfers really do ‚Äúdrive for show, but putt for dough‚Äù.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Drive for Show, Putt for Dough?"
    ]
  },
  {
    "objectID": "games/BlackJack/index.html",
    "href": "games/BlackJack/index.html",
    "title": "BlackJack Logistic Regression",
    "section": "",
    "text": "Please note that these material have not yet completed the required pedagogical and industry peer-reviews to become a published module on the SCORE Network. However, instructors are still welcome to use these materials if they are so inclined.\nThis module is available on the ISLE platform BlackJack Module\n\n\nIf you use this module in your work, please cite it as follows:\nTran, A. and Clark, N. (2025, July 25). Blackjack Logistic Regression. ‚ÄúThe SCORE Network.‚Äù https://doi.org/10.17605/OSF.IO/M4WV7\nYou can include this citation directly in your references or bibliography.",
    "crumbs": [
      "Home",
      "Games",
      "BlackJack Logistic Regression"
    ]
  },
  {
    "objectID": "games/BlackJack/index.html#module",
    "href": "games/BlackJack/index.html#module",
    "title": "BlackJack Logistic Regression",
    "section": "",
    "text": "Please note that these material have not yet completed the required pedagogical and industry peer-reviews to become a published module on the SCORE Network. However, instructors are still welcome to use these materials if they are so inclined.\nThis module is available on the ISLE platform BlackJack Module\n\n\nIf you use this module in your work, please cite it as follows:\nTran, A. and Clark, N. (2025, July 25). Blackjack Logistic Regression. ‚ÄúThe SCORE Network.‚Äù https://doi.org/10.17605/OSF.IO/M4WV7\nYou can include this citation directly in your references or bibliography.",
    "crumbs": [
      "Home",
      "Games",
      "BlackJack Logistic Regression"
    ]
  },
  {
    "objectID": "football/index.html",
    "href": "football/index.html",
    "title": "Football",
    "section": "",
    "text": "These modules use football data to teach topics in statistics and data science.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Elo ratings\n\n\n\nElo ratings\n\nBrier score\n\nprediction\n\n\n\nAn introduction to Elo ratings using NFL game outcomes.\n\n\n\n\n\nJul 9, 2024\n\n\nRon Yurko\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Football"
    ]
  },
  {
    "objectID": "esports/league-of-legends-buffing-nerfing/index.html",
    "href": "esports/league-of-legends-buffing-nerfing/index.html",
    "title": "League of Legends - Buffing and Nerfing",
    "section": "",
    "text": "Welcome video\n\n\n\n\nIntroduction\nLeague of Legends (LoL) is a 5 v. 5 multiplayer online battle arena (MOBA) game developed by Riot Games. In this game, players assume the role of a ‚Äúchampion‚Äù with unique abilities and engage in intense battles against a team of other players or computer-controlled champions. Riot Games continually collects data to evaluate the impact of each champion, adjusting and fine-tuning various aspects to ensure fair and competitive gameplay. With regular updates (patches) occurring every two weeks, champions can become either extremely efficient and strong or in need of adjustments to enhance their abilities. Maintaining overall game balance is crucial, and developers employ strategies known as ‚Äúnerfing‚Äù and ‚Äúbuffing‚Äù to achieve this balance. ‚ÄúNerfing‚Äù refers to reducing the power or effectiveness of a champion or item, while ‚Äúbuffing‚Äù involves increasing its power or effectiveness.\nIn this worksheet, we will analyze and describe histograms of Win Rates for different champions in LoL. The Win Rate, a key metric in the game, represents the percentage of games won by a champion out of the total games played. Understanding the distribution of Win Rates and identifying potential outliers can provide valuable insights into champion balance and performance, informing strategic decision-making in LoL gameplay.\n\n\n\n\n\n\nNoteLearning Objectives\n\n\n\n\n\nBy the end of this activity, you will be able to:\n\nUnderstand the concept of histograms and their relevance in statistical analysis.\nAnalyze and describe histograms to gain insights into the distribution of Win Rates in League of Legends. In particular, being able to describe the center, shape, and spread of a distribution based on the displayed graph.\nIdentify potential outliers in a numerical variable using numerical methods such as the ‚Äú1.5 IQR Rule‚Äù or z-scores.\nInterpret the implications of outliers in terms of champion balance and performance.\n\n\n\n\n\n\n\n\n\n\nNoteMethods\n\n\n\n\n\nTechnology requirement: The activity handout provides histograms and summary statistics so that no statistical software is required. However, the activity could be modified to ask students to produce that information from the raw dataset and/or extend the activity to investigate other variables available in the data.\n\nHistograms: Familiarity with histograms as a graphical representation of the distribution of a continuous variable, such as Win Rates, is crucial. You should understand how to interpret histograms, including the concepts of bins, frequencies, and the shape, center, and spread of distributions.\nOutliers: Knowledge of outliers, which are data points that deviate significantly from the overall pattern, is important.\nFamiliarity with basic statistical analysis techniques, such as measures of central tendency (mean, median) and measures of dispersion (standard deviation, range), will aid in interpreting and analyzing the histograms. These techniques can provide insights into the overall characteristics and variability of the Win Rates.\nKnowledge of outlier detection methods (such as the 1.5 IQR Rule and/or z-scores) is fundamental to the activity.\n\n\n\n\n\n\nData\nA data frame for 162 champions of the following 7 variables. Each row represents a Champion that you can choose when playing League of Legends during patches 12.22 and 12.23. Note that the activity provided in this module does not use all of the variables provided. Instead they are provided for further analyses at the discretion of the user.\nAvailable on the SCORE Data Repository\nDownload data: LOL_patch_12.22.csv\nDownload data: LOL_patch_12.23.csv\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nName\nname of the champion\n\n\nRole\nrole of the champion in a game\n\n\nKDA\nAverage kills, deaths and assists associated with each champion\n\n\nWRate\nwin rates of each champion\n\n\nPickRate\npick rates of each champion\n\n\nRolePerc\npercentage of time playing as a role\n\n\nBanPerc\nban percentages associated with each champion\n\n\n\n\n\nData Source\nLol champion stats, 12.22 master, win rates. METAsrc. (n.d.). https://www.metasrc.com/5v5/12.22/stats?ranks=master\nLol champion stats, 12.23 master, win rates. METAsrc. (n.d.-b). https://www.metasrc.com/5v5/12.23/stats?ranks=master\n\n\n\nMaterials\nClass handout\nClass handout - with solutions\n\n\n\n\n\n\nNoteConclusion\n\n\n\n\n\nIn conclusion, the analysis of Win Rates histograms in League of Legends has provided valuable insights into champion balance and performance. One notable finding from this worksheet is the identification of Sion as a low outlier in the Win Rates for the 12.22 patch. However, in the subsequent 12.23 patch, Sion‚Äôs Win Rate improved, and was no longer an outlier. Sion was given a Buff in patch 12.23, resulting in an enhanced performance and a more balanced Win Rate. However, patch 12.23 resulted in two new outliers with low win rates.\nThe continued presence of outliers highlights the importance of continuous monitoring and adjustments by game developers to ensure fair and competitive gameplay.\n\n\n\n\n\nHow to Cite\nIf you use this module in your work, please cite it as follows:\nRamler, I., Charalambous, G., & Dykstra, A. J. (2025, April 30). League of Legends. ‚ÄúThe SCORE Network.‚Äù https://doi.org/10.17605/OSF.IO/8R3YG\nYou can include this citation directly in your references or bibliography.",
    "crumbs": [
      "Home",
      "Esports",
      "League of Legends - Buffing and Nerfing"
    ]
  },
  {
    "objectID": "baseball/unbreakable-records/index.html",
    "href": "baseball/unbreakable-records/index.html",
    "title": "Unbreakable Records in Baseball",
    "section": "",
    "text": "This lesson introduces students to the Bernoulli trial and Binomial Experiments to understand the probability of breaking one of the longest lasting records in baseball. We also explain how to execute a Chi-Square test using baseball data on handedness (right or left-handed) of batters versus pitchers.",
    "crumbs": [
      "Home",
      "Baseball",
      "Unbreakable Records in Baseball"
    ]
  },
  {
    "objectID": "baseball/unbreakable-records/index.html#motivation",
    "href": "baseball/unbreakable-records/index.html#motivation",
    "title": "Unbreakable Records in Baseball",
    "section": "",
    "text": "This lesson introduces students to the Bernoulli trial and Binomial Experiments to understand the probability of breaking one of the longest lasting records in baseball. We also explain how to execute a Chi-Square test using baseball data on handedness (right or left-handed) of batters versus pitchers.",
    "crumbs": [
      "Home",
      "Baseball",
      "Unbreakable Records in Baseball"
    ]
  },
  {
    "objectID": "baseball/unbreakable-records/index.html#module",
    "href": "baseball/unbreakable-records/index.html#module",
    "title": "Unbreakable Records in Baseball",
    "section": "Module",
    "text": "Module\nhttps://isle.stat.cmu.edu/SCORE/Unbreakable_Records_Baseball_Hits/",
    "crumbs": [
      "Home",
      "Baseball",
      "Unbreakable Records in Baseball"
    ]
  },
  {
    "objectID": "baseball/mlb_prime_age/index.html",
    "href": "baseball/mlb_prime_age/index.html",
    "title": "What‚Äôs the prime age of an MLB player?",
    "section": "",
    "text": "NoteMethods/Facilitation notes\n\n\n\n\n\n\nThis module would be suitable for an in-class lab or take-home assignment in an introductory data science course that uses R.\nIt assumes a basic familiarity with the RStudio Environment and basic introduction to the tidyverse has already been covered, but tips on tidyverse code are provided throughout.\nStudents should be provided with the following data files (.csv) and Quarto document (.qmd) to produce visualizations and write up their answers to each exercise. Their final deliverable is to turn in an .html document produced by ‚ÄúRendering‚Äù the .qmd.\n\nbatter_stats.csv\npitcher_stats.csv\nStudent Quarto template\n\nPosit Cloud (via an Instructor account) or Github classroom are good options for disseminating files to students, but simply uploading files to your university‚Äôs course management system works, too.",
    "crumbs": [
      "Home",
      "Baseball",
      "What's the prime age of an MLB player?"
    ]
  },
  {
    "objectID": "baseball/mlb_prime_age/index.html#batters",
    "href": "baseball/mlb_prime_age/index.html#batters",
    "title": "What‚Äôs the prime age of an MLB player?",
    "section": "Batters",
    "text": "Batters\n\n\n\n\n\n\nCautionResearch question\n\n\n\nWhat is the average age a batter in the MLB reaches his prime?\n\n\nLet‚Äôs first note how our data is organized:\n\nhead(batter_stats)\n\n# A tibble: 6 √ó 18\n  x_mlbamid season team_name bats  player_name       age   war     g    ab    pa\n      &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    545361   2014 LAA       R     Mike Trout         22  8.29   157   602   705\n2    457763   2014 SFG       R     Buster Posey       27  7.52   147   547   605\n3    518960   2014 MIL       R     Jonathan Lucroy    28  7.44   153   585   655\n4    457705   2014 PIT       R     Andrew McCutch‚Ä¶    27  7.40   146   548   648\n5    519317   2014 MIA       R     Giancarlo Stan‚Ä¶    24  6.85   145   539   638\n6    488726   2014 CLE       L     Michael Brantl‚Ä¶    27  6.53   156   611   676\n# ‚Ñπ 8 more variables: h &lt;dbl&gt;, x1b &lt;dbl&gt;, x2b &lt;dbl&gt;, x3b &lt;dbl&gt;, hr &lt;dbl&gt;,\n#   r &lt;dbl&gt;, rbi &lt;dbl&gt;, best_war &lt;dbl&gt;\n\n\n\n\n\nCODE TIP: The function head() returns the first 6 rows of a dataset, and the function tail() returns the last 6. You can add the argument n = to display a different number of rows. Note these are base R functions and do not require the tidyverse to use.\nIf we arrange by x_mlbaid we can see that there can be multiple observations per player, where each row represents a different season.\n\nbatter_stats |&gt; \n  arrange(x_mlbamid) |&gt; \n  slice(1:10)\n\n# A tibble: 10 √ó 18\n   x_mlbamid season team_name bats  player_name    age     war     g    ab    pa\n       &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1    110029   2014 NYM       L     Bobby Abreu     40 -0.213     78   133   155\n 2    112526   2014 NYM       R     Bartolo Col‚Ä¶    41 -0.588     31    62    69\n 3    112526   2015 NYM       R     Bartolo Col‚Ä¶    42 -0.0408    33    58    64\n 4    112526   2016 NYM       R     Bartolo Col‚Ä¶    43 -0.243     34    60    65\n 5    112526   2017 - - -     R     Bartolo Col‚Ä¶    44 -0.266     28    19    20\n 6    112526   2018 TEX       R     Bartolo Col‚Ä¶    45 -0.0475    28     4     4\n 7    114739   2014 CLE       L     Jason Giambi    43 -0.496     26    60    70\n 8    115629   2014 COL       R     LaTroy Hawk‚Ä¶    41 -0.0141    57     1     1\n 9    115629   2015 - - -     R     LaTroy Hawk‚Ä¶    42 -0.0146    42     1     1\n10    116338   2014 DET       R     Torii Hunter    38  1.11     142   549   586\n# ‚Ñπ 8 more variables: h &lt;dbl&gt;, x1b &lt;dbl&gt;, x2b &lt;dbl&gt;, x3b &lt;dbl&gt;, hr &lt;dbl&gt;,\n#   r &lt;dbl&gt;, rbi &lt;dbl&gt;, best_war &lt;dbl&gt;\n\n\n\n\n¬†The pipe: Recall that |&gt; is called the ‚Äúpipe‚Äù function and can be read as ‚Äúand then.‚Äù In English, the code on the left can be read as ‚Äútake the batter_stats data and then arrange it by x_mlbamid and then slice the first 10 rows.‚Äù Mathematically, the pipe accomplishes f(g(x)) with the (psudeo-)code x |&gt; g() |&gt; f(). Read more about the pipe here.\n\ndplyr: arrange() and slice() are examples of dplyr verbs: tidyverse functions that do something to / act on the data. Other examples include filter(), select(), mutate(), group_by(), summarize(), relocate(), and many more. These verbs are often chained together with the pipe to accomplish multiple data wrangling tasks. Read more about data wrangling with dplyr here.\n\n\n\n\n\n\n\nNoteExercise 1:\n\n\n\nWhich seasons are included in this data?\n\n\n\n\nTIP: Try writing your answer as a full sentence in the .qmd using inline code. For example, if you have the first season saved in an object first_season, then including `r first_season` outside a code chunk will allow you to auto-populate this value in a sentence.\n\n\n\n\n\n\nImportant\n\n\n\nIn order to determine the prime age of each player, we need to look for the year in which his war reached its player-specific maximum. We can utilize the group_by() function to do this.\n\n\n\n\n\n\n\n\nNoteExercise 2:\n\n\n\nCopy, paste the following code and fill in the blanks to create a new variable best_war that contains a player‚Äôs maximum war.\n\nbatter_stats &lt;- batter_stats |&gt; \n  group_by(________) |&gt; \n  mutate(_______ = _______(_______)) |&gt; \n  ungroup()\n\n\n\n\n\nCODE TIP: group_by() allows all subsequent actions to be done for each group of the grouping variable. Therefore, if we group by player id, we‚Äôre able to determine the maximum war for each player, not simply the maximum war for the whole dataset. It‚Äôs often a good idea to ungroup() at the end of a chain of code, otherwise the next time you try to use your data, it will still perform every operation by group.\nTake a quick glimpse() of your data to confirm the first few values of best_war match those below before proceeding.\n\n\nRows: 13,917\nColumns: 18\n$ x_mlbamid   &lt;dbl&gt; 545361, 457763, 518960, 457705, 519317, 488726, 543685, 43‚Ä¶\n$ season      &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014‚Ä¶\n$ team_name   &lt;chr&gt; \"LAA\", \"SFG\", \"MIL\", \"PIT\", \"MIA\", \"CLE\", \"WSN\", \"TOR\", \"P‚Ä¶\n$ bats        &lt;chr&gt; \"R\", \"R\", \"R\", \"R\", \"R\", \"L\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\"‚Ä¶\n$ player_name &lt;chr&gt; \"Mike Trout\", \"Buster Posey\", \"Jonathan Lucroy\", \"Andrew M‚Ä¶\n$ age         &lt;dbl&gt; 22, 27, 28, 27, 24, 27, 24, 33, 31, 35, 28, 28, 31, 23, 30‚Ä¶\n$ war         &lt;dbl&gt; 8.2866, 7.5222, 7.4368, 7.4014, 6.8473, 6.5310, 6.4054, 6.‚Ä¶\n$ g           &lt;dbl&gt; 157, 147, 153, 146, 145, 156, 153, 155, 111, 148, 148, 158‚Ä¶\n$ ab          &lt;dbl&gt; 602, 547, 585, 548, 539, 611, 613, 553, 379, 549, 574, 608‚Ä¶\n$ pa          &lt;dbl&gt; 705, 605, 655, 648, 638, 676, 683, 673, 460, 614, 644, 695‚Ä¶\n$ h           &lt;dbl&gt; 173, 170, 176, 172, 155, 200, 176, 158, 110, 178, 163, 155‚Ä¶\n$ x1b         &lt;dbl&gt; 89, 118, 108, 103, 86, 133, 110, 96, 79, 125, 102, 93, 134‚Ä¶\n$ x2b         &lt;dbl&gt; 39, 28, 53, 38, 31, 45, 39, 27, 20, 33, 34, 31, 37, 37, 34‚Ä¶\n$ x3b         &lt;dbl&gt; 9, 2, 2, 6, 1, 2, 6, 0, 0, 1, 4, 2, 2, 9, 1, 1, 2, 1, 3, 1‚Ä¶\n$ hr          &lt;dbl&gt; 36, 22, 13, 25, 37, 20, 21, 35, 11, 19, 23, 29, 14, 16, 19‚Ä¶\n$ r           &lt;dbl&gt; 115, 72, 73, 89, 89, 94, 111, 101, 45, 79, 95, 93, 77, 92,‚Ä¶\n$ rbi         &lt;dbl&gt; 111, 89, 69, 83, 105, 97, 83, 103, 67, 77, 73, 98, 82, 69,‚Ä¶\n$ best_war    &lt;dbl&gt; 9.4559, 7.5222, 7.4368, 7.4014, 6.8473, 6.5310, 6.7801, 6.‚Ä¶\n\n\n\n\nCODE TIP: In real life data science work, you won‚Äôt usually be provided with the ‚Äúcorect‚Äù answer to compare to, so it‚Äôs often a good idea to do a quick check after any data transformation to make sure your code did what you expected. In this case, you might choose one player to verify that their best_war value is in fact equal to their maximum war value. You can do a quick filter for that player in your console, or use the search feature when Viewing the full data in spreadsheet view.\n\n\n\n\n\n\nNoteExercise 3:\n\n\n\nCreate a new dataset called prime_age that keeps only the rows where a player‚Äôs war is equal to his best_war.\nWhat are the dimensions of this new dataset?\n\n\n\n\n\nHint: what dyplr verb do you need to keep rows that meet a criteria?\nIdeally, we want there to be one row per player in our new dataset. However, if we check the number of unique players we have in our original data, we find this does not match the number of rows in prime_age.\n\n\nCODE TIP: Two options for checking the number of unique levels of a variable are length(unique(data$variable)) or data |&gt; distinct(variable) |&gt; nrow()\n\n\n\n\n\n\nNoteExercise 4:\n\n\n\nReport the number of unique players in the dataset batter_stats.\nInspect the prime_age data more closely. What is the maximum number of rows that appear for a player in this dataset? Comment on why this is happening. Hint: creating a new variable that counts the number of rows per id can help you investigate this.\n\n\n\n\nCODE TIP: group_by(grouping_variable) followed by mutate(n = n()) will count the number of rows per level of the grouping variable.\n\n\n\n\n\n\nNoteExercise 5:\n\n\n\nDetermine a strategy for reducing prime_age down to one row per person (still maintaining all relevant columns). Describe your strategy in words and then write code to accomplish it. Careful - don‚Äôt just arbitrarily throw away rows! There are multiple ways you might approach this, but you should justify your decision(s) and think through implications for your ultimate analysis goal: estimating prime age.\n\n\nYour reduced prime_age should look something like this:\n\n\nRows: 3,752\nColumns: 18\n$ x_mlbamid   &lt;dbl&gt; 457763, 518960, 457705, 519317, 488726, 430832, 431145, 13‚Ä¶\n$ season      &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014‚Ä¶\n$ team_name   &lt;chr&gt; \"SFG\", \"MIL\", \"PIT\", \"MIA\", \"CLE\", \"TOR\", \"PIT\", \"TEX\", \"M‚Ä¶\n$ bats        &lt;chr&gt; \"R\", \"R\", \"R\", \"R\", \"L\", \"R\", \"R\", \"R\", \"R\", \"R\", \"L\", \"L\"‚Ä¶\n$ player_name &lt;chr&gt; \"Buster Posey\", \"Jonathan Lucroy\", \"Andrew McCutchen\", \"Gi‚Ä¶\n$ age         &lt;dbl&gt; 27, 28, 27, 24, 27, 33, 31, 35, 28, 23, 30, 24, 27, 35, 29‚Ä¶\n$ war         &lt;dbl&gt; 7.5222, 7.4368, 7.4014, 6.8473, 6.5310, 6.1703, 6.1427, 5.‚Ä¶\n$ g           &lt;dbl&gt; 147, 153, 146, 145, 156, 155, 111, 148, 148, 148, 156, 140‚Ä¶\n$ ab          &lt;dbl&gt; 547, 585, 548, 539, 611, 553, 379, 549, 574, 558, 563, 524‚Ä¶\n$ pa          &lt;dbl&gt; 605, 655, 648, 638, 676, 673, 460, 614, 644, 640, 643, 616‚Ä¶\n$ h           &lt;dbl&gt; 170, 176, 172, 155, 200, 158, 110, 178, 163, 165, 150, 150‚Ä¶\n$ x1b         &lt;dbl&gt; 118, 108, 103, 86, 133, 96, 79, 125, 102, 103, 96, 89, 103‚Ä¶\n$ x2b         &lt;dbl&gt; 28, 53, 38, 31, 45, 27, 20, 33, 34, 37, 34, 28, 35, 37, 18‚Ä¶\n$ x3b         &lt;dbl&gt; 2, 2, 6, 1, 2, 0, 0, 1, 4, 9, 1, 1, 2, 1, 1, 3, 1, 7, 3, 2‚Ä¶\n$ hr          &lt;dbl&gt; 22, 13, 25, 37, 20, 35, 11, 19, 23, 16, 19, 32, 36, 16, 21‚Ä¶\n$ r           &lt;dbl&gt; 72, 73, 89, 89, 94, 101, 45, 79, 95, 92, 87, 89, 80, 85, 7‚Ä¶\n$ rbi         &lt;dbl&gt; 89, 69, 83, 105, 97, 103, 67, 77, 73, 69, 74, 78, 107, 82,‚Ä¶\n$ best_war    &lt;dbl&gt; 7.5222, 7.4368, 7.4014, 6.8473, 6.5310, 6.1703, 6.1427, 5.‚Ä¶\n\n\n\n\n\n\n\n\nNoteExercise 6:\n\n\n\nProduce a visualization that explores the distribution of prime ages, for all players in this data.\n\n\n\n\n\n\n\n\nNoteExercise 7:\n\n\n\nBased on the graph, ‚Äúeyeball‚Äù an initial answer to the research question: at what age do professional batters tend to be at their ‚Äúprime‚Äù?\n\n\n\n\n\n\n\n\nNoteExercise 8:\n\n\n\nCalculate the mean and the median prime age for batters in this data.\n\n\n\n\n\n\n\n\nNoteExercise 9:\n\n\n\nReproduce your graph from above but add 2 lines to the graph representing the mean and median of the distribution.\n\n\n\n\n\nTip: Add a layer called geom_vline to your ggplot code. Make sure the colors of the lines are different.",
    "crumbs": [
      "Home",
      "Baseball",
      "What's the prime age of an MLB player?"
    ]
  },
  {
    "objectID": "baseball/mlb_prime_age/index.html#pitchers",
    "href": "baseball/mlb_prime_age/index.html#pitchers",
    "title": "What‚Äôs the prime age of an MLB player?",
    "section": "Pitchers",
    "text": "Pitchers\n\n\n\n\n\n\nCautionResearch question\n\n\n\nWhat is the average age an MLB pitcher reaches his prime?\n\n\n\n\n\n\n\n\nNoteExercise 10\n\n\n\nCopy, paste, tweak appropriate code from previous exercises to determine the prime age of pitchers, using the pitcher_stats data.\n\n\n\n\n\nCheck: there are 2382 unique pitchers in the pitcher_stats data, so your final dataset for analysis should have that many rows.",
    "crumbs": [
      "Home",
      "Baseball",
      "What's the prime age of an MLB player?"
    ]
  },
  {
    "objectID": "baseball/index.html",
    "href": "baseball/index.html",
    "title": "Baseball",
    "section": "",
    "text": "These modules use baseball data to teach topics in statistics and data science.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMLB Injuries - Introductory Time Series Analysis\n\n\n\nTime series plots\n\nTime series decomposition\n\nResidual analysis\n\nSimple forecasting\n\n\n\nExploring MLB injury data through time series analysis and forecasting.\n\n\n\n\n\nNov 26, 2024\n\n\nJonathan Lieb\n\n\n\n\n\n\n\n\n\n\n\n\nWhat‚Äôs the prime age of an MLB player?\n\n\n\nData wrangling\n\nDplyr basics\n\nTidyverse\n\n\n\nUse data wrangling skills to explore the prime age of MLB players\n\n\n\n\n\nJul 26, 2024\n\n\nJazmine Gurrola, Joseph Hsieh, Dat Tran, Katie Fitzgerald\n\n\n\n\n\n\n\n\n\n\n\n\nStolen Bases\n\n\n\nNormality tests\n\n\n\n\n\n\n\n\n\nJul 23, 2023\n\n\nAndrew Lee and Jacob Hurtubise\n\n\n\n\n\n\n\n\n\n\n\n\nUnbreakable Records in Baseball\n\n\n\nBernoulli distribution\n\nBinomial distribution\n\nChi-Square Test\n\n\n\n\n\n\n\n\n\nJul 23, 2023\n\n\nAndrew Lee and Fr Gabriel Costa\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Baseball"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SCORE Module Repository",
    "section": "",
    "text": "The SCORE Network Module Repository enables you to search for modules by either sport or by statistics and data science topic. The modules listed in this repository have completed the required SCORE Network pedagogical and industry peer reviews to become a published module."
  },
  {
    "objectID": "index.html#all-modules",
    "href": "index.html#all-modules",
    "title": "SCORE Module Repository",
    "section": "All Modules",
    "text": "All Modules\n\n\n\n\n\n\nBrowse all modules below. Use the sidebar to filter by sport or visit by statistics and data science topic.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMMA Inter-rater Reliability Data Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStrength Ratios - Flat Dumbbell Press to Barbell Bench Press\n\n\n\nData Collection Methods\n\nConfidence Intervals\n\n\n\nCritically exploring self reported strength ratios.\n\n\n\n\n\nFeb 24, 2026\n\n\nVivian Johnson, Ivan Ramler\n\n\n\n\n\n\n\n\n\n\n\n\nPickleball: Estimating the proportion of dependable DUPR ratings\n\n\n\nConfidence Intervals\n\nProportions\n\n\n\nEstimate the proportion of collegiate pickleball players who have a dependable overall DUPR reliability score (of 60% or higher).\n\n\n\n\n\nDec 4, 2025\n\n\nFaith Rhinehart, Ivan Ramler, Jessica Chapman\n\n\n\n\n\n\n\n\n\n\n\n\nPGA - Drive for Show, Putt for Dough?\n\n\n\nCorrelation\n\n\n\nUsing tournament data for professional golfers to see if driving or putting are more strongly related to success.\n\n\n\n\n\nOct 23, 2025\n\n\nAlyssa Bigness, Robin Lock\n\n\n\n\n\n\n\n\n\n\n\n\nPGA - Drive for Show, Putt for Dough?\n\n\n\nCorrelation\n\n\n\nUsing tournament data for professional golfers to see if driving or putting are more strongly related to success.\n\n\n\n\n\nOct 23, 2025\n\n\nAlyssa Bigness, Robin Lock\n\n\n\n\n\n\n\n\n\n\n\n\nBlackJack Logistic Regression\n\n\n\nLogistic Regression\n\nBinary Data\n\n\n\nAn Introduction to Logistic Regression Using BlackJack\n\n\n\n\n\nJul 23, 2025\n\n\nAndrew Tran, Nicholas Clark\n\n\n\n\n\n\n\n\n\n\n\n\nOlympic Rowing Medals Between 1900 and 2022 - Data Wrangling\n\n\n\ndplyr\n\nfiltering\n\ngrouping and summarizing\n\nmutating\n\n\n\nArranging data to analyze the total number of medals and the weighted points for nations competing in rowing events in the Summer Olympic Games between 1900 and 2022.\n\n\n\n\n\nJun 5, 2025\n\n\nAbigail Smith, Robin Lock, Ivan Ramler\n\n\n\n\n\n\n\n\n\n\n\n\nOlympic Rowing Medals Between 1900 and 2022 - Summary Statistics\n\n\n\ndistribution and skewness\n\noutlier detection\n\nsummary statistics\n\nconfounding variable\n\n\n\nThe total number of medals and the weighted points for nations competing in rowing events in the Summer Olympic Games between 1900 and 2022.\n\n\n\n\n\nJun 5, 2025\n\n\nAbigail Smith, Ivan Ramler, Robin Lock\n\n\n\n\n\n\n\n\n\n\n\n\nAmerican Ninja Warrior - Kaplan-Meier Survival Analysis\n\n\n\nKaplan-Meier\n\nLog Rank test\n\nNonparametric tests\n\n\n\nExploring Survival Analysis using the Kaplan-Meier method with American Ninja Warrior data.\n\n\n\n\n\nJun 4, 2025\n\n\nJonathan Lieb, Rodney X. Sturdivant\n\n\n\n\n\n\n\n\n\n\n\n\nExpected Goals in Soccer\n\n\n\nLogistic Regression\n\nFeature Engineering\n\nUnder Sampling\n\n\n\nAn Introduction to Expected Goals Using Soccer\n\n\n\n\n\nMay 19, 2025\n\n\nColman Kim, Andrew Lee\n\n\n\n\n\n\n\n\n\n\n\n\nMLS - Types of Decision Errors\n\n\n\nDecision Errors\n\n\n\nExploring types of decision errors\n\n\n\n\n\nMar 30, 2025\n\n\nJonathan Lieb\n\n\n\n\n\n\n\n\n\n\n\n\nPGA - Scoring Average Confidence Intervals\n\n\n\nSingle Mean Confidence Intervals\n\nSingle Mean Hypothesis Testing\n\n\n\nExploring Single Mean Confidence Intervals with Golf Data\n\n\n\n\n\nJan 25, 2025\n\n\nJonathan Lieb\n\n\n\n\n\n\n\n\n\n\n\n\nPGA - Scoring Average Confidence Intervals (No R)\n\n\n\nSingle Mean Confidence Intervals\n\nSingle Mean Hypothesis Testing\n\n\n\nExploring Single Mean Confidence Intervals with Golf Data\n\n\n\n\n\nJan 25, 2025\n\n\nJonathan Lieb\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface\n\n\n\nANOVA\n\nTennis\n\n\n\nUsing tennis to teach ANOVA and linear regression with categorical variables\n\n\n\n\n\nJan 22, 2025\n\n\nZachary O. Binney, Heyi Yang\n\n\n\n\n\n\n\n\n\n\n\n\nMLB Injuries - Introductory Time Series Analysis\n\n\n\nTime series plots\n\nTime series decomposition\n\nResidual analysis\n\nSimple forecasting\n\n\n\nExploring MLB injury data through time series analysis and forecasting.\n\n\n\n\n\nNov 26, 2024\n\n\nJonathan Lieb\n\n\n\n\n\n\n\n\n\n\n\n\nWhat‚Äôs the prime age of an MLB player?\n\n\n\nData wrangling\n\nDplyr basics\n\nTidyverse\n\n\n\nUse data wrangling skills to explore the prime age of MLB players\n\n\n\n\n\nJul 26, 2024\n\n\nJazmine Gurrola, Joseph Hsieh, Dat Tran, Katie Fitzgerald\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Elo ratings\n\n\n\nElo ratings\n\nBrier score\n\nprediction\n\n\n\nAn introduction to Elo ratings using NFL game outcomes.\n\n\n\n\n\nJul 9, 2024\n\n\nRon Yurko\n\n\n\n\n\n\n\n\n\n\n\n\n2023 Boston Marathon - Variability in Finish Times\n\n\n\nhistograms\n\nsummary statistics\n\nbimodal data\n\n\n\nDescribing finish time for runners in the 2023 Boston Marathon\n\n\n\n\n\nMay 13, 2024\n\n\nIvan Ramler, Jack Fay\n\n\n\n\n\n\n\n\n\n\n\n\nFIRST Robotics Competition - Winning Chances\n\n\n\nBrier score\n\nprediction assessment\n\n\n\nEvaluating the predicted winning probabilities against the actual outcomes.\n\n\n\n\n\nMar 5, 2024\n\n\nJake Tan\n\n\n\n\n\n\n\n\n\n\n\n\nLeague of Legends - Buffing and Nerfing\n\n\n\noutliers\n\nsummary statistics\n\n\n\nInvestigating game play statistics for League of Legends champions in two different patches.\n\n\n\n\n\nFeb 21, 2024\n\n\nIvan Ramler, George Charalambous, A.J. Dykstra\n\n\n\n\n\n\n\n\n\n\n\n\nLacrosse Faceoff Proportions\n\n\n\nHypothesis testing\n\nSingle proportion\n\n\n\nUsing data from NCAA Div I lacrosse teams to explore the importance of winning faceoffs\n\n\n\n\n\nFeb 5, 2024\n\n\nJack Fay, Ivan Ramler, A.J. Dykstra\n\n\n\n\n\n\n\n\n\n\n\n\nLacrosse PLL vs.¬†NLL\n\n\n\nDifference in two means\n\n\n\nComparing scoring rates between indoor and outdoor profesional lacrosse leagues.\n\n\n\n\n\nFeb 5, 2024\n\n\nJack Cowan, Ivan Ramler, A.J. Dykstra, Robin Lock\n\n\n\n\n\n\n\n\n\n\n\n\nNASCAR Transformation Module\n\n\n\nLinear regression\n\nTransformations\n\nPolynomial regression\n\n\n\nUsing NASCAR driver rating data to explore a series of transformations to improve linearity in regression.\n\n\n\n\n\nFeb 5, 2024\n\n\nAlyssa Bigness, Ivan Ramler, Jack Fay\n\n\n\n\n\n\n\n\n\n\n\n\nIronman Triathlon (Canadian Females) - Multiple Linear Regression\n\n\n\nLinear regression\n\n\n\nUsing Lake Placid Ironman triathlon results for female Canadian finishers to predict run times for participants based on both swim and bike times.\n\n\n\n\n\nFeb 5, 2024\n\n\nA.J. Dykstra, Ivan Ramler\n\n\n\n\n\n\n\n\n\n\n\n\nStolen Bases\n\n\n\nNormality tests\n\n\n\n\n\n\n\n\n\nJul 23, 2023\n\n\nAndrew Lee and Jacob Hurtubise\n\n\n\n\n\n\n\n\n\n\n\n\nUnbreakable Records in Baseball\n\n\n\nBernoulli distribution\n\nBinomial distribution\n\nChi-Square Test\n\n\n\n\n\n\n\n\n\nJul 23, 2023\n\n\nAndrew Lee and Fr Gabriel Costa\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting NHL Shooting Percentages\n\n\n\nlinear regression\n\n\n\nAn Introduction to Simple Linear Regression\n\n\n\n\n\nJul 23, 2023\n\n\nSam Ventura\n\n\n\n\n\n\n\n\n\n\n\n\nMarathon Record-Setting Over Time\n\n\n\nExponential distribution\n\nPoisson process\n\n\n\nDetermining whether the setting of world records in the marathon is historically a Poisson process.\n\n\n\n\n\nJul 23, 2023\n\n\nNicholas Clark, Rodney Sturdivant, and Kate Sanborn\n\n\n\n\n\n\n\n\n\n\n\n\nIronman Triathlete Performance\n\n\n\nScatterplots\n\nCorrelation\n\n\n\nGaining insight into the performance patterns of triathletes by exploring the relationships between swimming, biking, and running times.\n\n\n\n\n\nJul 23, 2023\n\n\nMichael Schuckers, Matt Abell, AJ Dykstra, Sarah Weaver, Ivan Ramler, and Robin Lock\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#contributing-andor-joining-the-score-network",
    "href": "index.html#contributing-andor-joining-the-score-network",
    "title": "SCORE Module Repository",
    "section": "Contributing and/or Joining the SCORE Network",
    "text": "Contributing and/or Joining the SCORE Network\nInterested in submitting your own module? Click here to find out more information about the SCORE Network module submission process."
  },
  {
    "objectID": "index.html#preprint-servers",
    "href": "index.html#preprint-servers",
    "title": "SCORE Module Repository",
    "section": "Preprint Servers",
    "text": "Preprint Servers\nYou can also access preprint repositories maintained by members of the SCORE Network. These preprint repositories are created and maintained by faculty and students from the respective institutions. Please note that these materials have not yet completed the required pedagogical and industry peer reviews to become a published module on the SCORE Network. However, instructors are still welcome to use these materials if they are so inclined.\n\nCarnegie Mellon University\nSt.¬†Lawrence University\nBaylor University + Azusa Pacific University\nWest Point"
  },
  {
    "objectID": "index.html#funding-source",
    "href": "index.html#funding-source",
    "title": "SCORE Module Repository",
    "section": "Funding Source",
    "text": "Funding Source\nThe development of the SCORE with Data network is funded by the National Science Foundation (award 2142705)."
  },
  {
    "objectID": "baseball/mlb_injuries/index.html",
    "href": "baseball/mlb_injuries/index.html",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "",
    "text": "NoteFacilitation notes\n\n\n\n\n\n\nThis module would be suitable for an in-class lab or take-home assignment in an intermediate statistics course.\nIt assumes a familiarity with the RStudio Environment and R programming language.\nStudents should be provided with the following data file (.csv) and Quarto document (.qmd) to produce visualizations and write up their answers to each exercise. Their final deliverable is to turn in an .html document produced by ‚ÄúRendering‚Äù the .qmd.\n\nMonthly Injury Data\nTommy John Surgeries Data\nStudent Quarto template\n\nPosit Cloud (via an Instructor account) or Github classroom are good options for disseminating files to students, but simply uploading files to your university‚Äôs course management system works, too.\nThe data for the mlb_injuries_monthly.csv file was derived from data found on prosportstransactions.com. The original data from the site contained rows of observations showing each transaction that involved sending a player to the injured list or bringing a player off of the injured list. The data was then filtered to include only times when a player was sent to the injured list and aggregated by month. The 2000 season was originally included in the data, but was removed after incomplete data was found for that season.\nThe data for the tj_surgeries_mlb_milb.csv file was derived from data accumulated by @MLBPlayerAnalys over many years. The original data lists any reported Tommy John surgery for a player now playing in the MLB or MiLB. This data was aggregated by year to produce the data used in this module.",
    "crumbs": [
      "Home",
      "Baseball",
      "MLB Injuries - Introductory Time Series Analysis"
    ]
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#classical-decomposition",
    "href": "baseball/mlb_injuries/index.html#classical-decomposition",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "Classical Decomposition",
    "text": "Classical Decomposition\nThere are two common methods for decomposing time series data: classical decomposition and STL decomposition. In this module we will focus on classical decomposition.\nClassical decomposition (additive) has four main steps:\n\nComputing a trend-cycle component (\\(T_t\\)) using a moving average. A moving average is a technique for smoothing time series data by averaging the values of neighboring points. This helps to remove short-term fluctuations and highlight longer-term trends.\nComputing a series without the trend-cycle component (\\(y_t - T_t\\)).\nEstimating the seasonal component (\\(S_t\\)) by averaging the values from the detrended series for the season.\nComputing the remainder component (\\(R_t\\)) by subtracting the trend-cycle and seasonal components from the original series. \\(R_t = y_t - T_t - S_t\\)\n\nWe can use the classical_decomposition function inside of the model() function to decompose our time series data.\n\ninjuries |&gt; \n  model(classical_decomposition(Count)) |&gt;\n  components() |&gt;\n  autoplot() +\n  labs(title = \"Classical Additive Decomposition of Monthly MLB Injury Counts\")\n\n\n\n\n\n\n\n\nClassical multiplicative decomposition works similarly to the additive decomposition, but with a few key differences.\n\nThe detrended series is computed as \\(y_t / T_t\\) instead of \\(y_t - T_t\\).\nThe seasonal component is estimated as \\(S_t = y_t / T_t\\) instead of \\(y_t - T_t\\).\nThe remainder component is computed as \\(R_t = y_t / (T_t \\times S_t)\\) instead of \\(y_t - T_t - S_t\\).\n\n\n\nNOTE: Ideally, after doing a decomposition, the remainder component should be white noise.\nClassical multiplicative decomposition can be used by setting the type argument to \"multiplicative\" in the classical_decomposition() function.",
    "crumbs": [
      "Home",
      "Baseball",
      "MLB Injuries - Introductory Time Series Analysis"
    ]
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#stl-decomposition",
    "href": "baseball/mlb_injuries/index.html#stl-decomposition",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "STL Decomposition",
    "text": "STL Decomposition\nSeasonal and Trend decomposition using Loess (STL) is a more advanced method for decomposing time series data. It is more flexible than classical decomposition and can handle any type of seasonality, not just monthly or quarterly. It also allows the user to control the length of the smoothing window for the trend-cycle. Lastly, it is more robust to outliers so that they do not affect the trend and seasonal estimates as much.\nBelow is an example of how to use the STL() function to decompose the time series data.\n\ninjuries |&gt; \n  model(STL(Count ~ trend(window = 21)+ \n            season(window = \"periodic\"),\n            robust = TRUE)) |&gt;\n  components() |&gt;\n  autoplot() +\n  labs(title = \"STL Additive Decomposition of Monthly MLB Injury Counts\")\n\n\n\n\n\n\n\n\n\n\nTIP: The window argument in the trend() function controls the length of the smoothing window for the trend-cycle. The larger the window, the smoother the trend. This value should always be an odd number so that a central point can be used. trend(window = 21) is a common choice for monthly data. This relatively large window size helps to prevent the trend from being influenced by short-term fluctuations in just a single year, but rather capture long-term trends and cycles.\nTIP: The window argument in the season() function controls how many years the seasonal component should be estimated over. The default value is 11. When the seasonal window is set to periodic season(window = \"periodic\"), it is the equivalent setting the window to all of the data. When periodic is used, the seasonal component is assumed to be the same each year. The seasonal window argument should always be an odd number or ‚Äúperiodic‚Äù.\n\n\n\n\n\n\nNoteExercise 3: Changing Decomposition Types\n\n\n\n\n\nCreate a classical multiplicative decomposition of the monthly MLB injury counts. How do the components differ from the additive decomposition? Which decomposition method‚Äôs remainder component looks more like white noise (classical additive or classical multiplicative)?\nCreate an STL decomposition of the monthly MLB injury counts with a shorter length for the trend smoothing window. How does the decomposition change with a shorter trend smoothing window? Particularly, how does the trend component change?\n\n\n\n\nNOTE: You can actually forecast with decomposition as well. If you‚Äôd like to learn more about this click here",
    "crumbs": [
      "Home",
      "Baseball",
      "MLB Injuries - Introductory Time Series Analysis"
    ]
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#the-mean-method",
    "href": "baseball/mlb_injuries/index.html#the-mean-method",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "The Mean Method",
    "text": "The Mean Method\nAn extremely simple method for forecasting is the mean method. This method forecasts the next observation as the average of all the observations in the training data. This method will produce a flat forecast that is equal to the mean of the training data. The mean method is useful when the data doesn‚Äôt have a trend or seasonality.",
    "crumbs": [
      "Home",
      "Baseball",
      "MLB Injuries - Introductory Time Series Analysis"
    ]
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#the-naive-method",
    "href": "baseball/mlb_injuries/index.html#the-naive-method",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "The Naive Method",
    "text": "The Naive Method\nThe naive method is another simple forecasting method. It forecasts the next observation as the value of the last observation in the training data. This method will produce a flat forecast that is equal to the last observation in the training data. The naive method is useful when the data appears to be random.",
    "crumbs": [
      "Home",
      "Baseball",
      "MLB Injuries - Introductory Time Series Analysis"
    ]
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#seasonal-naive-method",
    "href": "baseball/mlb_injuries/index.html#seasonal-naive-method",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "Seasonal Naive Method",
    "text": "Seasonal Naive Method\nThe seasonal naive method is similar to the naive method, but it forecasts the next observation as the value from the same season in the previous year. This method will produce a repeating pattern of forecasts that are equal to the observations from the same season in the previous year. (Basically forever repeating the last year‚Äôs pattern). The seasonal naive method is useful when the data has a strong seasonal pattern but no trend.",
    "crumbs": [
      "Home",
      "Baseball",
      "MLB Injuries - Introductory Time Series Analysis"
    ]
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#drift-method",
    "href": "baseball/mlb_injuries/index.html#drift-method",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "Drift Method",
    "text": "Drift Method\nThe drift method is a simple forecasting method that assumes a linear trend in the data. It forecasts the next observation as the value of the last observation plus the average change between observations. This method will produce a forecast that will continue on a linear trend from the first observation and through the last observation in the training data. The drift method is useful when the data has a linear trend but no seasonality.\n\n\n\n\n\n\nNoteExercise 4: Basic Forecasting Methods\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich of the above time series plots would be best forecasted using the mean method?\nWhich of the above time series plots would be best forecasted using the seasonal naive method?\nWhich of the above time series plots would be best forecasted using the drift method?\n\nUse the following plots of the monthly MLB injury counts and forecasts to answer the following questions.\n\n\n\n\n\n\n\n\n\n\nAfter looking at the forecasts from the mean, naive, seasonal naive, and drift methods for the MLB injury data, which method appears to be the best forecast?\nWhich ones appear to be the worst forecasts?\nWhat is one major issue seen in all of the forecasts in regards to their prediction intervals?",
    "crumbs": [
      "Home",
      "Baseball",
      "MLB Injuries - Introductory Time Series Analysis"
    ]
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#residuals",
    "href": "baseball/mlb_injuries/index.html#residuals",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "Residuals",
    "text": "Residuals\nChecking the residuals of a model is one of the most effective ways to see how well the model is performing. Residuals are the differences between the observed values and the values predicted by the model. \\(e_t = y_t - \\hat{y}_t\\)\n\n\nIt is important to note that when we are talking about residuals in this module that we are referring to the innovation residuals. Most of the time innovation residuals are the same as regular residuals, such as with our seasonal naive model. Innovation residuals are the residuals that are left over after accounting for changes made to the data such as transformations or differencing. These residuals are the ones that are used to check the model assumptions and to evaluate the model‚Äôs performance.\nThere are 3 main things to look at when evaluating residuals:\n\nDo the residuals appear to be white noise? Remember that white noise has a mean of 0, constant variance, and shows no obvious patterns. This can be checked by looking at a time plot of the residuals.\nAre the residuals normally distributed? This can be checked by looking at a histogram of the residuals or by using a normal probability plot.\nAre the residuals uncorrelated? This can be checked by looking at the ACF plot of the residuals. There are also statistical tests that can be used to check for autocorrelation in the residuals such as the Ljung-Box test. We can use the Box.test() function in R to perform this test.\n\n\n\nThe formula for the test statistic for a Ljung-Box test is:\n\\[Q^{*} = T(T+2) \\sum_{k=1}^{l} \\frac{r^2_k}{T-k}\\]\nwhere:\n\n\\(T\\) is the number of observations\n\\(l\\) is the max number of lags\n\\(r_k\\) is the sample autocorrelation at lag \\(k\\)\n\nThe null hypothesis for the Ljung-Box test is that the data is not distinguishable from white noise. If the p-value is less than the significance level, we reject the null hypothesis and conclude that the data is autocorrelated.\nThankfully there is a very easy way to check all of these at once in R using the gg_tsresiduals() function.\n\n\n\n\n\n\nNoteExercise 5: Residuals Analysis\n\n\n\nBelow is code that will create a time plot, histogram, and ACF plot of the residuals from the seasonal naive method.\n\nsnaive_mod &lt;- injuries |&gt;\n  model(SNAIVE(Count ~ lag('year')))\n  \nsnaive_mod |&gt; \n  gg_tsresiduals()\n\n\n\n\n\n\n\n\n\nDo the residuals appear to be white noise?\nWhat stands out about the time plot of the residuals?\nDoes the histogram of the residuals appear to be normally distributed?\nAre the residuals uncorrelated? What lag(s) show the most significant autocorrelation and what could this mean for the model?",
    "crumbs": [
      "Home",
      "Baseball",
      "MLB Injuries - Introductory Time Series Analysis"
    ]
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#testing-and-training-for-point-estimate-evaluations",
    "href": "baseball/mlb_injuries/index.html#testing-and-training-for-point-estimate-evaluations",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "Testing and Training for Point Estimate Evaluations",
    "text": "Testing and Training for Point Estimate Evaluations\nIf you want to evaluate the point estimates of a model, you can use a testing and training split. This involves training the model on the first part of the data and then testing the model on the second part of the data. This allows you to see how well the model can forecast future observations.\nFor this example, we will split the data into a training set that contains the first 75% of the data and a testing set that contains the last 25% of the data. This means we will train the models on the data from January 2001 to December 2017 and test the models on the data from January 2018 to December 2023.\n\n\nTIP: 75-25 is a common split for training and testing data, but you can use any split that makes sense for your data. Generally the more data you have, the less percentage you need for testing. Other common splits are 70-30 or 80-20.\n\ntraining &lt;- injuries |&gt; filter(year(Month) &lt; 2018)\ntesting &lt;- injuries |&gt; filter(year(Month) &gt;= 2018)\n\nNow that we have our training and testing data, we can fit the models to the training data and then forecast the testing data.\n\ninjury_fit &lt;- training |&gt; \n  model(mean = MEAN(Count),\n        naive = NAIVE(Count),\n        snaive = SNAIVE(Count ~ lag('year')),\n        drift = RW(Count ~ drift()))\n\ninjury_forecasts &lt;- injury_fit |&gt; \n  forecast(new_data = testing)\n\n\n\nTIP: You can fit multiple models at once by using the model() function as seen in the code to the left. The values to the left of the = are the names we are giving to the models and the values to the right of the = are the models we are fitting to the data.\nTIP: When using the forecast() function, you can specify the new data you want to forecast by using the new_data argument. In this case, we are forecasting for the testing data.\nLet‚Äôs visualize the forecasts from the training data and compare them to the testing data.\n\ninjury_forecasts |&gt;\n  autoplot(injuries, level = NULL) +\n  labs(title = \"Forecasting Methods for Monthly MLB Injury Counts\")+\n  guides(color = guide_legend(title = \"Forecast\"))\n\n\n\n\n\n\n\n\nThe seasonal naive method certainly appears to be the best forecast.\nWe can also evaluate these models using the accuracy() function. This function calculates a variety of accuracy measures for the forecasts, including the mean absolute error, root mean squared error, mean absolute percentage error, and more.\n\nThe mean absolute error (MAE) is the average of the absolute errors between the forecasts and the actual values. It is a measure of the average magnitude of the errors in the forecasts. We want this value to be as close to 0 as possible.\n\n\nThe formula for the mean absolute error is: \\[\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} | y_i - \\hat{y}_i |\\] where \\(y_i\\) is the actual value and \\(\\hat{y}_i\\) is the forecasted value.\n\nThe root mean squared error (RMSE) is the square root of the average of the squared errors between the forecasts and the actual values. It is a measure of the standard deviation of the errors in the forecasts. We want this value to be as close to 0 as possible.\n\n\nThe formula for the root mean squared error is: \\[\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\\] Where \\(y_i\\) is the actual value and \\(\\hat{y}_i\\) is the forecasted value.\n\nThe mean absolute percentage error (MAPE) is the average of the absolute percentage errors between the forecasts and the actual values. It is a measure of the accuracy of the forecasts. We want this value to be as close to 0 as possible.\n\n\nThe formula for the mean absolute percentage error is: \\[\\text{MAPE} = \\frac{100}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right|\\] where \\(y_i\\) is the actual value and \\(\\hat{y}_i\\) is the forecasted value.\n\nThe code below displays the accuracy measures for the forecasts.\n\naccuracy(injury_forecasts, testing)\n\n# A tibble: 4 √ó 10\n  .model .type    ME  RMSE   MAE   MPE  MAPE  MASE RMSSE  ACF1\n  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 drift  Test   86.0 122.   86.0   Inf   Inf   NaN   NaN 0.698\n2 mean   Test   36.2  93.9  79.2  -Inf   Inf   NaN   NaN 0.698\n3 naive  Test   86.0 122.   86.0   100   100   NaN   NaN 0.698\n4 snaive Test   14.3  58.4  34.5  -Inf   Inf   NaN   NaN 0.424\n\n\nThis confirms that the seasonal naive method is the best forecast, as it has the lowest MAE and RMSE values. The MAPE is shown at -Inf to Inf because some of the actual values are 0, which causes the percentage error to be infinite.",
    "crumbs": [
      "Home",
      "Baseball",
      "MLB Injuries - Introductory Time Series Analysis"
    ]
  },
  {
    "objectID": "baseball/stolen-bases/index.html",
    "href": "baseball/stolen-bases/index.html",
    "title": "Stolen Bases",
    "section": "",
    "text": "This lesson introduces students to the concept of normality tests (Shapiro-Wilks and Kolmogorov-Smirnov) and summation of normal distributions to investigate stolen base success rates. Featuring Jacob Hurtubise, a West Point‚Äôs all-time leader in stolen bases and baseball player for the Cincinnati Reds‚Äô Double-A affiliate, the Chattanooga Lookouts.",
    "crumbs": [
      "Home",
      "Baseball",
      "Stolen Bases"
    ]
  },
  {
    "objectID": "baseball/stolen-bases/index.html#motivation",
    "href": "baseball/stolen-bases/index.html#motivation",
    "title": "Stolen Bases",
    "section": "",
    "text": "This lesson introduces students to the concept of normality tests (Shapiro-Wilks and Kolmogorov-Smirnov) and summation of normal distributions to investigate stolen base success rates. Featuring Jacob Hurtubise, a West Point‚Äôs all-time leader in stolen bases and baseball player for the Cincinnati Reds‚Äô Double-A affiliate, the Chattanooga Lookouts.",
    "crumbs": [
      "Home",
      "Baseball",
      "Stolen Bases"
    ]
  },
  {
    "objectID": "baseball/stolen-bases/index.html#module",
    "href": "baseball/stolen-bases/index.html#module",
    "title": "Stolen Bases",
    "section": "Module",
    "text": "Module\nhttps://isle.stat.cmu.edu/SCORE/stolen-bases-module/",
    "crumbs": [
      "Home",
      "Baseball",
      "Stolen Bases"
    ]
  },
  {
    "objectID": "esports/index.html",
    "href": "esports/index.html",
    "title": "Esports",
    "section": "",
    "text": "These modules use esports data to teach topics in statistics and data science.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeague of Legends - Buffing and Nerfing\n\n\n\noutliers\n\nsummary statistics\n\n\n\nInvestigating game play statistics for League of Legends champions in two different patches.\n\n\n\n\n\nFeb 21, 2024\n\n\nIvan Ramler, George Charalambous, A.J. Dykstra\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Esports"
    ]
  },
  {
    "objectID": "fitness_and_training/dumbbell_barbell_bench_ratio/index.html",
    "href": "fitness_and_training/dumbbell_barbell_bench_ratio/index.html",
    "title": "Strength Ratios - Flat Dumbbell Press to Barbell Bench Press",
    "section": "",
    "text": "If you are unfamiliar with dumbbell or flat bench press weightlifting, please watch this video:\n\n\n\n\n\nWeightlifters may be inclined to track the specific ratio between flat dumbbell press and barbell bench press.\nA dumbbell is a short bar with equal weight on both sides designed to be held in the lifter‚Äôs hands. Flat dumbbell press is when the lifter lies on a flat bench with their arms positioned at roughly 45 degree angles, lifts two equal sized dumbbells, brings them back to the chest, and then keeps going.\nA barbell is similar in shape to a dumbbell, but is a longer, much heavier bar where multiple weighted plates can be placed on either side. Barbell bench press is when the lifter lies on a bench with the barbell positioned at their chest, lifts the barbell, and then brings it back down.\nThe ratio of flat dumbbell press to barbell bench press works to track how much someone is lifting two dumbbells compared to how much they are able to lift the weighted barbell. When calculating the ratio, it was multiplied by two in order to account for the total weight lifted when doing a flat dumbbell press, as there are two dumbbells that would have to be held by the lifter.\nAn example of this is as follows:\nSay Melissa is able to flat dumbbell press two 30 lbs dumbbells. The total amount she can flat dumbbell press is 60 lbs (one dumbbell for each arm). On barbell bench press, she can lift 75 lbs. Her ratio would be calculated by dividing flat dumbbell press by barbell bench press (60/75) which equals 0.8, meaning that Melissa can lift two dumbbells up to 80% as heavy as she can bench press the barbell.\nWhile both exercises predominately target the chest muscles, each can focus on different secondary muscle groups. Many weightlifters like to track this ratio to work towards specific strength goals and target any weakness in certain muscle groups1.\nFor example, if someone‚Äôs ratio is low (i.e.¬†they can barbell bench press much more than they can flat dumbbell press), it highlights that they might want to work on targeting the secondary muscles used in flat dumbbell press. In this case, these muscles are usually referred to as the stabilizer muscles. This could mean they rely more on the barbell for their pressing strength and might benefit from incorporating more dumbbell exercises to improve muscle balance and stability.\nOn the flip side, if someone‚Äôs ratio is high, it might indicate strong stabilizer muscles and good overall balance. This could mean they have well-developed coordination and muscular control, potentially reducing the risk of injury and enhancing overall athletic performance. However, it might also suggest that their barbell press strength is relatively underdeveloped, implying a need to focus more on barbell training to ensure balanced muscle development or to increase their overall strength.\nFinally, a lifter might be interested in being close to the average ratio of dumbbell to barbell press because it indicates a balanced strength development. In particular, this can be useful if they are able to compare themselves to the average from a group of peers. Being close to the average suggests that their training regimen is effective in developing both primary pressing muscles and stabilizer muscles evenly. This balance can contribute to better overall performance, reduced injury risk, and a more well-rounded physique.\nThe data on the ratio between dumbbell and barbell weightlifting comes from responses in a Reddit thread on the r/fitness subreddit. People who tend to use the fitness subreddit are likely fitness enthusiasts - including beginners, intermediate, and advanced lifters. This community is diverse, with members ranging from casual gym-goers to serious athletes. As with most social media, typical ages of users on the fitness subreddit tend to be in the younger demographic, often ranging from late teens to early 30s. However, there are still users of all ages, including older individuals who are interested in maintaining their fitness and connecting with the community.\nThis module involves using the strength ratio to look at dot plots, construct and use confidence intervals, and discuss the effect that data collection methods have on the reliability of the data.\n\n\n\n\n\n\nNoteActivity Length\n\n\n\n\n\nThis could serve as an in class activity and should take roughly 30 minutes to complete.\n\n\n\n\n\n\n\n\n\nNoteLearning Objectives\n\n\n\n\n\nBy the end of this activity, you will:\n\nEnhance proficiency in constructing and interpreting confidence intervals.\nUnderstand the limitations of using smaller datasets of self-selected individuals.\nExplain the importance of obtaining a sample that reaches across multiple audiences.\n\n\n\n\n\n\n\n\n\n\nNoteMethods\n\n\n\n\n\nStudents are expected to have been exposed to one-sample inference for means (most likely t-intervals). Students will also need to have been exposed to general sampling practices - including common biases or problematics sampling schemes such as convenience samples and self-reported data.\n\n\n\n\n\n\n\n\n\nNoteTechnology Requirements\n\n\n\n\n\nNo explicit technology is required, although a calculator is recommended. Additionally, the module worksheets can easily be adapted to be used with statistical software.\n\n\n\n\n\n\nThe dumbbell_barbell_weight_ratio data set contains observed strength ratios from 18 different weightlifters. Each row represents a different weightlifter.\nData is available on the SCORE Data Repository Website\nDownload data: dumbbell_barbell_weight_ratio.csv\n\n\nVariable Descriptions\n\n\n\n\nVariable\nDescription\n\n\n\n\nRatio\nFlat dumbbell press to barbell bench press weight ratio\n\n\n\n\n\n\nWeight Ratio\nThe data is sourced from a self-reported Reddit open forum. Users provided their weight for both a flat dumbbell press and a barbell bench press, and it was compiled by another user into the corresponding weight ratio. It is important to note that as this is a self-reported open forum, biases may be introduced that wouldn‚Äôt otherwise be present.\nNote that the Reddit user that compiled the data calculated the ratio as the weight of a single dumbbell divided by the barbell weight. The data used in this module uses the weight of the pair of dumbbells divided by the barbell weight (to represent a ratio of the total amount of weight being lifted for each method). To revert to the original source style, simply divide the data from this module by two.\n\n\n\n\nClass handout\nClass handout - with solutions\n\n\n\n\n\n\nNoteConclusion\n\n\n\n\n\nUpon conclusion, students should recognize that the data collected likely strictly limits which population the resulting confidence interval can be applied to. Additionally, students should recognize that, even with the limitations of these data, they can still provide value.\nThis learning module offers valuable insight into the process of data collection and the role it has in making concrete conclusions. There are some obvious biases in the way the data was collected, those who report their ratios belong to similar audiences, and we can‚Äôt assume that the sample is representative of a broad population. Nevertheless, it presents a valuable opportunity for thought and good practice at constructing one sample t tests and recognizing the extent to which they can be used.",
    "crumbs": [
      "Home",
      "Fitness and Training",
      "Strength Ratios - Flat Dumbbell Press to Barbell Bench Press"
    ]
  },
  {
    "objectID": "fitness_and_training/dumbbell_barbell_bench_ratio/index.html#module",
    "href": "fitness_and_training/dumbbell_barbell_bench_ratio/index.html#module",
    "title": "Strength Ratios - Flat Dumbbell Press to Barbell Bench Press",
    "section": "",
    "text": "If you are unfamiliar with dumbbell or flat bench press weightlifting, please watch this video:\n\n\n\n\n\nWeightlifters may be inclined to track the specific ratio between flat dumbbell press and barbell bench press.\nA dumbbell is a short bar with equal weight on both sides designed to be held in the lifter‚Äôs hands. Flat dumbbell press is when the lifter lies on a flat bench with their arms positioned at roughly 45 degree angles, lifts two equal sized dumbbells, brings them back to the chest, and then keeps going.\nA barbell is similar in shape to a dumbbell, but is a longer, much heavier bar where multiple weighted plates can be placed on either side. Barbell bench press is when the lifter lies on a bench with the barbell positioned at their chest, lifts the barbell, and then brings it back down.\nThe ratio of flat dumbbell press to barbell bench press works to track how much someone is lifting two dumbbells compared to how much they are able to lift the weighted barbell. When calculating the ratio, it was multiplied by two in order to account for the total weight lifted when doing a flat dumbbell press, as there are two dumbbells that would have to be held by the lifter.\nAn example of this is as follows:\nSay Melissa is able to flat dumbbell press two 30 lbs dumbbells. The total amount she can flat dumbbell press is 60 lbs (one dumbbell for each arm). On barbell bench press, she can lift 75 lbs. Her ratio would be calculated by dividing flat dumbbell press by barbell bench press (60/75) which equals 0.8, meaning that Melissa can lift two dumbbells up to 80% as heavy as she can bench press the barbell.\nWhile both exercises predominately target the chest muscles, each can focus on different secondary muscle groups. Many weightlifters like to track this ratio to work towards specific strength goals and target any weakness in certain muscle groups1.\nFor example, if someone‚Äôs ratio is low (i.e.¬†they can barbell bench press much more than they can flat dumbbell press), it highlights that they might want to work on targeting the secondary muscles used in flat dumbbell press. In this case, these muscles are usually referred to as the stabilizer muscles. This could mean they rely more on the barbell for their pressing strength and might benefit from incorporating more dumbbell exercises to improve muscle balance and stability.\nOn the flip side, if someone‚Äôs ratio is high, it might indicate strong stabilizer muscles and good overall balance. This could mean they have well-developed coordination and muscular control, potentially reducing the risk of injury and enhancing overall athletic performance. However, it might also suggest that their barbell press strength is relatively underdeveloped, implying a need to focus more on barbell training to ensure balanced muscle development or to increase their overall strength.\nFinally, a lifter might be interested in being close to the average ratio of dumbbell to barbell press because it indicates a balanced strength development. In particular, this can be useful if they are able to compare themselves to the average from a group of peers. Being close to the average suggests that their training regimen is effective in developing both primary pressing muscles and stabilizer muscles evenly. This balance can contribute to better overall performance, reduced injury risk, and a more well-rounded physique.\nThe data on the ratio between dumbbell and barbell weightlifting comes from responses in a Reddit thread on the r/fitness subreddit. People who tend to use the fitness subreddit are likely fitness enthusiasts - including beginners, intermediate, and advanced lifters. This community is diverse, with members ranging from casual gym-goers to serious athletes. As with most social media, typical ages of users on the fitness subreddit tend to be in the younger demographic, often ranging from late teens to early 30s. However, there are still users of all ages, including older individuals who are interested in maintaining their fitness and connecting with the community.\nThis module involves using the strength ratio to look at dot plots, construct and use confidence intervals, and discuss the effect that data collection methods have on the reliability of the data.\n\n\n\n\n\n\nNoteActivity Length\n\n\n\n\n\nThis could serve as an in class activity and should take roughly 30 minutes to complete.\n\n\n\n\n\n\n\n\n\nNoteLearning Objectives\n\n\n\n\n\nBy the end of this activity, you will:\n\nEnhance proficiency in constructing and interpreting confidence intervals.\nUnderstand the limitations of using smaller datasets of self-selected individuals.\nExplain the importance of obtaining a sample that reaches across multiple audiences.\n\n\n\n\n\n\n\n\n\n\nNoteMethods\n\n\n\n\n\nStudents are expected to have been exposed to one-sample inference for means (most likely t-intervals). Students will also need to have been exposed to general sampling practices - including common biases or problematics sampling schemes such as convenience samples and self-reported data.\n\n\n\n\n\n\n\n\n\nNoteTechnology Requirements\n\n\n\n\n\nNo explicit technology is required, although a calculator is recommended. Additionally, the module worksheets can easily be adapted to be used with statistical software.\n\n\n\n\n\n\nThe dumbbell_barbell_weight_ratio data set contains observed strength ratios from 18 different weightlifters. Each row represents a different weightlifter.\nData is available on the SCORE Data Repository Website\nDownload data: dumbbell_barbell_weight_ratio.csv\n\n\nVariable Descriptions\n\n\n\n\nVariable\nDescription\n\n\n\n\nRatio\nFlat dumbbell press to barbell bench press weight ratio\n\n\n\n\n\n\nWeight Ratio\nThe data is sourced from a self-reported Reddit open forum. Users provided their weight for both a flat dumbbell press and a barbell bench press, and it was compiled by another user into the corresponding weight ratio. It is important to note that as this is a self-reported open forum, biases may be introduced that wouldn‚Äôt otherwise be present.\nNote that the Reddit user that compiled the data calculated the ratio as the weight of a single dumbbell divided by the barbell weight. The data used in this module uses the weight of the pair of dumbbells divided by the barbell weight (to represent a ratio of the total amount of weight being lifted for each method). To revert to the original source style, simply divide the data from this module by two.\n\n\n\n\nClass handout\nClass handout - with solutions\n\n\n\n\n\n\nNoteConclusion\n\n\n\n\n\nUpon conclusion, students should recognize that the data collected likely strictly limits which population the resulting confidence interval can be applied to. Additionally, students should recognize that, even with the limitations of these data, they can still provide value.\nThis learning module offers valuable insight into the process of data collection and the role it has in making concrete conclusions. There are some obvious biases in the way the data was collected, those who report their ratios belong to similar audiences, and we can‚Äôt assume that the sample is representative of a broad population. Nevertheless, it presents a valuable opportunity for thought and good practice at constructing one sample t tests and recognizing the extent to which they can be used.",
    "crumbs": [
      "Home",
      "Fitness and Training",
      "Strength Ratios - Flat Dumbbell Press to Barbell Bench Press"
    ]
  },
  {
    "objectID": "fitness_and_training/dumbbell_barbell_bench_ratio/index.html#references",
    "href": "fitness_and_training/dumbbell_barbell_bench_ratio/index.html#references",
    "title": "Strength Ratios - Flat Dumbbell Press to Barbell Bench Press",
    "section": "References",
    "text": "References\n\nThumbnail image taken from https://adventurefitness.club/barbell-vs-dumbbell-bench-press/",
    "crumbs": [
      "Home",
      "Fitness and Training",
      "Strength Ratios - Flat Dumbbell Press to Barbell Bench Press"
    ]
  },
  {
    "objectID": "fitness_and_training/dumbbell_barbell_bench_ratio/index.html#how-to-cite",
    "href": "fitness_and_training/dumbbell_barbell_bench_ratio/index.html#how-to-cite",
    "title": "Strength Ratios - Flat Dumbbell Press to Barbell Bench Press",
    "section": "How to Cite",
    "text": "How to Cite\nIf you use this module in your work, please cite it as follows:\nJohnson, V. & Ramler, I. (2026, February 24). Strength Ratios - Flat Dumbbell Press to Barbell Bench Press. ‚ÄúThe SCORE Network.‚Äù https://doi.org/10.17605/OSF.IO/SQ43F\nYou can include this citation directly in your references or bibliography.",
    "crumbs": [
      "Home",
      "Fitness and Training",
      "Strength Ratios - Flat Dumbbell Press to Barbell Bench Press"
    ]
  },
  {
    "objectID": "fitness_and_training/dumbbell_barbell_bench_ratio/index.html#footnotes",
    "href": "fitness_and_training/dumbbell_barbell_bench_ratio/index.html#footnotes",
    "title": "Strength Ratios - Flat Dumbbell Press to Barbell Bench Press",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlthough many weightlifters may use this ratio for tracking progress and examining muscle balance, it isn‚Äôt an official statistic used in competitions, only a personal one.‚Ü©Ô∏é",
    "crumbs": [
      "Home",
      "Fitness and Training",
      "Strength Ratios - Flat Dumbbell Press to Barbell Bench Press"
    ]
  },
  {
    "objectID": "football/nfl-elo-ratings/index.html",
    "href": "football/nfl-elo-ratings/index.html",
    "title": "Introduction to Elo ratings",
    "section": "",
    "text": "Elo ratings are one of the most popular approaches for estimating player/team strength across a variety of sports. You can find a number of different sports examples maintained by sportswriter Neil Paine, as well as older versions that were featured in the popular website FiveThirtyEight. These dynamic ratings are adjusted for opponent strength and can be used for historical comparisons, such as who is the greatest tennis player of all time?, and for predicting outcomes. In this module you will learn the basics of Elo ratings in the context of measuring NFL team strength, walking through steps to implement and assess Elo ratings from scratch in R.",
    "crumbs": [
      "Home",
      "Football",
      "Introduction to Elo ratings"
    ]
  },
  {
    "objectID": "football/nfl-elo-ratings/index.html#motivation",
    "href": "football/nfl-elo-ratings/index.html#motivation",
    "title": "Introduction to Elo ratings",
    "section": "",
    "text": "Elo ratings are one of the most popular approaches for estimating player/team strength across a variety of sports. You can find a number of different sports examples maintained by sportswriter Neil Paine, as well as older versions that were featured in the popular website FiveThirtyEight. These dynamic ratings are adjusted for opponent strength and can be used for historical comparisons, such as who is the greatest tennis player of all time?, and for predicting outcomes. In this module you will learn the basics of Elo ratings in the context of measuring NFL team strength, walking through steps to implement and assess Elo ratings from scratch in R.",
    "crumbs": [
      "Home",
      "Football",
      "Introduction to Elo ratings"
    ]
  },
  {
    "objectID": "football/nfl-elo-ratings/index.html#learning-objectives",
    "href": "football/nfl-elo-ratings/index.html#learning-objectives",
    "title": "Introduction to Elo ratings",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this module, you will be able to:\n\nCompute the expected outcome or predicted probability based on player/team ratings.\nUpdate ratings following the observed outcome of a match/game.\nImplement the complete Elo ratings framework in R for a full NFL season.\nAssess Elo rating predictions using Brier scores.\nTune the update factor and other settings to yield more optimal predictions.",
    "crumbs": [
      "Home",
      "Football",
      "Introduction to Elo ratings"
    ]
  },
  {
    "objectID": "football/nfl-elo-ratings/index.html#data",
    "href": "football/nfl-elo-ratings/index.html#data",
    "title": "Introduction to Elo ratings",
    "section": "Data",
    "text": "Data\nThe dataset and description are available at the SCORE Network Data Repository.",
    "crumbs": [
      "Home",
      "Football",
      "Introduction to Elo ratings"
    ]
  },
  {
    "objectID": "football/nfl-elo-ratings/index.html#module-materials",
    "href": "football/nfl-elo-ratings/index.html#module-materials",
    "title": "Introduction to Elo ratings",
    "section": "Module Materials",
    "text": "Module Materials\n\n\n\n\n\n\nWarningPrerequisites\n\n\n\nPrior to working on through this module, students are expected to know the following:\n\nFamiliar with R with the ability to read and write functions.\nSome exposure to predicting outcomes with probabilities.\n\nThe module has sections indicating which portions are challenging exercises, and is designed to take an undergraduate student roughly 1-3 hours to complete (and 3-4 hours with the challenge exercise).\n\n\nStudent assignment qmd file\nView instructor solutions",
    "crumbs": [
      "Home",
      "Football",
      "Introduction to Elo ratings"
    ]
  },
  {
    "objectID": "football/nfl-elo-ratings/index.html#how-to-cite",
    "href": "football/nfl-elo-ratings/index.html#how-to-cite",
    "title": "Introduction to Elo ratings",
    "section": "How to Cite",
    "text": "How to Cite\nIf you use this module in your work, please cite it as follows:\nYurko, R. (2025, March 11). Introduction to ELO Ratings. ‚ÄúThe SCORE Network.‚Äù https://doi.org/10.17605/OSF.IO/DHUQ2\nYou can include this citation directly in your references or bibliography.",
    "crumbs": [
      "Home",
      "Football",
      "Introduction to Elo ratings"
    ]
  },
  {
    "objectID": "games/index.html",
    "href": "games/index.html",
    "title": "Games",
    "section": "",
    "text": "These modules use games data to teach topics in statistics and data science.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlackJack Logistic Regression\n\n\n\nLogistic Regression\n\nBinary Data\n\n\n\nAn Introduction to Logistic Regression Using BlackJack\n\n\n\n\n\nJul 23, 2025\n\n\nAndrew Tran, Nicholas Clark\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Games"
    ]
  },
  {
    "objectID": "golf/index.html",
    "href": "golf/index.html",
    "title": "Golf",
    "section": "",
    "text": "These modules use golf data to teach topics in statistics and data science.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPGA - Drive for Show, Putt for Dough?\n\n\n\nCorrelation\n\n\n\nUsing tournament data for professional golfers to see if driving or putting are more strongly related to success.\n\n\n\n\n\nOct 23, 2025\n\n\nAlyssa Bigness, Robin Lock\n\n\n\n\n\n\n\n\n\n\n\n\nPGA - Drive for Show, Putt for Dough?\n\n\n\nCorrelation\n\n\n\nUsing tournament data for professional golfers to see if driving or putting are more strongly related to success.\n\n\n\n\n\nOct 23, 2025\n\n\nAlyssa Bigness, Robin Lock\n\n\n\n\n\n\n\n\n\n\n\n\nPGA - Scoring Average Confidence Intervals\n\n\n\nSingle Mean Confidence Intervals\n\nSingle Mean Hypothesis Testing\n\n\n\nExploring Single Mean Confidence Intervals with Golf Data\n\n\n\n\n\nJan 25, 2025\n\n\nJonathan Lieb\n\n\n\n\n\n\n\n\n\n\n\n\nPGA - Scoring Average Confidence Intervals (No R)\n\n\n\nSingle Mean Confidence Intervals\n\nSingle Mean Hypothesis Testing\n\n\n\nExploring Single Mean Confidence Intervals with Golf Data\n\n\n\n\n\nJan 25, 2025\n\n\nJonathan Lieb\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Golf"
    ]
  },
  {
    "objectID": "golf/pga_masters/index.html",
    "href": "golf/pga_masters/index.html",
    "title": "PGA - Scoring Average Confidence Intervals",
    "section": "",
    "text": "NoteFacilitation notes\n\n\n\n\n\n\nThis module would be suitable for an in-class lab or take-home assignment in an introductory statistics course.\nIt assumes a basic familiarity with the RStudio Environment and R programming language.\nStudents should be provided with the following data file (.csv) and Quarto document (.qmd) to produce visualizations and write up their answers to each exercise. Their final deliverable is to turn in an .html document produced by ‚ÄúRendering‚Äù the .qmd.\n\nData\nStudent Quarto template\n\nThe data for this module is derived largely from the ESPN website. However, the tours were manually added to the data based on the what they played on in 2023 at the time of the Masters.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals"
    ]
  },
  {
    "objectID": "golf/pga_masters/index.html#terms-to-know",
    "href": "golf/pga_masters/index.html#terms-to-know",
    "title": "PGA - Scoring Average Confidence Intervals",
    "section": "Terms to know",
    "text": "Terms to know\nBefore proceeding with the analysis, let‚Äôs make sure we know some important golf terminology that will help us master this module.\n\nGolf Terminology\n\nPar in golf is the amount of strokes that a good golfer is expected to take to get the ball in the hole.\n\nEach hole in golf has its own par. There are par 3 holes, par 4 holes, and par 5 holes.\nThere are 18 holes on a golf course and the pars of each of these holes sums to par for the course, also known the course par.\n\nA round in golf is when a golfer plays the full set of 18 holes on the course.\n\nIn most professional golf tournaments, all golfers play 2 rounds, the best golfers are selected and those golfers play 2 more rounds for a total of 4 rounds.\n\n\n\n\n\n\n\n\nImportantTypes of Golf Tours\n\n\n\n\nIn golf there are a few tours, better thought of as leagues, that golfers regularly compete in\n\nThe PGA Tour, or ‚ÄúProfessional Golf Association Tour‚Äù, has long been considered the preeminant golfing tour, hosting most tournaments and containing the most skilled members.\nThe LIV Golf tour is a Saudi-backed alternative to the PGA Tour that began playing tournaments in 2022. LIV is the Roman numeral for 54 and is related to the fact that LIV tournaments only allow 54 players and only play 54 holes, compared to the normal PGA Tour 72 holes.\nThe PGA Tour Champions is a branch off of the PGA tour for players 50 or older. It used to be called the ‚ÄúSenior PGA Tour‚Äù until 2003, when it began being called the Champions tour\nAn amateur is a golfer who is not yet a professional. They are not allowed to win money in professional golf tournaments. Most amateurs are college golfers.\n\n\n\n\n\n\nPGA vs.¬†LIV\nClick here to read about LIV golf‚Äôs founding and its continued impact on the PGA tour.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals"
    ]
  },
  {
    "objectID": "golf/pga_masters/index.html#variable-descriptions",
    "href": "golf/pga_masters/index.html#variable-descriptions",
    "title": "PGA - Scoring Average Confidence Intervals",
    "section": "Variable descriptions",
    "text": "Variable descriptions\nThe masters_2023 data you‚Äôll be analyzing in this module provides scores for each round by each golfer in the 2023 Masters. The data includes the names of golfers, the round, their scores, and their tour.\n\n\nVariable Descriptions\n\n\n\n\nVariable\nDescription\n\n\n\n\nplayer\nGolfer‚Äôs name\n\n\nround\nRound of the tournament\n\n\nscore\nScore for the 18-hole course\n\n\ntour\nThe tour the player generally competes on",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals"
    ]
  },
  {
    "objectID": "golf/pga_masters/index.html#t-interval-for-single-means",
    "href": "golf/pga_masters/index.html#t-interval-for-single-means",
    "title": "PGA - Scoring Average Confidence Intervals",
    "section": "t-interval for single means",
    "text": "t-interval for single means\nA t-distribution is used for calculating a single mean confidence interval if the sample size is small (rule of thumb: less than 30) and the population standard deviation is unknown.\nThe formula for calculating a CI using this method is shown below: \\[CI = \\bar{X} \\pm t_{\\alpha/2, df} \\times \\frac{S}{\\sqrt{n}}\\]\n\n\nClick here to learn more about the t-distribution and play around with t-distribution graphs.\nWhere \\(\\bar{X}\\) is the sample mean,\n\\(t_{\\alpha/2, df}\\) is the critical value for the t-distribution with \\(df = n-1\\),\n\\(S\\) is the sample standard deviation,\nand \\(n\\) is the sample size.\n\n\nNOTE: The t-distribution changes based on the degrees of freedom, approaching the normal distribution as the degrees of freedom increase.\nThe critical value for the t-distribution is determined by the confidence level and the degrees of freedom. This is calculated by finding the value of \\(t_{\\alpha/2, df}\\) such that the area under the t-distribution curve (with that specific degrees of freedom) between \\(-t_{\\alpha/2, df}\\) and \\(t_{\\alpha/2, df}\\) is equal to the confidence level.\n\n\n\n\n\n\n\n\n\n\n\nNOTE: Using R to calculate the critical value for the t-distribution saves time and adds accuracy compared to using a t-table.\nR can easily calculate the critical value for the t-distribution using the qt() function.\nThe qt() function takes two arguments: the first is the confidence level, and the second is the degrees of freedom. The example below shows how to calculate the t-value for a 95% confidence level with 10 degrees of freedom.\nNote that the 95% confidence interval has \\(\\alpha = .05\\) and \\(\\alpha / 2 =  .025\\) so we need to use qt(.975, df= 10) or qt(.025, df = 10) to find the critical values.\n\nqt(0.975, df = 10)\n\n[1] 2.228139\n\n\n\n\n\n\n\n\nNoteExample: t-distribution confidence intervals\n\n\n\n\n\nWe would like to make a 90% confidence interval for the true mean of scoring for Jon Rahm at the Masters. We can pull his scores and put them in a vector with following code. We will also set the sample size (n) to the amount of observations in the sample.\n\njon_rahm &lt;- masters_2023 |&gt; \n  filter(player == \"Jon Rahm\") |&gt; \n  pull(score)\n\nn &lt;- length(jon_rahm)\n\nWe start by calculating the the sample mean (\\(\\bar{x}\\)).\n\nx_bar &lt;- mean(jon_rahm)\n\nNext we calculate the sample standard deviation (\\(s\\)).\n\ns &lt;- sd(jon_rahm)\n\nOur next step is to find the critical value for a 90% confidence interval with 3 degrees of freedom (\\(t_{\\alpha/2, df}\\)). We‚Äôll use the qt function from R to do this. Note that we use .95 as the first argument because alpha is .1 and half of our error should be in each end.\n\ncv &lt;- qt(.95, 3)\n\nLastly we will use the confidence interval formula to find the bounds of our 90% confidence interval.\n\nlower_confidence_boundary &lt;- x_bar - cv * (s/n)\nupper_confidence_boundary &lt;- x_bar + cv * (s/n)\npaste0(\"90% confidence interval: (\", lower_confidence_boundary, \", \", upper_confidence_boundary, \")\")\n\n[1] \"90% confidence interval: (67.078486801804, 70.921513198196)\"\n\n\nFinally we can interpret the confidence interval and say that we are 95% confident that the true mean of Jon Rahm‚Äôs scores at Augusta National is between 67.1 and 70.9.\n\n\n\n\n\n\n\n\n\nNoteExercise 2: t-distribution confidence intervals\n\n\n\n\nRun the code below to create the proper subset of the data for this exercise and calculate the sample mean and sample standard deviation.\n\namateurs_round1 &lt;- masters_2023 |&gt; \n  filter(round == 1, tour == \"Amateur\")\nx_bar &lt;- amateurs_round1 |&gt; \n  summarise(mean(score)) |&gt; \n  pull()\nstd_dev &lt;- amateurs_round1 |&gt; \n  summarize(sd(score)) |&gt; \n  pull()\nn &lt;- nrow(amateurs_round1)\nt_cv &lt;- qt(0.95, df = n - 1)\n\nUse the formula for creating a confidence interval using the t-distribution to calculate the upper and lower limits of a confidence interval for the true mean of amateur scoring using the amateurs in the first round as our sample. Use a 90% confidence interval (\\(\\alpha = .1\\)).\n\nWhat is the lower bound of the confidence interval?\nWhat is the upper bound of the confidence interval?\nWhat is the interpretation of this confidence interval?\n\n\n\nRepeat this process with a 99% confidence interval (\\(\\alpha = .01\\)).\n\nWhat is the new lower bound of the confidence interval?\nWhat is the new upper bound of the confidence interval?\nIs this 99% confidence interval larger, smaller, or the same as 90% confidence interval?\nWhen thinking about your anwer to f, what do you think could explain this?\n\n\n\nTIP: Values and objects can be stored in variables in R. For example, x &lt;- 5 stores the value 5 in the variable x.\nTIP: The pipe operator is a powerful tool in R that allows you to chain functions together. It is denoted by |&gt; and is used to pass the output of one function to the input of another function.\nTIP: The pull() function is used to extract a single column from a data frame as a vector.\nTIP: The nrow() function is used to calculate the number of rows in a data frame.\n\nTIP: When interpreting a confidence interval do not say ‚Äúthere is a 90% chance that the true mean is between the lower and upper bounds‚Äù. Instead, say ‚Äúwe are 90% confident that the true mean is between the lower and upper bounds‚Äù.\n\nTIP: Only the t_cv variable needs to be changed to recompute with the new confidence interval. Use qt(0.995, df = n - 1) to calculate the critical value for the new 99% confidence interval.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals"
    ]
  },
  {
    "objectID": "golf/pga_masters/index.html#z-interval-for-single-means",
    "href": "golf/pga_masters/index.html#z-interval-for-single-means",
    "title": "PGA - Scoring Average Confidence Intervals",
    "section": "z-interval for single means",
    "text": "z-interval for single means\nA standard normal distribution (also known as a z-distribution) is used to calculate the confidence interval for a single mean if the sample size is large enough (greater than 30) or the population standard deviation is known. The first case is common as oftentimes samples are greater than 30. The second case is rare because it is uncommon to know the population standard deviation but not the population mean.\nThe formula for the confidence interval for a single mean using the z-distribution is very similar to that of the t-distribution\n\n\nClick here for more information about the standard normal distribution.\n\\(CI = \\bar{X} \\pm Z_{\\alpha/2} \\times \\frac{\\sigma}{\\sqrt{n}}\\)\nWhere \\(\\bar{X}\\) is once again the sample mean, \\(Z_{\\alpha/2}\\) is the critical value for the standard normal distribution at the specified confidence level, \\({\\sigma}\\) is the population standard deviation, and \\(n\\) is the sample size.\nThe reasoning behind why we can use the standard normal distribution when the sample is greater than 30 even if the population standard deviation is unknown is found in the Central Limit Theorem, which says that as the sample size increases, the sampling distribution of the sample mean approaches a normal distribution. This means that when the sample size is greater than 30 we can use the sample standard deviation to estimate the population standard deviation and create a confidence interval as seen below\n\\(CI = \\bar{X} \\pm Z_{\\alpha/2} \\times \\frac{S}{\\sqrt{n}}\\)\nOnce again the critical value for the z-distribution is the value of \\(Z_{\\alpha/2}\\) such that the area under the standard normal distribution curve between \\(-Z_{\\alpha/2}\\) and \\(Z_{\\alpha/2}\\) is equal to the confidence level.\nR can compute the z-value for you using the qnorm() function. The qnorm() function takes in the probability and returns the z-value that corresponds to that probability. For example, qnorm(.975) will return the z-value that corresponds to the 97.5th percentile of the standard normal distribution. No degrees of freedom are needed.\nThe code below calculates the critical values for the z-distribution for a 95% confidence interval.\n(Note that .975 is used for a 95% confidence interval because \\(\\alpha = .05\\) and since the confidence interval is two sided we need half of the error each side so \\(\\alpha / 2  = .025\\), which means we want to use qnorm(.975) or qnorm(.025))\n\nqnorm(.975)\n\n[1] 1.959964\n\nqnorm(.025)\n\n[1] -1.959964\n\n\n\n\n\n\n\n\nNoteExample: z-distribution confidence intervals\n\n\n\n\n\nWe would like to make a 98% confidence interval for the true mean of scoring for PGA Tour golfers based off of a sample from the third round. This sample can be created with the following code.\n\npga_round3_scores &lt;- masters_2023 |&gt; \n  filter(tour == \"PGA\", round == 3) |&gt; \n  pull(score)\n\nWe can check the size of our sample with the following code.\n\npga_round3_n &lt;- length(pga_round3_scores)\npga_round3_n\n\n[1] 39\n\n\nThat‚Äôs 39 observations we have in our sample, which means we can use the z-distribution.\nWe start by calculating the the sample mean (\\(\\bar{x}\\)). R can do this very easily as seen below\n\npga_round3_mean &lt;- mean(pga_round3_scores)\n\nNext we calculate the sample standard deviation (\\(s\\)), which is our estimate for \\(\\sigma\\). Once again, R can do this quickly.\n\npga_round3_sd &lt;- sd(pga_round3_scores)\n\nOur next step is to find the critical value for a 98% confidence interval for a z-distribution (\\(Z_{\\alpha/2}\\)). We can use qnorm to do this.\n\nz_cv &lt;- qnorm(.99)\n\nLastly we will use the confidence interval formula to find the bounds for our 98% confidence interval. \\[\nCI = \\bar{X} \\pm Z_{\\alpha/2} \\times \\frac{s}{\\sqrt{n}}\n\\]\n\nlower_confidence_boundary &lt;- pga_round3_mean - z_cv * pga_round3_sd /\n  sqrt(pga_round3_n)\nupper_confidence_boundary &lt;- pga_round3_mean + z_cv * pga_round3_sd /\n  sqrt(pga_round3_n)\npaste0(\"98% confidence interval: (\", lower_confidence_boundary,\n       \", \", upper_confidence_boundary, \")\")\n\n[1] \"98% confidence interval: (72.1152269671053, 73.9360550841767)\"\n\n\nFinally we can interpret the confidence interval and say that we are 98% confident that the true mean of PGA Tour golfers‚Äô scores at the 2023 masters is between 72.1 and 73.9.\n\n\n\n\n\n\n\n\n\nNoteExercise 3: z-distribution confidence intervals\n\n\n\nRun the code below to create the proper subset of the data for this exercise and calculate the sample mean, sample standard deviation, and critical value for the z-distribution.\n\npga_round1 &lt;- masters_2023 |&gt; \n  filter(round == 1, tour == \"PGA\")\nx_bar &lt;- pga_round1 |&gt; \n  summarise(mean(score)) |&gt; \n  pull()\nsigma &lt;- pga_round1 |&gt; \n  summarise(sd(score)) |&gt; \n  pull()\nz_cv &lt;- qnorm(.975)\n\nUse the formula for creating a confidence interval using the standard normal distribution to calculate the upper and lower limits of a confidence interval for the true mean of PGA professional scoring at Augusta using the PGA pros in the first round as our sample. Use a 95% confidence interval (\\(\\alpha = .05\\)).\n\nWhy can we use the standard normal distribution to calculate this confidence interval?\nWhat is the confidence interval for the true mean of scoring for PGA professionals at Augusta National?\nWhat is the interpretation of this confidence interval?",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals"
    ]
  },
  {
    "objectID": "golf/pga_masters/index.html#r-for-single-mean-confidence-intervals",
    "href": "golf/pga_masters/index.html#r-for-single-mean-confidence-intervals",
    "title": "PGA - Scoring Average Confidence Intervals",
    "section": "R for Single Mean Confidence Intervals",
    "text": "R for Single Mean Confidence Intervals\nR functions can help to speed up the process of finding these confidence intervals and will also help us with testing hypotheses later. The t.test function from the stats package makes a confidence interval for the t-distribution and the z.test function from the BSDA package does the same for the standard normal distribution.\nRun the code below to view what type of arguments these functions take, what they output, and the see some example uses\n\n?t.test\n?z.test\n\nThese functions return more than just a confidence interval. If we want to see just the confidence interval $conf.int should be used. Run the example below to see how this is done.\n\n# t-test example\nseniors_round1 &lt;- masters_2023 |&gt; \n  filter(round == 1, tour == \"Senior\")\n\nt.test(seniors_round1$score, conf.level = .95)$conf.int\n\n[1] 72.39194 79.03663\nattr(,\"conf.level\")\n[1] 0.95\n\n# z-test example\npga_round2 &lt;- masters_2023 |&gt; \n  filter(round == 2, tour == \"PGA\")\n\n## Note that the Z test require the standard deviation to be passed in as an argument `sigma.x`\nz.test(pga_round2$score,\n       sigma.x = sd(pga_round2$score),\n       conf.level = .95)$conf.int\n\n[1] 72.24391 73.71973\nattr(,\"conf.level\")\n[1] 0.95\n\n\n\n\nTIP: The $ operator is used to access a specific element of a list. In this case, the conf.int element of the list returned by the t.test and z.test functions. It can also be used to access elements of data frames and other objects in R. The line of code seniors_round1$score is used to access the score column of the seniors_round1 data frame.\n\nTIP: Notice that the z.test function requires the standard deviation to be passed in as an argument sigma.x. Use the sd() function to calculate the standard deviation of the sample and use it as an estimate the population standard deviation.\n\n\n\n\n\n\n\nNoteExercise 4: R for confidence intervals\n\n\n\n\nUse the t.test function to find the confidence interval for amateur scoring using the amateur_round1 sample from earlier in the lesson. What is the confidence interval?\n\n\n\nUse the z.test function to find the confidence interval for PGA professional scoring using the pga_round1 sample from earlier in the lesson. What is the confidence interval?\nAre these confidence intervals the same as what you calculated manually earlier? If no, why might that be?\n\n\n\n\nTIP If you didn‚Äôt create the amateur_round1 sample earlier run the code below\n\namateur_round1 &lt;- masters_2023 |&gt; \n  filter(round == 1, tour == \"Amateur\")\n\nTIP If you didn‚Äôt create the pga_round1 sample earlier run the code below\n\npga_round1 &lt;- masters_2023 |&gt; \n  filter(round == 1, tour == \"PGA\")",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals"
    ]
  },
  {
    "objectID": "golf/pga_masters/index.html#test-statistics",
    "href": "golf/pga_masters/index.html#test-statistics",
    "title": "PGA - Scoring Average Confidence Intervals",
    "section": "Test Statistics",
    "text": "Test Statistics\nLike confidence intervals, we have two different tests for hypothesis testing for the population mean. Remember that if the population standard deviation is unknown and the sample size is less than 30, we use the t-distribution. If the population standard deviation is known or the sample size is greater than 30, we use the standard normal distribution.\nEach of these distributions have their own tests, the t-test and the z-test. This means that we have different test statistics to calculate depending on the situation.\n\nt-test\nThe t-test statistic is calculated using the formula:\n\\[t = \\frac{\\bar{x} - \\mu_0}{\\frac{s}{\\sqrt{n}}}\\]\nwhere \\(\\bar{x}\\) is the sample mean, \\(\\mu_0\\) is the hypothesized population mean, \\(s\\) is the sample standard deviation, and \\(n\\) is the sample size.\n\n\nz-test\nThe z-test statistic is calculated using the formula:\n\\[z = \\frac{\\bar{x} - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}\\]\nwhere \\(\\bar{x}\\) is the sample mean, \\(\\mu_0\\) is the hypothesized population mean, \\(\\sigma\\) is the population standard deviation, and \\(n\\) is the sample size.\nOnce again, if the sample size is over 30 and the population standard deviation is unknown, we use the sample standard deviation to approximate the population standard deviation.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals"
    ]
  },
  {
    "objectID": "golf/pga_masters/index.html#to-reject-or-fail-to-reject",
    "href": "golf/pga_masters/index.html#to-reject-or-fail-to-reject",
    "title": "PGA - Scoring Average Confidence Intervals",
    "section": "To Reject or Fail to Reject",
    "text": "To Reject or Fail to Reject\nThere are two ways to make a decision about the null hypothesis.\nMethod 1: Critical values, along with test statistics, can be used to determine if the hypothesized population mean is within the confidence interval for the true mean.\nA critical value is a value that separates the rejection region from the non-rejection region. The rejection region is the area where the null hypothesis is rejected. The non-rejection region is the area where the null hypothesis is not rejected. The critical value is determined by the significance level (\\(\\alpha\\)) and the degrees of freedom (if it is a t-test). The critical value is compared to the test statistic to determine if the null hypothesis should be rejected. If the test statistic is within the non-rejection region, the null hypothesis is not rejected. If the test statistic is within the rejection region, the null hypothesis is rejected and the alternative hypothesis is accepted.\nBelow is an example of using critical values and a test-statistic for a z-test with a 95% confidence level (two-sided). The critical value is 1.96. This means that if the test statistic is greater than 1.96 or less than -1.96, the null hypothesis is rejected. The blue represents the non-rejection region and the red the rejection region. Since the test statistic for this hypothetical example is 1.1 (less than 1.96 and greater than -1.96), we fail to reject the null hypothesis.\n\n\n\n\n\n\n\n\n\nThis method corresponds directly to the related confidence intervals produced for the sample data.\nIf the hypothesized population mean is within the confidence interval, we fail to reject the null hypothesis. If the hypothesized population mean is not within the confidence interval, the null hypothesis is rejected and the alternative hypothesis is accepted.\n\n\nNote: We can say that there is significant evidence to accept the alternative hypothesis if the null hypothesis is rejected. However, it should never be said that we accept the null hypothesis. We can only fail to reject it.\nMethod 2: The second method is to use a p-value. The p-value is the probability of observing a test statistic as extreme as the one calculated from the sample data given that the null hypothesis is true. The p-value is compared to the significance level (\\(\\alpha\\)) to determine if the null hypothesis should be rejected. If the p-value is less than \\(\\alpha\\), the null hypothesis is rejected. If the p-value is greater than \\(\\alpha\\), the null hypothesis is not rejected.\n\n\nNOTE: Our alternative hypothesis determines whether we are looking for the probability that the test statistic is greater than or less than the observed value.\n\nFor a two-sided test, the p-value is the probability that the test statistic is greater than the observed value or less than the negative of the observed value. Find the area in one of the tails and double it.\nFor a left-tailed test (\\(H_a: \\mu &lt; \\mu_0\\)), the p-value is the probability that the test statistic is less than the observed value.\nFor a right-tailed test (\\(H_a: \\mu &gt; \\mu_0\\)), the p-value is the probability that the test statistic is greater than the observed value.\n\nFirst let‚Äôs visualize what a p-value is telling us.\n\n\n\n\n\n\n\n\n\nNow let‚Äôs see how we can calculate p-value for our example using R. We have a hypothetical z-test statistic of 1.1 and alpha of .05 for a two-tailed test. We want to find the probability of getting a test statistic less than -1.1 or greater than 1.1. In R the pnorm function calculates the probability that a test statistic is less than a given value. Since we know that the z-distribution is symmetrical we can simply multiply the probability that the test statistic is less than -1.1 by 2 to get our p-value.\n\np_value &lt;- pnorm(-1.1) * 2\np_value\n\n[1] 0.2713321\n\n\nSince the p-value is greater than our alpha value of .05 we fail to reject the null hypothesis.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals"
    ]
  },
  {
    "objectID": "golf/pga_masters/index.html#hypothesis-testing-in-r",
    "href": "golf/pga_masters/index.html#hypothesis-testing-in-r",
    "title": "PGA - Scoring Average Confidence Intervals",
    "section": "Hypothesis testing in R",
    "text": "Hypothesis testing in R\nThankfully R can help us with this as well. The t.test function and the z.test function can both perform hypothesis tests, without having to do each step manually.\nThe code below tests if the true mean is not equal to 50 given that the example_vector is our sample. The confidence level is set to 95%.\n\nexample_vector &lt;- seq(10, 100, by = 10)\nt.test(example_vector, mu = 50, alternative = \"two.sided\", conf.level = .95)\n\n\n    One Sample t-test\n\ndata:  example_vector\nt = 0.52223, df = 9, p-value = 0.6141\nalternative hypothesis: true mean is not equal to 50\n95 percent confidence interval:\n 33.34149 76.65851\nsample estimates:\nmean of x \n       55 \n\n\nAs can be seen in the output of the code above, the p-value is 0.61, which is much greater than the significance level of 0.05. The test-statistic is 0.522 which is within the non-rejection region since critical values for this test would be -2.26 and 2.26. Therefore, we fail to reject the null hypothesis.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals"
    ]
  },
  {
    "objectID": "golf/pga_masters/index.html#hypothesizing-par-as-the-population-mean",
    "href": "golf/pga_masters/index.html#hypothesizing-par-as-the-population-mean",
    "title": "PGA - Scoring Average Confidence Intervals",
    "section": "Hypothesizing Par as the Population Mean",
    "text": "Hypothesizing Par as the Population Mean\n\n\nAugusta National is breathtakingly beautiful, but if golfers get distracted by the scenic views, tall pines, bunkers, water, and azaleas may catch their balls.\n\n\n\nAugusta Hole 13\n\n\nImage Source: Your Golf Travel, CC 4.0\nIn golf par is considered to be the number of strokes a good golfer is expected to take. The par for the course at Augusta National is 72. It is known that Augusta National is a tougher than usual course but we would like to test if that is the case for different groups.\nOur null hypothesis will generally be that the mean of the group is equal to 72.\n\n\n\n\n\n\nNoteExercise 5: t-test for single mean hypothesis testing\n\n\n\n\nAmateurs, who are not yet professional golfers, are generally expected to score higher than professionals. We would like to test if the mean of amateur scoring is above par at Augusta National\n\nWhat is the null hypothesis for this test?\nWhat is the alternative hypothesis for this test?\nUse the t.test function to test if the mean of amateur scoring is greater than 72 using the amateur_round1 sample from earlier in the lesson. What is the p-value?\nBased on the p-value, is there statistically significant evidence that the mean of amateur scoring is greater than 72?\n\n\n\n\nAmateurs generally struggle in the Masters, but in 2023 Sam Bennett, a Texas A&M student, made the cut and finished 16th. However, due to his amateur status, he was not eligible to win money and missed out on $261,000.\n\n\n\n\n\n\nNoteExercise 6: z-test for single mean hypothesis testing\n\n\n\nPGA professionals would generally average somewhere around par. We would like to test if the mean of PGA professional scoring is not equal to 72 at Augusta National.\n\nWhat is the null hypothesis for this test?\nWhat is the alternative hypothesis for this test?\n\n\n\nUse the z.test function to test if the mean of PGA professional scoring is not equal to 72 using the pga_round1 sample from earlier in the lesson. What is the p-value?\nBased on the confidence interval, is there statistically significant evidence that the mean of PGA professional scoring is not equal to 72?\nExplain your answer to part d.\n\n\n\n\nTIP Use the $score column from the pga_round1 sample as the first argument in the z.test function\nTIP Remember that the z.test function requires the population standard deviation as the second argument. Use the sd function to calculate the standard deviation of the pga_round1 sample.\n\nsd(pga_round1$score)",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals"
    ]
  },
  {
    "objectID": "hockey/index.html",
    "href": "hockey/index.html",
    "title": "Hockey",
    "section": "",
    "text": "These modules use hockey data to teach topics in statistics and data science.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting NHL Shooting Percentages\n\n\n\nlinear regression\n\n\n\nAn Introduction to Simple Linear Regression\n\n\n\n\n\nJul 23, 2023\n\n\nSam Ventura\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Hockey"
    ]
  },
  {
    "objectID": "lacrosse/college_lacrosse_faceoffs/index.html",
    "href": "lacrosse/college_lacrosse_faceoffs/index.html",
    "title": "Lacrosse Faceoff Proportions",
    "section": "",
    "text": "Introduction\nIn this engaging activity, we explore the exciting sport of NCAA Division I Lacrosse, with a special focus on faceoff percentages‚Äîa critical aspect of the game. A faceoff occurs at the start of each quarter and after every goal, where two players compete to gain possession of the ball, setting the stage for their team‚Äôs offensive play. Winning a high percentage of faceoffs is often key to controlling the game and can significantly impact a team‚Äôs overall performance.\n\n\n\n\n\n\nNoteVideo Demonstrating a Faceoff\n\n\n\n\n\n\n\n\n\nOur primary goal is to compare a specific team‚Äôs faceoff performance with overall league statistics for the 2022-2023 season. Through this exploration, we‚Äôll introduce you to the concept of one-sample proportion hypothesis testing, a powerful statistical tool widely used in sports analytics. By the end of this exercise, you‚Äôll gain a fundamental understanding of hypothesis testing and how it can be practically applied to evaluate team performance in lacrosse and beyond.\n\n\n\n\n\n\nNoteActivity Length\n\n\n\n\n\nThis activity would be suitable for an in-class example (of approximately 10 - 20 minutes) or can be modified to be a quiz or part of an exam.\n\n\n\n\n\n\n\n\n\nNoteLearning Objectives\n\n\n\n\n\n\nComprehend the concept of one sample proportion hypothesis testing and its relevance in sports statistics.\nAnalyze and interpret dataset variables related to faceoff percentages in NCAA Division I Lacrosse.\nEvaluate a specific team‚Äôs faceoff performance by comparing it with league-wide statistics using hypothesis testing.\n\n\n\n\n\n\n\n\n\n\nNoteMethods\n\n\n\n\n\nStudents are expected to have been exposed to the following concepts and use the activity to reinforce their understanding of these methods.\n\nBasic probability and percentages.\nNull and alternative hypotheses.\nSample size and sample proportion calculations.\nSuccess-failure condition for hypothesis testing.\nCalculation of test statistics (Z-score).\nUnderstanding significance levels (‚ç∫) and p-values.\nDrawing conclusions and implications from hypothesis test results.\n\n\n\n\n\n\nData\nNote that because the activity only uses results from one team, students do not necessarily need to directly access this data. However, the activity can easily be adapted to use other teams. Instructors are encouraged to personalize the activity if they so choose.\nThe data set where the activities statistics come from contains 72 rows and 22 columns. Each row represents the season results for a lacrosse team at the NCAA Division 1 level from the 2022-2023 season.\nDownload data: lax_2022_2023.csv\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nTeam\ncollege of the team\n\n\navg_assists\naverage assists to goals per game\n\n\navg_caused_turnovers\naverage turnovers forced by the team per game\n\n\nclearing_pctg\npercentage of successful attempts to earn an offensive opportunity after gaining the ball in the teams own half\n\n\ntotal_faceoffs\ntotal faceoffs taken by a team for the season\n\n\nfaceoff_wins\ntotal faceoff wins by a team for the season\n\n\nfaceoff_win_pct\nproportion of total faceoff wins out of total faceoffs\n\n\navg_goals\naverage goals per game\n\n\navg_goals_allowed\naverage goals allowed by the team per game\n\n\navg_ground_balls\naverage loose balls picked up by the team per game\n\n\nman_down_defense_pctg\nproportion of times a team stops the opponent from scoring while man down due to a penalty\n\n\nman_up_offense_pctg\nproportion of times the offense scores out of total opportunities while man up\n\n\navg_scoring_margin\naverage margin of goals per game\n\n\nopp_clear_pctg\nopponents clearing percentage averaged by game\n\n\navg_points\naverage team points per game\n\n\navg_saves\naverage saves per game\n\n\nshot_pctg\nproportion of shots that go in out of total shots\n\n\navg_turnovers\naverage turnovers that are directly the fault of a player per game\n\n\nW\ntotal wins by the team\n\n\nL\ntotal losses by the team\n\n\nwin_loss_pctg\nproportion of games won out of total games\n\n\n\nData Source\nThe data were collected from the NCAA Website for Men‚Äôs Lacrosse Division I\nhttp://stats.ncaa.org/rankings/change_sport_year_div\nInstructors interested in updating the data to a newer season can do so via the following\n\nGo to http://stats.ncaa.org/rankings/change_sport_year_div\nSelect Men‚Äôs Lacrosse, season of choice, Division I, Final Statistics\nIn the ‚ÄúTeams‚Äù, download each of the data tables.\nRead in each file, join the tables, and do some light cleaning. The code below shows an example used for the 2022-2023 season.\n\n\n\nShow the code\nlibrary(tidyverse)\n\n\n# reading\n# the files listed here are what\n# you will download from the site\n\nassists&lt;- read_csv(\"assists_l.csv\", col_select = 1:2)\ncaused_turnovers&lt;- read_csv(\"caused_turnovers_l.csv\", col_select = 1:2)\nclearing&lt;- read_csv(\"clearing_pctg_l.csv\", col_select = 1:2)\nfo &lt;- read_csv(\"fo_win_pctg.csv\", col_select = 1:4)\ngoals_against&lt;- read_csv(\"goals_against.csv\", col_select = 1:2)\ngoals&lt;- read_csv(\"goals_l.csv\", col_select = 1:2)\ngroundballs&lt;- read_csv(\"ground_balls_l.csv\", col_select = 1:2)\nman_down &lt;- read_csv(\"man_down_defense_l.csv\", col_select = 1:2)\nman_up &lt;- read_csv(\"man_up__offense_l.csv\", col_select = 1:2)\nmargin &lt;- read_csv(\"margin_l.csv\", col_select = 1:2)\nopp_clear &lt;- read_csv(\"opp_clear_l.csv\", col_select = 1:2)\npoints &lt;- read_csv(\"points_l.csv\", col_select = 1:2)\nsaves &lt;- read_csv(\"saves_l.csv\", col_select = 1:2)\nshot &lt;- read_csv(\"shot_pctg_l.csv\", col_select = 1:2)\nturnovers&lt;- read_csv(\"turnovers_l.csv\", col_select = 1:2)\nshots_per_game &lt;- read_csv(\"shots_per_game.csv\", col_select = 1:3)\nwin_loss &lt;- read_csv(\"win_loss_l.csv\")\n\n# joining\n# students familiar with the purrr package could\n# use the reduce function to reduce the amount of code\n\nlax_2022_2023 &lt;- \n  left_join(assists, caused_turnovers, by = \"Team\") %&gt;%\n  left_join(clearing, by = \"Team\") %&gt;%\n  left_join(fo, by = \"Team\")  %&gt;%\n  left_join(goals, by = \"Team\")  %&gt;%\n  left_join(goals_against, by = \"Team\")  %&gt;%\n  left_join(groundballs, by = \"Team\") %&gt;%\n  left_join(man_down, by = \"Team\") %&gt;%\n  left_join(man_up, by = \"Team\") %&gt;%\n  left_join(margin, by = \"Team\") %&gt;%\n  left_join(opp_clear, by = \"Team\") %&gt;%\n  left_join(points, by = \"Team\") %&gt;%\n  left_join(saves, by = \"Team\") %&gt;%\n  left_join(shot, by = \"Team\") %&gt;%\n  left_join(turnovers, by = \"Team\") %&gt;%\n  left_join(shots_per_game, by = \"Team\") %&gt;%\n  left_join(win_loss, by = \"Team\")\n\n# cleaning\nlax_2022_2023 &lt;- lax_2022_2023 %&gt;%\n  separate(Team, into = c(\"Team\",\"Conference\"), sep = \"\\\\(\", extra = \"merge\")%&gt;%\n  mutate(Conference = str_remove_all(Conference,\"\\\\)\"),\n         Team = str_trim(Team))%&gt;%\n  mutate(shots_per_game = Shots/Games)%&gt;%\n  select(-20, -21)\n\n# saving\nwrite_csv(x = lax_2022_2023, file = \"lax_2022_2023.csv\")\n\n\n\n\n\nMaterials\nClass handout\nClass handout - with solutions\n\n\n\n\n\n\nNoteConclusion\n\n\n\n\n\nIn this insightful exploration of NCAA Division I Lacrosse faceoff percentages, we have embarked on a statistical journey to evaluate a specific team‚Äôs performance in comparison to league-wide statistics. Through the application of one sample proportion hypothesis testing, we gained valuable insights into the team‚Äôs faceoff win percentage, unveiling strong evidence that their performance exceeded what we would expect by random chance alone. As we consider the broader implications of faceoffs in Division I Lacrosse, it becomes evident that faceoff wins play a pivotal role in team rankings and outcomes. The fact that Duke, the second-best team in the country, exhibited a faceoff win percentage above the league average highlights the significance of excelling in this aspect of the game. Winning faceoffs likely translates to higher goal-scoring opportunities, ultimately leading to more successful game outcomes.",
    "crumbs": [
      "Home",
      "Lacrosse",
      "Lacrosse Faceoff Proportions"
    ]
  },
  {
    "objectID": "lacrosse/lacrosse_pll_vs_nll/index.html",
    "href": "lacrosse/lacrosse_pll_vs_nll/index.html",
    "title": "Lacrosse PLL vs.¬†NLL",
    "section": "",
    "text": "Introduction\nThis module examines the goals and shots in two prominent lacrosse leagues: the Premier Lacrosse League (PLL) and the National Lacrosse League (NLL). The PLL and NLL are highly regarded professional lacrosse leagues that feature top-tier athletes from around the world.\nThe PLL is played in an outdoor setting, following the field lacrosse format. This style of lacrosse is characterized by its larger field size, typically 110 yards by 60 yards. Field lacrosse involves 10 players per team and promotes a style of play that emphasizes long passes, intricate plays, and individual skills. Founded in 2019, the PLL operates with a touring model where teams travel to different cities each weekend, bringing the sport to a wide audience and fostering a festival-like atmosphere at each event. The league‚Äôs modernized approach includes a strong emphasis on media presence and player engagement. The PLL features approximately 200 athletes across its teams.\nIn contrast, the NLL follows the box lacrosse format, which is played indoors on a smaller, enclosed field, generally 200 feet by 85 feet. Box lacrosse involves 6 players per team, and the gameplay is marked by frequent physical interactions, quick ball movements, and high-intensity transitions. Established in 1986, the NLL has a traditional franchise model with teams based in specific cities across the United States and Canada. This structure has cultivated strong local fan bases and deep community ties, contributing to the league‚Äôs longevity. The NLL features approximately 500 athletes, many of which also play in the PLL.\nThese data, from the 2021-2022 seasons, allow for an analysis of goal-scoring within these leagues to identify differences between indoor (NLL) and outdoor (PLL) play. By examining goals and shots, we aim to understand how the environment and format of the game influence offensive strategies and overall scoring trends in professional lacrosse. It‚Äôs important to recognize that outdoor field lacrosse and indoor box lacrosse are distinct sports, each with its own unique dynamics, rules, and playing styles. By acknowledging these nuances, we can may gain a deeper understanding of how various playing conditions and league structures impact statistical outcomes.\n\n\n\n\n\n\nNoteActivity Length\n\n\n\n\n\nThis activity would be suitable for an in-class example or can be modified to be a quiz or part of an exam.\n\n\n\n\n\n\n\n\n\nNoteLearning Objectives\n\n\n\n\n\nThe learning goals associated with this module are:\n\nStudents will be able to test for a difference in means between two groups.\nStudents will be able to find a confidence interval for a difference in means between two groups.\n\n\n\n\n\n\n\n\n\n\nNoteMethods\n\n\n\n\n\nThis module requires students use a two-sample test and confidence interval (e.g., t-test or randomization) to compare the means of two groups.\nStudents are expected to have access any equations and/or lecture notes to complete the activity.\nTechnology requirement:\n\nThe provided handout assumes that students can use technology such as calculators to perform a two-sample t-test and interval for a difference in means between two groups.\nThe raw data is provided to allow instructors to customize the handout to incorporate other forms of technology. e.g., Students can use software such as Minitab to calculate test statistics and p-values for a t-test or StatKey for simulation based inference.\n\n\n\n\n\n\nData\nThe data set has 162 rows with 9 columns. Each row represents a single lacrosse match either in the Premier Lacrosse League or the National Lacrosse League during the 2021-2022 season.\nDownload data: lacrosse_pll_nll_2021-2022.csv\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nLeague\nThe Premier Lacrosse league and the National Lacrosse League\n\n\nAway_team\nThe traveling team\n\n\nHome_team\nThe hosting team\n\n\nAway_shots\nHow many shots the Away_team had on net\n\n\nHome_shots\nHow many shots the Home_team had on net\n\n\nAway_goals\nHow many goals the Away_team had on net\n\n\nHome_goals\nHow many goals the Home_team had on net\n\n\nGoals\nThe total amount of goals scored each game\n\n\nGoals_per_48\nThe average amount of goals for the first 48 minutes of a game\n\n\n\n\n\nData Sources\nPremier Lacrosse League stats. Premier Lacrosse League Stats. (n.d.). https://stats.premierlacrosseleague.com/\nPlayer stats. NLL. (2023, January 26). https://www.nll.com/stats/all-player-stats/\n\n\n\nMaterials\nThe data and worksheet associated with this module are available for download through the following links.\n\nlacrosse_pll_nll_2012-2022.csv - Dataset with game-by-game shots and goals scored for both leagues in the 2021-2022 season..\nlacrosse_pll_vs_nll_t-test_worksheet.docx- Activity worksheet to compare scoring and shots between indoor and outdoor leagues using t-distributions.\nlacrosse_pll_vs_nll_randomization_worksheet.docx- Activity worksheet to compare scoring and shots between indoor and outdoor leagues using randomization tests implements via StatKey.\n\nSample solutions to the worksheets\n\nlacrosse_pll_vs_nll_t-test_worksheet_key.docx - Activity worksheet using t-distributions with sample solutions.\nlacrosse_pll_vs_nll_randomization_worksheet_key.docx - Activity worksheet using randomization tests with sample solutions.\n\n\n\n\n\n\n\nNoteConclusion\n\n\n\n\n\nStudents should notice that while no discernible difference between average goals per game was discovered, after adjusting for the length of the game, PLL (i.e., the 48-minute outdoor league) has the higher rate per 48-minutes.\nFurther, students will ideally see how a confidence interval can be used to supplement the conclusion of a hypothesis test by bringing effect sizes into the interpretation in addition to statistical significance.",
    "crumbs": [
      "Home",
      "Lacrosse",
      "Lacrosse PLL vs. NLL"
    ]
  },
  {
    "objectID": "marathons/index.html",
    "href": "marathons/index.html",
    "title": "Marathons",
    "section": "",
    "text": "These modules use marathons data to teach topics in statistics and data science.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023 Boston Marathon - Variability in Finish Times\n\n\n\nhistograms\n\nsummary statistics\n\nbimodal data\n\n\n\nDescribing finish time for runners in the 2023 Boston Marathon\n\n\n\n\n\nMay 13, 2024\n\n\nIvan Ramler, Jack Fay\n\n\n\n\n\n\n\n\n\n\n\n\nMarathon Record-Setting Over Time\n\n\n\nExponential distribution\n\nPoisson process\n\n\n\nDetermining whether the setting of world records in the marathon is historically a Poisson process.\n\n\n\n\n\nJul 23, 2023\n\n\nNicholas Clark, Rodney Sturdivant, and Kate Sanborn\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Marathons"
    ]
  },
  {
    "objectID": "mixed_martial_arts/index.html",
    "href": "mixed_martial_arts/index.html",
    "title": "Mixed Martial Arts",
    "section": "",
    "text": "These modules use mixed_martial_arts data to teach topics in statistics and data science.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMMA Inter-rater Reliability Data Analysis\n\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Mixed Martial Arts"
    ]
  },
  {
    "objectID": "motor_sports/index.html",
    "href": "motor_sports/index.html",
    "title": "Motor Sports",
    "section": "",
    "text": "These modules use motor_sports data to teach topics in statistics and data science.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNASCAR Transformation Module\n\n\n\nLinear regression\n\nTransformations\n\nPolynomial regression\n\n\n\nUsing NASCAR driver rating data to explore a series of transformations to improve linearity in regression.\n\n\n\n\n\nFeb 5, 2024\n\n\nAlyssa Bigness, Ivan Ramler, Jack Fay\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Motor Sports"
    ]
  },
  {
    "objectID": "obstacle_competitions/american_ninja_warrior/index.html",
    "href": "obstacle_competitions/american_ninja_warrior/index.html",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analysis",
    "section": "",
    "text": "NoteFacilitation notes\n\n\n\n\n\n\nThis module would be suitable for an in-class lab or take-home assignment in an intermediate statistics course.\nIt assumes a familiarity with the RStudio Environment and R programming language.\nStudents should be provided with the following data file (.csv) and Quarto document (.qmd) to produce visualizations and write up their answers to each exercise. Their final deliverable is to turn in an .html document produced by ‚ÄúRendering‚Äù the .qmd.\n\n2021 Stage 1 Finals Data\n2023 Stage 1 Finals Data\nStudent Quarto template\n\nPosit Cloud (via an Instructor account) or Github classroom are good options for disseminating files to students, but simply uploading files to your university‚Äôs course management system works, too.\nThe anw_2021_stage1.csv data is derived largely from americanninjawarriornation.com. Additional columns such as sex were individually researched and added to the data.\nThe anw_2023_stage1.csv data is derived largely from sasukepedia.",
    "crumbs": [
      "Home",
      "Obstacle Competitions",
      "American Ninja Warrior - Kaplan-Meier Survival Analysis"
    ]
  },
  {
    "objectID": "obstacle_competitions/american_ninja_warrior/index.html#terms-to-know",
    "href": "obstacle_competitions/american_ninja_warrior/index.html#terms-to-know",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analysis",
    "section": "Terms to know",
    "text": "Terms to know\nBefore proceeding with the survival analysis, let‚Äôs make sure we understand American Ninja Warrior and some of it‚Äôs vocabulary to help us climb our way through this lab.\n\nHow does American Ninja Warrior work?\nAmerican Ninja Warrior is an NBC competition show where participants attempt to complete a series of obstacle courses of increasing difficulty. In a single obstacle course, the competitors must complete a series of obstacles in a row. If they fail an obstacle (usually this happens when they fall into the water below), they are eliminated from the competition. The competitors also have a time limit to complete the course. The competitors are ranked based on how far they get in the course and how quickly they complete it.\nMost of the obstacles are designed to test the competitors‚Äô upper body strength. Some obstacles require balance and agility though.\n\n\nThe warped wall is arguably the most famous, although now least difficult, obstacle on an American Ninja Warrior course. The warped wall is a curved wall that competitors must run up and grab the top of. The warped wall is on every course and is often the final obstacle, although this is not the case on the Finals courses.\nThe warped wall was previously 14 feet and is now 14.5 feet tall. They have even had a 18 foot warped wall on the show.\n\n\n\nWarped Wall\n\n\nImage Source: Dustin Batt, CC BY-SA 2.0, via Wikimedia Commons\nThe obstacles in American Ninja Warrior are all given names. For example, the famed Warped Wall is a curved wall that competitors must run up and grab the top of. The Salmon Ladder is a series of rungs that competitors must move up by jumping and pulling themselves up.\nWatch Enzo Wilson complete the American Ninja Warrior course at the 2021 Finals Stage 1 (Season 13) in the video below.\n\n\n\n\n\n\n\nImportantKey Terms\n\n\n\n\nObstacle: A challenge that competitors must complete to move on in the competition.\nCourse: A series of obstacles that competitors must complete in a row. A typical course has 6-10 obstacles.\nStage: A round of the competition. The competition starts with Stage 1 and progresses to Stage 4.\nTime Limit: The amount of time competitors have to complete the course, often between 2-4 minutes.",
    "crumbs": [
      "Home",
      "Obstacle Competitions",
      "American Ninja Warrior - Kaplan-Meier Survival Analysis"
    ]
  },
  {
    "objectID": "obstacle_competitions/american_ninja_warrior/index.html#variable-descriptions",
    "href": "obstacle_competitions/american_ninja_warrior/index.html#variable-descriptions",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analysis",
    "section": "Variable Descriptions",
    "text": "Variable Descriptions\nThe ninja data you‚Äôll be analyzing in this lab provides the individual run information for each ninja in the 2021 Finals Stage 1 (Season 13). The data includes the ninja‚Äôs name, their sex, the obstacle they failed on, and the cause of that failure.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nname\nName of the American Ninja Warrior\n\n\nsex\nSex of the American Ninja Warrior (M/F)\n\n\nobstacle\nThe name of the obstacle the ninja fell on or the last obstacle they completed if they ran out of time or finished the course\n\n\nobstacle_number\nThe obstacle‚Äôs place in the run\n\n\ncause\nWhat caused the ninja to fail (Fall/Time/Complete)\n\n\n\n\n\nNote: It is important to recognize that if the competitor fell on the 7th obstacle the obstacle number is 7. However, if the competitor ran out of time on the 7th obstacle, the obstacle number is 6, since the 6th obstacle was the last time that we could say they either fell or completed an obstacle. This will be important for censoring later. If the competitor completed the course, the obstacle number is set to the last obstacle they completed, which is the 9th obstacle in the 2021 Finals Stage 1 course.",
    "crumbs": [
      "Home",
      "Obstacle Competitions",
      "American Ninja Warrior - Kaplan-Meier Survival Analysis"
    ]
  },
  {
    "objectID": "obstacle_competitions/american_ninja_warrior/index.html#points-of-confusion",
    "href": "obstacle_competitions/american_ninja_warrior/index.html#points-of-confusion",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analysis",
    "section": "Points of Confusion",
    "text": "Points of Confusion\nUse the following code to create a table of the obstacle names and their corresponding obstacle numbers. This will help you understand the order of the obstacles in the course.\n\nninja |&gt; \n  distinct(obstacle, obstacle_number)\n\n# A tibble: 10 √ó 2\n   obstacle        obstacle_number\n   &lt;chr&gt;                     &lt;dbl&gt;\n 1 Slide Surfer                  1\n 2 Swinging Blades               2\n 3 Double Dipper                 3\n 4 Jumping Spider                4\n 5 Tire Run                      5\n 6 Dipping Birds                 7\n 7 Warped Wall                   6\n 8 The High Road                 8\n 9 Fly Hooks                     8\n10 Cargo Net                     9\n\n\nIt can be seen quickly that there are 2 obstacles with number 8. The duplicate obstacle number 8 is due to the fact that Stage One of the 2021 Finals allowed a split-decision. This means that competitors could choose between two different obstacles for obstacle 8.\n\n\nRead about the 2021 Stage 1 Split Decision here.\nAdditionally, one competitor Joe Moravsky ran the course twice (falling the first time and completing it the second time). This is because he was the Safety Pass Winner from the previous round. The Safety Pass allows a competitor to run the course again if they fail the first time. This poses some questions about how to handle this observation. We could\n\nInclude both runs in the analysis, treating them as separate observations.\nInclude only the first run in the analysis.\nInclude only the second run in the analysis.\n\nIf we include the second run in the analysis, we are neglecting the fact that Joe Moravsky had already attempted the course once and may have learned from his mistakes.\nIf we include the first run in the analysis, an argument could be made that Moravsky only failed the first time because he knew he had a second chance.\nIn most survival analysis situations, an individual would not be capable of participating twice from the beginning (after all if death were truly the event of interest, it would be safe to say there is no second chance). Therefore, we will only include the first run in the analysis.\nRun the code below to remove the second run from the data.\n\nninja &lt;- ninja |&gt; \n  filter(name != \"Joe Moravsky (Safety Pass)\")\n\nNow that we‚Äôve cleared some of the muddiness, let‚Äôs move on to the fun stuff!",
    "crumbs": [
      "Home",
      "Obstacle Competitions",
      "American Ninja Warrior - Kaplan-Meier Survival Analysis"
    ]
  },
  {
    "objectID": "obstacle_competitions/american_ninja_warrior/index.html#censored-data",
    "href": "obstacle_competitions/american_ninja_warrior/index.html#censored-data",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analysis",
    "section": "Censored Data",
    "text": "Censored Data\nIn survival analysis, we often need to censor data. Censored data occurs when the event of interest has not occurred for some of the observations. For the heart attack example, suppose we follow subjects for 10 years. It is not unlikely that some will not have a second heart attack in that time. It is also possible that we are unable to follow all subjects for the full 10 years. Such data is censored. We have information that the person did not have a second heart attack for a certain amount of time, but we do not have information beyond that time.\nWe could, of course, just exclude these observations and use only those who had a second heart attack. The problem is that this creates potential bias. Suppose those who had treatment A do not have a second heart attack as quickly. More in that group might not have a heart attack in the 10 years. By excluding them we would bias our estimate of the average time until a heart attack for this group to be shorter than it actually is.",
    "crumbs": [
      "Home",
      "Obstacle Competitions",
      "American Ninja Warrior - Kaplan-Meier Survival Analysis"
    ]
  },
  {
    "objectID": "obstacle_competitions/american_ninja_warrior/index.html#introducing-survival-analysis-using-american-ninja-warrior-data",
    "href": "obstacle_competitions/american_ninja_warrior/index.html#introducing-survival-analysis-using-american-ninja-warrior-data",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analysis",
    "section": "Introducing Survival Analysis using American Ninja Warrior Data",
    "text": "Introducing Survival Analysis using American Ninja Warrior Data\nIn the heart attack study, we have two key variables. One is the time each subject is observed \\(t\\) = years, which we suppose are only integer values 1, 2, 3,‚Ä¶(technically this would be interval censored data but we will not explore this more advanced topic here). The second is the censoring variable \\(c_i\\), which tells us if we observed the event \\(c_i = 1\\) or not \\(c_i = 0\\).\nOur data for two subjects might look like:\n\n\nSubjectTimeCensor1812100\n\n\nWe see that subject 1 was observed for 8 years and had a heart attack at year 8. Subject 2, on the other hand, was observed for the full 10 years and did not have a heart attack.\nTo illustrate survival analysis for the ANW data, we will define the time using the obstacle number. So, instead of time \\(t = 0, 1, 2,...\\) years we will have time \\(n = 0, 1, 2, 3,...\\) obstacles. Note that there is an actual time element to the obstacle course, but for the purpose of learning survival analysis our time is the obstacle number.\nThe event of interest is the failure of a competitor on an obstacle. This is important as it is possible to fail the course itself, for stage 1, if the allotted time expires. However, that is not the outcome of interest here.\nAs with the heart attack example, there are several ways in which the data might be censored. If a competitor completes the course, we do not know how many obstacles they would complete before the event (failing an obstacle) would occur. This is similar to the case where a subject was followed for 10 years without a heart attack; our subject was followed for 9 obstacles (our time variable) without failing an obstacle.\nAn additional possible censoring occurs if the time limit is reached. Again, if that occurs we do not know how many obstacles the competitor would complete before failing one, although we do know that they didn‚Äôt fail on any of the ones they completed before time was up. In the heart attack study this would be similar to a case where we observed a subject for 6 years and they died due to some other cause but never had a heart attack.\n\n\nNote: Another advanced survival model involves analyzing ‚Äúcompeting risks‚Äù if interest is in multiple different outcomes such as death from a heart attack or death from some other cause. In the ANW case, failing an obstacle or running out of time would be competing risks. We will not explore this topic here.\nUse the code below to create a new column called censor in the ninja data that is a binary indicator of whether or not the observation should be censored. This column will be used to indicate whether the data is censored or not.\n\n# Makes a column called censor that is 1 if the competitor failed and 0 if they completed the course or ran out of time\nninja &lt;- ninja |&gt; \n  mutate(censor = if_else(cause %in%  c(\"Complete\", \"Time\"), 0, 1))\n\nThree competitors in the resulting data are:\n\n\nnamesexobstacleobstacle_numbercausecensorMeagan MartinFSlide Surfer1Fall1Sean BryanMCargo Net9Complete0Brett SimsMFly Hooks8Time0\n\n\nWe can interpret this data:\n\nMeaghan Martin failed on the first obstacle so the time (obstacle_number) is 1 and censor = 1.\n\nSean Bryan completed the course so the obstacle_number is 9 but censor = 0. We do not know how many obstacles Sean completes before failing one; he moved on to the second stage course but we did not collect data after the first stage. He was censored due to the ‚Äústudy time‚Äù of 9 obstacles.\nBrett Sims ran out of time while on the 9th obstacle. We know he completed 8 obstacles so his ‚Äútime‚Äù is obstacle_number = 8 but we do not know if he would have failed the 9th obstacle or not (or any further obstacles) so his observation is censored.",
    "crumbs": [
      "Home",
      "Obstacle Competitions",
      "American Ninja Warrior - Kaplan-Meier Survival Analysis"
    ]
  },
  {
    "objectID": "obstacle_competitions/american_ninja_warrior/index.html#estimating-the-survival-function-using-kaplan-meier",
    "href": "obstacle_competitions/american_ninja_warrior/index.html#estimating-the-survival-function-using-kaplan-meier",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analysis",
    "section": "Estimating the Survival Function using Kaplan-Meier",
    "text": "Estimating the Survival Function using Kaplan-Meier\nThe Kaplan-Meier estimator uses information from all of the observations in the data to provide a non-parametric estimate of the survival function. The estimator considers survival to a certain point in time as a series of steps defined at the observed times.\nIn order to calculate the probability of surviving past a certain point in time (past a certain obstacle in this case), the conditional probability of surviving past that point given that the competitor has survived up to that point must be calculated first.\nThe formula for the conditional probability of surviving past a point in time (\\(t_i\\)) given that the competitor has survived up to that point in time(\\(t_{i-1}\\)) is:\n\n\nNote: This function could also be written as \\(P(T &gt; t_i | T \\geq t_{i-1}) = \\frac{n_i - d_i}{n_i}\\)\n\\(P(T \\geq t_i | T \\geq t_{i-1}) = 1- \\frac{d_i}{n_i}\\)\nWhere:\n\n\\(d_i\\) is the number of competitors that failed at time \\(t_i\\)\n\\(n_i\\) is the number of competitors that were at risk at time \\(t_i\\)\n\nThe Kaplan-Meier estimator is the product of the conditional probabilities of surviving past each point in time up through that point in time.\n\\(\\hat{S}(t) = \\prod_{t_i \\leq t} (1 - \\frac{d_i}{n_i})\\)\n\n\nNote: Censored data does not count in the at risk competitors\nwhere \\(n_i = n_{i-1} - d_{i-1} - c_{i-1}\\)\n\n\\(c_i\\) is the number of competitors censored at time \\(t_i\\)\n\nFor example, we create a data set with 25 competitors and 5 obstacles:\n\n# Setting a seed for reproducibility\nset.seed(123)\n\n# Creating fake data\nfake_data &lt;- tibble(obstacle_number = c(1:5, 2,5), censor = c(rep(1, 5), rep(0, 2))) |&gt; \n  sample_n(25, replace = TRUE)\n\nhead(fake_data)\n\n# A tibble: 6 √ó 2\n  obstacle_number censor\n            &lt;dbl&gt;  &lt;dbl&gt;\n1               5      0\n2               5      0\n3               3      1\n4               2      0\n5               3      1\n6               2      1\n\n\nEach row of the data is a competitor (\\(i = 1,...,25\\)) and the first column (‚Äúobstacle_number‚Äù) is the last obstacle for which each was observed. The ‚Äúcensor‚Äù variable is the indicator of whether the obstacle was failed (1 = failed).\nWe will step through a few calculations to illustrate the Kaplan-Meier (KM) estimator. The calculation is easiest if the data is put in a format by the obstacle number (time) when things occurred. The code below produces this format:\n\nfake_data_summary &lt;- fake_data |&gt; \n  group_by(obstacle_number) |&gt; \n  summarize(fails = sum(censor == 1),\n            censored = sum(censor == 0)) |&gt;\n  ungroup() \n\nfake_data_summary\n\n# A tibble: 5 √ó 3\n  obstacle_number fails censored\n            &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n1               1     4        0\n2               2     3        4\n3               3     7        0\n4               4     2        0\n5               5     3        2\n\n\nWe see that each obstacle (time) had events. For the first obstacle, these events were all failures (4 fell). At obstacle 2, there were 3 failures but 4 censored. The censored competitors ran out of time before they could either complete or fail obstacle 3. We similarly see failures and censored observations at the last obstacle. Those censored at the last obstacle were those who completed the course.\nSuppose we wanted to calculate the Kaplan-Meier estimate of surviving past obstacle 2 we would need to find the following probabilities:\n\\(P(T &gt; 1 | T &gt; 0) = P(T &gt; 1) =  1 - \\frac{\\text{number of competitors that failed at obstacle 1}}{\\text{number of competitors that attempted obstacle 1}}\\)\n\\(P(T &gt; 2 | T &gt; 1) = 1 - \\frac{\\text{number of competitors that failed at obstacle 2}}{\\text{number of competitors that attempted obstacle 2}}\\)\nBelow we calculate the first probability:\n\nfake_data_summary &lt;- fake_data_summary |&gt; \n  mutate(at_risk = c(25,rep(NA,4)),\n         p_surv_cond = (at_risk - fails)/at_risk,\n         p_surv_km = p_surv_cond*1)\n\nfake_data_summary\n\n# A tibble: 5 √ó 6\n  obstacle_number fails censored at_risk p_surv_cond p_surv_km\n            &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;   &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1               1     4        0      25        0.84      0.84\n2               2     3        4      NA       NA        NA   \n3               3     7        0      NA       NA        NA   \n4               4     2        0      NA       NA        NA   \n5               5     3        2      NA       NA        NA   \n\n\nWe compute the probability using the formula earlier:\n\\(P(T &gt; t_i | T \\geq t_{i-1}) = \\frac{n_i - d_i}{n_i}\\)\nWe create a column for the number at risk (\\(n_1\\)) which is 25 for the first obstacle. \\(d_i\\) is found already in the ‚Äúfails‚Äù column.\nIn order to compute the second probability, we need to compute the next at risk value, \\(n_2\\). This value is 25 minus the 4 failures or 21. Since 0 were censored, we do not lose any other competitors. From there, we can again compute the conditional probability using the formula:\n\nfake_data_summary &lt;- fake_data_summary |&gt; \n  mutate(at_risk = c(25, 21, rep(NA,3)),\n         p_surv_cond = (at_risk - fails)/at_risk)\n\nfake_data_summary\n\n# A tibble: 5 √ó 6\n  obstacle_number fails censored at_risk p_surv_cond p_surv_km\n            &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;   &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1               1     4        0      25       0.84       0.84\n2               2     3        4      21       0.857     NA   \n3               3     7        0      NA      NA         NA   \n4               4     2        0      NA      NA         NA   \n5               5     3        2      NA      NA         NA   \n\n\nFinally, we then need to multiply these two probabilities together to get the Kaplan-Meier estimate of surviving past obstacle 2.\n\\(P(T &gt; 2) = P(T &gt; 1) * P(T &gt; 2 | T &gt; 1)\\)\nThe following code calculates the Kaplan-Meier estimate of surviving past obstacle 2:\n\nfake_data_summary$p_surv_km[2] &lt;- fake_data_summary$p_surv_cond[1]*\n  fake_data_summary$p_surv_cond[2]\nfake_data_summary\n\n# A tibble: 5 √ó 6\n  obstacle_number fails censored at_risk p_surv_cond p_surv_km\n            &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;   &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1               1     4        0      25       0.84       0.84\n2               2     3        4      21       0.857      0.72\n3               3     7        0      NA      NA         NA   \n4               4     2        0      NA      NA         NA   \n5               5     3        2      NA      NA         NA   \n\n\nThe Kaplan-Meier estimate of surviving past obstacle 2 in this fake example is 0.72.\n\n\n\n\n\n\nNoteExercise 2: Fake Data Kaplan-Meier Estimates\n\n\n\nIn this exercise you will calculate the remaining three Kaplan-Meier estimates of surviving past each obstacle for the fake data. Important note: how many are at risk for the next obstacle (3) in the table? 21 attempted obstacle 2 and three failed. However, 4 ran out of time and were censored! Thus, the correct number at risk, \\(n_3\\) is \\(21 - 7 = 14\\)! Use this value to help you complete the table.",
    "crumbs": [
      "Home",
      "Obstacle Competitions",
      "American Ninja Warrior - Kaplan-Meier Survival Analysis"
    ]
  },
  {
    "objectID": "obstacle_competitions/american_ninja_warrior/index.html#kaplan-meier-estimator-manual-calculation",
    "href": "obstacle_competitions/american_ninja_warrior/index.html#kaplan-meier-estimator-manual-calculation",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analysis",
    "section": "Kaplan-Meier Estimator Manual Calculation",
    "text": "Kaplan-Meier Estimator Manual Calculation\nThe ninja data frame contains information about individual competitors in the ninja competition. We will need to summarize the data to calculate the Kaplan-Meier estimator manually.\n\n\n\n\n\n\nNoteExercise 3: Manual Calculation of Kaplan-Meier Estimator\n\n\n\nIn this exercise you will calculate the Kaplan-Meier estimator of surviving past each obstacle in the ninja competition step-by-step.\n\n\n\n\n\n\nNotePart 1: Number of Events\n\n\n\nThe first step is to calculate the number of competitors that failed and the number of competitors that were censored at each point in time. These are the \\(d_i\\) and \\(c_i\\) values needed to calculate the conditional probability of surviving past each point in time.\nUse the following code to sum the number of competitors that failed and the number of competitors that were censored at each obstacle.\n\n\nninja_summary &lt;- ninja |&gt; \n  group_by(obstacle = obstacle_number) |&gt;\n  summarize(fails = sum(cause == \"Fall\"),\n            censored = sum(cause %in% c(\"Complete\", \"Time\")))\n\n\nAt which obstacle did the most competitors fail?\nAt which obstacle were the most competitors censored (not including obstacle 9 which is completion)?\n\n\n\n\n\n\n\n\n\nNotePart 2: At Risk Competitors\n\n\n\nThe second step is to calculate the number of competitors at risk at each point in time. This is the \\(n_i\\) value needed to calculate the conditional probability of surviving past each point in time.\nUse the following code to calculate the number of competitors at risk at each point in time from the ninja_summary data frame.\n\n\nninja_summary &lt;- ninja_summary |&gt; \n  mutate(attempts = 68 - lag(cumsum(fails), default = 0) - \n           lag(cumsum(censored), default = 0))\n\n\nWhich obstacle had the most competitors at risk?\nWhy don‚Äôt previously censored competitors contribute to the number of competitors at risk at the obstacle?\n\n\n\n\n\n\n\n\n\nNotePart 3: Conditional Survival Probability\n\n\n\nThe third step is to calculate the conditional probability of survival at each point in time. This is the \\(P(T \\geq t_i | T \\geq t_{i-1})\\) value needed to calculate the Kaplan-Meier estimator and is calculated as \\(1 - \\frac{d_i}{n_i}\\) or 1 minus the conditional ‚Äúfailure probability‚Äù.\n\nUse the ninja_summary data frame to calculate the probability that someone survives each obstacle. Do this using the mutate function to create a new column called surv_prob. Survival probability is 1 minus the number of competitors that failed divided by the number of competitors at risk. Save this data frame as ninja_summary.\n\n\n\nWhat percentage of at-risk competitors survived the first obstacle?\nWhat percentage of at-risk competitors failed the fifth obstacle?\nWhich obstacle had the highest conditional fail probability?\nDid obstacle 2 or obstacle 7 have a higher conditional survival rate?\n\n\n\n\n\n\n\n\n\nNotePart 4: Kaplan-Meier\n\n\n\nThe final step is to calculate the Kaplan-Meier estimator of surviving past each point in time (\\(\\hat{S}(t)\\)) This is calculated as the product of the conditional probabilities of surviving past each point in time up through the desired point in time (\\(\\prod_{i=1}^{t} P(T \\geq t_i | T \\geq t_{i-1})\\)).\nUse the following code to calculate the Kaplan-Meier estimator manually by multiplying the conditional probabilities of surviving past each point in time up through the desired point in time.\n\n\nninja_summary &lt;- ninja_summary |&gt; \n  mutate(km = cumprod(surv_prob))\n\n\nWhat is the Kaplan-Meier estimate for surviving past the first obstacle?\nWhat is the Kaplan-Meier estimate for surviving past first five obstacles?\nWhat is the farthest obstacle that for which the Kaplan-Meier estimator has more 50% of competitors surviving?\n\n\n\n\nPlotting the Kaplan-Meier Estimator\nWe will now use ggplot2 to plot the Kaplan-Meier estimator for the ninja competitors. The Kaplan-Meier estimator is a step function, so we will use geom_step to plot the estimator. We will also use geom_point to plot the points where the estimator changes.\n\n\n\n\n\n\n\nNotePart 5: Plotting the Kaplan-Meier Estimator\n\n\n\n\n\nUse the ninja_summary data frame in conjunction with ggplot2‚Äôs geom_step and geom_point to plot the Kaplan-Meier estimator for the ninja competitors.\nComment on the plot.\nWhat do you notice about where the lowest point on the plot is in regard to survival probability? Does survival probability reach zero? Why or why not?\n\n\n\n\n\n\n Note: The censored column is the number of competitors that were not tracked after that obstacle. For any obstacle that is not the last one, the number censored are the amount that ran out of time on the next obstacle. For the last obstacle, the number censored are the competitors that completed the course.\n\n\nNote: The lag function shifts the cumsum of the fails column down one row. The default = 0 argument fills in the first row with 0. This is necessary to help calculate the number of competitors at risk at each obstacle. Note that the lag function is not used in conjunction with the cumsumfunction for the censored column.\n\n TIP: You can pipe the data frame into the mutate function to create a new column.\nTIP: The mutate function works like so: data_frame |&gt; mutate(new_column = calculation)\n\n Note: The cumprod function calculates the cumulative product of the values given to it.\n\n Type ?geom_step, ?geom_point, or ?ggplot in the console to learn more about these functions.\n\nTIP: Remember that you can add the + operator to continue adding layers to the plot like seen below\n\nggplot(your_data, aes(x = time_var, y = kaplan_meier_var)) +\n  geom_step() +\n  geom_point()\n\nTIP: You can also add labels to the plot using the labs function like seen below\n\nggplot(your_data, aes(x = time_var, y = kaplan_meier_var)) +\n  geom_step() +\n  geom_point() +\n  labs(title = \"Your Title\",\n       x = \"X Axis Label\",\n       y = \"Y Axis Label\")",
    "crumbs": [
      "Home",
      "Obstacle Competitions",
      "American Ninja Warrior - Kaplan-Meier Survival Analysis"
    ]
  },
  {
    "objectID": "obstacle_competitions/american_ninja_warrior/index.html#using-r-packages-to-automatically-calculate-the-kaplan-meier-estimator",
    "href": "obstacle_competitions/american_ninja_warrior/index.html#using-r-packages-to-automatically-calculate-the-kaplan-meier-estimator",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analysis",
    "section": "Using R Packages to Automatically Calculate the Kaplan-Meier Estimator",
    "text": "Using R Packages to Automatically Calculate the Kaplan-Meier Estimator\nPhew! That was a lot of tedious work to calculate and plot the Kaplan-Meier estimator manually. Luckily, there is a much easier way to calculate the Kaplan-Meier estimator using R.\nThe survival package in R provides a function called survfit that can be used to calculate the Kaplan-Meier estimator. The survfit function requires a Surv object as input. The Surv object is created using the Surv function, which requires two arguments:\n\nThe time to event data. The time to event data is the time at which the event occurred or the time at which the individual was censored. In our case this is the obstacle_number in our ninja data.\nThe event status. The event status is a binary variable that indicates whether the event occurred or the individual was censored. The event status is coded as 1 if the event occurred and 0 if the individual was censored. This is contained in the censor column of the ninja data.\n\nBelow a survfit model is created for the ninja dataset and the results are stored in the ninja_km object.\n\nninja_km &lt;- survfit(Surv(obstacle_number, censor) ~ 1, data = ninja)\n\n\n\n\n\n\n\nNoteExercise 4: Kaplan-Meier Estimates and Interpretation\n\n\n\nUse summary(ninja_km) to view a summary of the Kaplan-Meier estimator.\n\nDo the values in the survival column match the values you calculated manually?\n\n\nThe output also shows the 95% confidence intervals.\n\nWhich obstacle number is the first point in time where a survival rate of less than .5 falls within the 95% confidence interval?\nWhat do you notice about the standard error as the time increases and more ninjas have been eliminated?\n\n\n\n\nThe computations for calculating the Confidence Interval for the K-M Estimate are fairly complex. The method most commonly used is called the log-log survival function and was proposed by Kalbfleisch and Prentice (2002). This function is computed by \\(ln(-ln[\\hat{S}(t)])\\) with variance derived from the delta method and calculated by \\[\n\\frac{1}{[ln(\\hat{S}(t))]^2}\\sum_{t_i\\leq{t}}\\frac{d_i}{n_i(n_i - d_i)}\n\\].\nThe endpoints for the confidence interval for the log-log survival function are therefore found by \\(ln(-ln[\\hat{S}(t)]) \\pm Z_{1-\\alpha / 2} SE [ln(-ln[\\hat{S}(t)]) ]\\)\nAnd the endpoints expressed by the computer and seen in the summary are \\(exp[-exp(\\hat{c}_u)] \\text{ and } exp[-exp(\\hat{c}_l)]\\)\n\nQuartile Interpretation\nThe three quartiles are common statistics to look at when doing a survival analysis. The interpretations of these are as follows:\n\n\nNote: If the data is uncensored the estimate is just the median of the data. If the data is censored, the KM estimate is used to find these by finding the time at which it drops below the percentile\n\n25th Percentile- 75% of the people survive past this point in time\nMedian- 50% of the people will survive past this time\n75th Percentile- 25% survive past this time\n\n\n\n\n\n\n\nNoteExercise 5: Interpreting Quartiles\n\n\n\nUse the results from quantile(ninja_km) to answer the following questions\n\nWhat is the earliest time that the confidence intervals imply that the true mean of surviving past that time could be 75%? What is the latest time?\nWhat is the interpretation of the NA values in the 75th percentile columns?\nWhat is the earliest time (within the 95% confidence interval) at which the true survival rate suggests 50% of the competitors would fail on or before?\n\n\n\n\n\nPlotting with R\nAfter fitting a Kaplan-Meier model, we can use the ggsurvplot function from the survminer package to plot the Kaplan-Meier estimator. The ggsurvplot function requires the Kaplan-Meier model as input.\nBelow is an example of how easy it is to plot the Kaplan-Meier estimator using R.\n\nggsurvplot(ninja_km,\n           conf.int = TRUE)\n\nIgnoring unknown labels:\n‚Ä¢ fill : \"Strata\"\nIgnoring unknown labels:\n‚Ä¢ fill : \"Strata\"",
    "crumbs": [
      "Home",
      "Obstacle Competitions",
      "American Ninja Warrior - Kaplan-Meier Survival Analysis"
    ]
  },
  {
    "objectID": "obstacle_competitions/american_ninja_warrior/index.html#the-log-rank-test-optionaladvanced",
    "href": "obstacle_competitions/american_ninja_warrior/index.html#the-log-rank-test-optionaladvanced",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analysis",
    "section": "The Log-Rank Test (optional/advanced)",
    "text": "The Log-Rank Test (optional/advanced)\nThe Log-Rank Test is a statistical test used to compare the survival probabilities of two or more groups. The test is used to determine if there is a statistically significant difference between the survival probabilities of the groups.\nThe hypotheses for our log-rank test are as follows:\n\n\\(H_0: S_M(t) = S_F(t)\\) for all \\(t\\)\n\\(H_a: S_M(t) \\neq S_F(t)\\) for at least one \\(t\\)\n\nwhere \\(S_M(t)\\) is the survival probability for males at time \\(t\\) and \\(S_F(t)\\) is the survival probability for females at time \\(t\\).\nWhen comparing two groups like this, we can calculate the expected number of deaths in each group. Below is the formula for calculating the number of expected deaths for group 0 at time \\(t_i\\):\n\\[\\hat{e}_{0i} = \\frac{n_{0i}d_i}{n_i}\\]\nwhere \\(n_{0i}\\) is the number of individuals at risk in group 0 at time \\(t_i\\), \\(d_i\\) is the total number of deaths at time \\(t_i\\), and \\(n_i\\) is the total number of individuals at risk at time \\(t_i\\).\nThe variance estimator is drawn from the hypergeometric distribution. The formula for the variance of the number of deaths in group 0 at time \\(t_i\\) is:\n\\[\\hat{v}_{0i} = \\frac{n_{0i}n_{1i}d_i(n_i - d_i)}{n_i^2(n_i - 1)}\\]\nwhere \\(n_{0i}\\) is the number of individuals at risk in group 0 at time \\(t_i\\), \\(n_{1i}\\) is the number of individuals at risk in group 1 at time \\(t_i\\), \\(d_i\\) is the total number of deaths at time \\(t_i\\), and \\(n_i\\) is the total number of individuals at risk at time \\(t_i\\).\nThe test statistic is calculated as the square of the sum of the differences between the observed and expected number of deaths for the group divided by the sum of the variance of the number of deaths for the group at each time point. The formula for the test statistic is as follows:\n\\[Q = \\frac{[\\sum_{i=1}^m (d_{0i} - \\hat{e}_{0i})]^2}{\\sum_{i=1}^m \\hat{v}_{0i}}\\]\nUsing the null hypothesis, the p-value can be calculated using the chi-squared distribution with 1 degree of freedom.\n\\[p = P(X^2(1) &gt; Q)\\]\n\n\nNOTE: This use of the chi-squared distribution assumes that the censoring is independent of the group.\n\nNOTE: The degrees of freedom for the chi-squared distribution is 1 because we are comparing two groups. If we were comparing more than two groups, the degrees of freedom would be the number of groups minus 1.\n\nThankfully R has a built-in function to perform the log-rank test. The survdiff function in the survival package can be used to perform the log-rank test. The survdiff function requires a Surv object as input. It will then perform the log-rank test and return the test statistic and p-value.\n\n\nNOTE: The log-rank test is a non-parametric test. This means that it does not assume that the data is normally distributed.\nThe code below runs the log-rank test on the ninja data set to compare the survival of male and female competitors.\n\nninja_km_diff &lt;- survdiff(Surv(obstacle_number,\n                               censor) ~ sex, data = ninja)\n\n\n\n\n\n\n\nNoteExercise 7: Comparing Survival vs.¬†Expected\n\n\n\nUse the code below to see the results of the survdiff function\n\nninja_km_diff\n\nCall:\nsurvdiff(formula = Surv(obstacle_number, censor) ~ sex, data = ninja)\n\n       N Observed Expected (O-E)^2/E (O-E)^2/V\nsex=F 12       10     4.05      8.72      11.1\nsex=M 56       27    32.95      1.07      11.1\n\n Chisq= 11.1  on 1 degrees of freedom, p= 9e-04 \n\n\n\nHow many female competitors are in the data set? How many fell? How many were expected to fall (round to the nearest whole number)?\nDid more or less male competitors fall than expected?\nWhat is the p-value of the test? What does this mean?",
    "crumbs": [
      "Home",
      "Obstacle Competitions",
      "American Ninja Warrior - Kaplan-Meier Survival Analysis"
    ]
  },
  {
    "objectID": "obstacle_competitions/american_ninja_warrior/index.html#other-nonparametric-tests-optionaladvanced",
    "href": "obstacle_competitions/american_ninja_warrior/index.html#other-nonparametric-tests-optionaladvanced",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analysis",
    "section": "Other Nonparametric Tests (optional/advanced)",
    "text": "Other Nonparametric Tests (optional/advanced)\nAlthough the survdiff function uses the most common test for comparing Kaplan-Meier curves, there are a variety of other methods that can be used. These other methods developed because of the log rank test‚Äôs greatest weakness: It weights all time points equally even though there are fewer people at risk later than at the beginning. These methods are all similar to a standard log-rank test but attempt to weight time points in order to detect differences better throughout time as opposed to the end, which is where the log-rank test finds most of its differences. The ratio of the observed and expected number of deaths is calculated in a similar manner but with weights applied as seen below:\n\\[Q = \\frac{[\\sum_{i=1}^m w_i(d_0i - \\hat{e}_{0i})]^2}{\\sum_{i=1}^m w_i^2\\hat{v}_{0i}}\\]\nBelow some of the other methods that can be used are broken down, with their weighting and purpose explained:\n\nWilcoxon (Gehan-Breslow) Test: This test gives more weight to early time points based on the number of individuals at risk. Its weighting is: \\[w_i = n_i\\]\nTarone-Ware Test: This test gives more weight to time points with more individuals at risk, but less heavily than the Gehan-Breslow test. Its weighting is: \\[w_i = \\sqrt{n_i}\\]\nPeto-Prentice Test: This test also gives more weight to earlier time points, but not as much as the Gehan-Breslow test. Its weighting is:\n\n\\[w_i = \\tilde{S}(t_{(i)})\\] where \\[\\tilde{S}(t_{(i)}) = \\prod_{t_{(j)}&lt;t} \\left(1 - \\frac{d_j}{n_j}\\right)\\]\n\nFleming-Harrington Test: This test allows the user to chose \\(\\rho\\) and \\(q\\) values to weight the time points. If \\(\\rho\\) is larger it will weight the earlier time points more heavily, and if \\(q\\) is larger it will weight the later time points more heavily. Its weighting is:\n\n\\[w_i = [\\tilde{S}(t_{(i-1)})]^{\\rho}[1 - \\tilde{S}(t_{(i-1)})]^q\\] where \\[\\tilde{S}(t_{(i- 1)}) = \\text{Kaplan-Meier Estimate at time } t_{i-1}\\]\nThankfully the surv_pvalue function in the survminer package can be used to calculate the p-value for all of these tests by changing the method argument. See the table below for the different method arguments to use:\n\n\n\nTest\nMethod Argument\n\n\n\n\nLog Rank Test\nDefault- no argument needed\n\n\nWilcoxon/Gehan-Breslow\nmethod = ‚Äún‚Äù\n\n\nTarone-Ware\nmethod = ‚ÄúTW‚Äù\n\n\nPeto-Prentice\nmethod = ‚ÄúPP‚Äù\n\n\nFleming-Harrington\nmethod = ‚ÄúFH‚Äù\n\n\n\nThe surv_pvalue function does need a survfit object as input. We can use the ninja_km_gender object created earlier to check the p-values for the different methods.\n\n\n\n\n\n\nNoteExercise 8: Log-Rank Tests\n\n\n\nRun the code below to see the p-values for the different methods.\n\nsurv_pvalue(ninja_km_gender) #log rank\nsurv_pvalue(ninja_km_gender, method = \"n\") #Gehan Breslow (generalized Wilcoxon)\nsurv_pvalue(ninja_km_gender, method = \"TW\") #tarone-ware\nsurv_pvalue(ninja_km_gender, method = \"PP\") #Peto-Prentice\nsurv_pvalue(ninja_km_gender, method = \"FH\") #Fleming-Harrington\n\n\nUsing \\(\\alpha = 0.05\\), do all of the tests lead to the same conclusion? If so what is the conclusion? If not which ones agree and which ones do not?\nWhich test had the smallest p-value?\nWhich test had the largest p-value?\nBased off of the p-values for the different tests, would you conclude that the difference between the genders is most likely more significant at the beginning or end of the course?",
    "crumbs": [
      "Home",
      "Obstacle Competitions",
      "American Ninja Warrior - Kaplan-Meier Survival Analysis"
    ]
  },
  {
    "objectID": "robotics/FIRST_Robotics_Competition/index.html",
    "href": "robotics/FIRST_Robotics_Competition/index.html",
    "title": "FIRST Robotics Competition - Winning Chances",
    "section": "",
    "text": "Introduction\nThe FIRST Robotics Competition (FRC) is a high school level robotics competition, in which ‚Äú[u]nder strict rules and limited time and resources, teams of high school students are challenged to build industrial-size robots to play a difficult field game in alliance with other teams.‚Äù It combines ‚Äúthe excitement of sport with the rigors of science and technology‚Äù.\nOne of the key features of FRC is that robot/team competes not individually, but in alliance with other teams. So, it is important for teams to ‚Äúscout‚Äù other teams as potential alliance partners. Various methodologies/models to evaluate each team‚Äôs potential contribution were developed. One of the popular models is called Expected Points Added (EPA) model.\nDetailed algorithm of the EPA model can be found at here (https://www.statbotics.io/blog/epa). Briefly, the EPA model builds upon the Elo rating system which is a ‚Äúwell-known method for ranking chess players, and has been adapted to many other domains.‚Äù It produces predicted probabilities of winning for the alliance based on the past performances of each team in the alliance, as well as teams in the opposition alliance. As such, there is a desire/need to assess how good the EPA model prediction is.\nBrier score originated with weather forecast research. It was designed to evaluate the predicted probabilities against the actual outcomes and is straight forward to calculate. While it is not widely used outside specific use cases, it is one of many approaches for the important step of evaluating models based on their predictions. Since the EPA model provides the predicted winning probabilities, Brier score is useful for evaluating its performance by comparing its predicted winning probabilities to the actual FRC outcomes.\n\n\n\n\n\n\nNoteLearning Objectives\n\n\n\n\n\nBy the end of this activity, you will be able to:\n\nCalculate Brier score.\nInterpret Brier score.\n\n\n\n\n\n\nData\nIn this lesson, we will use the EPA data and competition outcomes calculated and compiled by the website statbotics.io.\n\ndtAll &lt;- read.csv(\"matches.csv\")\ndtUse &lt;- subset(dtAll, status==\"Completed\" & offseason==\"f\", \n                select=c(year, event, playoff, comp_level, winner, epa_win_prob))\n\nBelow is a description of the variables:\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nyear\nthe year/season of the FRC event\n\n\nevent\nunique identifier for each FRC event\n\n\nplayoff\n‚Äút‚Äù for playoff match; ‚Äúf‚Äù for qualifying match\n\n\ncomp_level\n‚Äúqm‚Äù for qualifying match; ‚Äúsf‚Äù for semifinals match; ‚Äúf‚Äù for finals match\n\n\nwinner\nwinning alliance (‚Äúred‚Äù or ‚Äúblue‚Äù) of the match\n\n\nepa_win_prob\npredicted winning probability for the Red Alliance by EPA model\n\n\n\n\nThe data covers the competition seasons from 2002 to 2023, except for 2021 due to the COVID pandemic.\n\nunique(dtUse$year)\n\n [1] 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016\n[16] 2017 2018 2019 2020 2022 2023\n\n\nFor our example, we will use a particular event, the Hopper Division competition at the 2023 FRC World Championship in Houston, to illustrate the calculation of Brier score.\n\ndt2023hop &lt;- subset(dtUse, event==\"2023hop\")\n\nBelow is what the raw data looks like. Each row is a match between a Red alliance and a Blue alliance. Each alliance consists of three robots/teams.\n\n\n\n\n\n\n\n\nBrier Score\nFor match \\(i\\), let \\(f_i\\) denote the probability forecast. In our case, it is the predicted winning probability for the Red Alliance by EPA model, i.e., the variable epa_win_prob. Let \\(o_i\\) denote the match outcome: \\(o_i=1\\) when the Red alliance won and \\(o_i=0\\) when the Blue alliance won. The Brier score for match \\(i\\) is calculated as \\((f_i - o_i)^2\\). For example, suppose it is predicted that the Red alliance will win with 80% probability, i.e., \\(f_i=0.8\\), if the actual outcome is that the Red alliance won, the Brier score is \\((0.8-1)^2=0.04\\). If the actual outcome is that the Blue alliance won, the Brier score is \\((0.8-0)^2=0.64\\).\nBrier score is a quantity bounded by \\(0\\) and \\(1\\). Brier score of \\(0\\) means correctly predicting the outcome with 100% certainty. 50:50 random guess would give a Brier score of \\(0.25\\). The overall Brier score for all the matches during a competition event or season is simply the average of individual match scores: \\[\\frac{1}{N} \\sum_{i=1}^N (f_i - o_i)^2\\] The following table shows the calculation for each match.\n\n\n\n\n\n\nThe overall Brier score for the 2023 Hopper Division event is 0.1366352, which is slightly worse than the half way between perfect \\(0\\) and random guess \\(0.25\\).\n\n\nYour Turn\nNow, it‚Äôs your turn. Please use the data from the Turing Division competition at the 2022 FRC World Championship to calculate the average Brier score for the event. You should find the data in the file dt2022tur.csv.\n\ndt2022tur &lt;- subset(dtUse, event==\"2022tur\")\nwrite.csv(dt2022tur, \"dt2022tur.csv\")\n\n\n\n\n\n\n\nNoteAnswer\n\n\n\n\n\nThe average Brier score for the 2022 Turing Division event is 0.1160236, which is better than the Brier score for the 2023 Hopper Division event.\n\n\n\n\n\nOver the Years\nSince we have the data for more than 20 years, we could answer an interesting question: did the predictive ability of the EPA model change over the years?\nWe build two simple functions to do the calculations.\n\n\nTwo Functions\n\nThe first function calculates the Brier score for a given data set. Occasionally, a game can end in a draw. We assign the value of \\(0.5\\) to \\(o_i\\) for a draw.\n\ncalcBS &lt;- function(dt){\n  n &lt;- nrow(dt)\n  outcome &lt;- rep(NA, n)\n  outcome[dt$winner==\"red\"] &lt;- 1\n  outcome[dt$winner==\"draw\"] &lt;- 0.5\n  outcome[dt$winner==\"blue\"] &lt;- 0\n  diff &lt;- dt$epa_win_prob - outcome\n  Brier &lt;- mean(diff^2)\n  c(n=n, Brier=Brier)\n}\n\nThe second function separates the data by year and does the calculation for each year.\n\nbyYear &lt;- function(dt=dtUse) {\n  yrs &lt;- unique(dt$year)\n  m &lt;- length(yrs)\n  size &lt;- Brier &lt;- rep(NA, m)\n  for (i in 1:m) {\n    dat &lt;- subset(dt, year==yrs[i])\n    res &lt;- calcBS(dt=dat)\n    size[i] &lt;- res[1]\n    Brier[i] &lt;- res[2]\n  }\n  data.frame(year=yrs, n=size, Brier=Brier)\n}\nFRC &lt;- byYear()\n\n\nBelow are the Brier scores from 2002 to 2023.\n\nknitr::kable(FRC)\n\n\n\n\nyear\nn\nBrier\n\n\n\n\n2002\n2197\n0.2351889\n\n\n2003\n3173\n0.2225493\n\n\n2004\n3198\n0.2069319\n\n\n2005\n2059\n0.2043631\n\n\n2006\n3283\n0.1997467\n\n\n2007\n3563\n0.2099309\n\n\n2008\n4036\n0.1892942\n\n\n2009\n4567\n0.1961946\n\n\n2010\n5564\n0.1691369\n\n\n2011\n6224\n0.1621286\n\n\n2012\n7707\n0.1841302\n\n\n2013\n8242\n0.1704309\n\n\n2014\n10663\n0.1906669\n\n\n2015\n11810\n0.1841460\n\n\n2016\n13286\n0.1794790\n\n\n2017\n15429\n0.2043697\n\n\n2018\n16930\n0.1750251\n\n\n2019\n18022\n0.1758972\n\n\n2020\n4634\n0.1817734\n\n\n2022\n14645\n0.1480655\n\n\n2023\n16319\n0.1604984\n\n\n\n\n\nIt is interesting to note that the predictive ability of the EPA model has improved for the past 20 years. Since the model has not changed, I believe the improvement comes from established teams becoming more consistent and predictable. Meanwhile, the pool of newer, less experienced teams has stayed healthy.\n\nplot(FRC$year, FRC$Brier)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteConclusion\n\n\n\n\n\nIn conclusion, Brier score is a simple statistic that assesses the probability prediction against actual outcome. A smaller Brier score corresponds to better prediction.\nThe EPA model has been getting better at predicting FRC match outcome.\n\n\n\n\n\nAuthor\nCreated by Jake Tan (Wissahickon High School). Jake is a subsystem leader at FRC Team 341, Miss Daisy. Team 341 competed at FRC World Championship in the Turing Division in 2022 and Hopper Division in 2023.\n\n\nHow to Cite\nIf you use this module in your work, please cite it as follows:\nTan, J. (2025, January 15). FIRST Robotics Module. ‚ÄúThe SCORE Network,‚Äù https://doi.org/10.17605/OSF.IO/BRG8Z\nYou can include this citation directly in your references or bibliography.",
    "crumbs": [
      "Home",
      "Robotics",
      "FIRST Robotics Competition - Winning Chances"
    ]
  },
  {
    "objectID": "rowing/index.html",
    "href": "rowing/index.html",
    "title": "Rowing",
    "section": "",
    "text": "These modules use rowing data to teach topics in statistics and data science.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOlympic Rowing Medals Between 1900 and 2022 - Data Wrangling\n\n\n\ndplyr\n\nfiltering\n\ngrouping and summarizing\n\nmutating\n\n\n\nArranging data to analyze the total number of medals and the weighted points for nations competing in rowing events in the Summer Olympic Games between 1900 and 2022.\n\n\n\n\n\nJun 5, 2025\n\n\nAbigail Smith, Robin Lock, Ivan Ramler\n\n\n\n\n\n\n\n\n\n\n\n\nOlympic Rowing Medals Between 1900 and 2022 - Summary Statistics\n\n\n\ndistribution and skewness\n\noutlier detection\n\nsummary statistics\n\nconfounding variable\n\n\n\nThe total number of medals and the weighted points for nations competing in rowing events in the Summer Olympic Games between 1900 and 2022.\n\n\n\n\n\nJun 5, 2025\n\n\nAbigail Smith, Ivan Ramler, Robin Lock\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Rowing"
    ]
  },
  {
    "objectID": "rowing/olympic_rowing_introstat/index.html",
    "href": "rowing/olympic_rowing_introstat/index.html",
    "title": "Olympic Rowing Medals Between 1900 and 2022 - Summary Statistics",
    "section": "",
    "text": "Introduction to Rowing\nIf you are unfamiliar with the sport of rowing, we encourage you to watch the following video from World Rowing\n\n\n\n\nIntroduction to Module\nThis activity looks at the total number of medals and points for nations in Olympic rowing between 1900 and 2022.\nThe Summer Olympic Games are an international athletics event held every four years and hosted in different countries around the world. Rowing was added to the Olympics in 1896 and has been in every Summer Olympics since. Rowing races in the Olympic context are typically regatta style, meaning that there are multiple boats racing head-to-head against each other in multiple lanes. Since 1912, the standard distance for Olympic regattas has been 2000m. The boat that is first to cross the finish line is awarded a gold medal, the second a silver medal, and the third a bronze. Over the course of its time as an Olympic sport there have been 25 different event entries.\nIn this dataset, the medals are counted as one medal towards each boat as opposed to each athlete in the boat. In looking at the total medals and total points for each nation, it is interesting to see which nations dominate in Olympic rowing. Additionally, looking at the overall distribution of the medals for all countries provides insight on just how lob-sided medaling can be in rowing at the Olympic level.\n\n\n\n\n\n\nNoteActivity Length\n\n\n\n\n\nThis activity could be used as an example or a short take home assessment.\n\n\n\n\n\n\n\n\n\nNoteLearning Objectives\n\n\n\n\n\nBy the end of the activity, students will be able to:\n\nAssess and interpret data distribution using histograms\nObtain summary statistics with with statistical software\nIdentify outliers with IQR\n\n\n\n\n\n\n\n\n\n\nNoteMethods\n\n\n\n\n\nStudents will use an understanding of histograms and summary statistics to assess data distribution. Students will also use the IQR method to identify outliers.\n\n\n\n\n\n\n\n\n\nNoteTechnology Requirements\n\n\n\n\n\nThe non tech version of the worksheet will only require a calculator, but the tech version will require the use of basic statistical software.\n\n\n\n\n\nData\nIn the data set there are 41 medalling nations that competed in 25 different events. Each row represents a nation and their medals and points which are cumulative from all rowing Olympics between 1900 and 2022. In total, there are 41 rows with 3 variables. In the original dataset, there were 101 nations in rowing, but the data has been adjusted to include only nations that medalled.\n\nDownload data:\n\nrowing_medals.csv\n\n\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nNOC\nNational Olympic Committee or the nation competing.\n\n\nmedals\nThe total number of medals for that country in that event.\n\n\npoints\nThe total number of points for that country in that event. The points are scaled with a gold medal counting for 3 points, a silver for 2, and a bronze for 1.\n\n\ngold\nThe total number of gold medals for that country.\n\n\nsilver\nThe total number of silver medals for that country.\n\n\nbronze\nThe total number of bronze medals for that country.\n\n\n\nData Source\nOriginal Kaggle Dataset - 120-years-of-olympic-history-athletes-and-results\n\n\n\nMaterials\nWe provide editable MS Word handouts that don‚Äôt require additional technology. We also provide editable worksheets that require the use of R (MS Word and Quarto format). Solutions are provided for all versions.\nNo Tech Required\n\nWorksheet\nWorksheet Answers\n\nTech Required\nMS Word Documents - solutions written in R, but any software will suffice.\n\nTech Worksheet - MS Word\nTech Worksheet Answers - MS Word\n\nQuarto Documents - assumes students will use R\n\nTech Worksheet - R Quarto\nTech Worksheet Answers - R Quarto\n\n\n\n\n\n\n\nNoteConclusion\n\n\n\n\n\nThis Olympic rowing medals worksheet builds students‚Äô understanding of data distribution through histograms, summary statistics, and outliers. It also strengthens students‚Äô ability to critically evaluate confounding variables and devising relationships amongst variables through looking at barplots. Additionally, it provides an interesting opportunity for students to look at patterns in medals for Olympic rowing.\n\n\n\n\n\nHow to Cite\nIf you use this module in your work, please cite it as follows:\nSmith, A., Ramler, I., & Lock, R. (2025, June 12). Olympic Rowing - Summary Statistics. ‚ÄúThe SCORE Network.‚Äù https://doi.org/10.17605/OSF.IO/6YGJV\nYou can include this citation directly in your references or bibliography.",
    "crumbs": [
      "Home",
      "Rowing",
      "Olympic Rowing Medals Between 1900 and 2022 - Summary Statistics"
    ]
  },
  {
    "objectID": "soccer/index.html",
    "href": "soccer/index.html",
    "title": "Soccer",
    "section": "",
    "text": "These modules use soccer data to teach topics in statistics and data science.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExpected Goals in Soccer\n\n\n\nLogistic Regression\n\nFeature Engineering\n\nUnder Sampling\n\n\n\nAn Introduction to Expected Goals Using Soccer\n\n\n\n\n\nMay 19, 2025\n\n\nColman Kim, Andrew Lee\n\n\n\n\n\n\n\n\n\n\n\n\nMLS - Types of Decision Errors\n\n\n\nDecision Errors\n\n\n\nExploring types of decision errors\n\n\n\n\n\nMar 30, 2025\n\n\nJonathan Lieb\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Soccer"
    ]
  },
  {
    "objectID": "tennis/Teaching_ANOVA_Through_Aces/index.html",
    "href": "tennis/Teaching_ANOVA_Through_Aces/index.html",
    "title": "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface",
    "section": "",
    "text": "An accompanying worksheet for instructors is available here\nA worksheet for students to follow is available here\nThis module is interactive and can be accessed here",
    "crumbs": [
      "Home",
      "Tennis",
      "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface"
    ]
  },
  {
    "objectID": "tennis/Teaching_ANOVA_Through_Aces/index.html#welcome",
    "href": "tennis/Teaching_ANOVA_Through_Aces/index.html#welcome",
    "title": "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface",
    "section": "",
    "text": "An accompanying worksheet for instructors is available here\nA worksheet for students to follow is available here\nThis module is interactive and can be accessed here",
    "crumbs": [
      "Home",
      "Tennis",
      "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface"
    ]
  },
  {
    "objectID": "tennis/Teaching_ANOVA_Through_Aces/index.html#authors",
    "href": "tennis/Teaching_ANOVA_Through_Aces/index.html#authors",
    "title": "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface",
    "section": "Authors",
    "text": "Authors\nZachary O. Binney, PhD MPH and Heyi Yang\nOxford College of Emory University",
    "crumbs": [
      "Home",
      "Tennis",
      "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface"
    ]
  },
  {
    "objectID": "tennis/Teaching_ANOVA_Through_Aces/index.html#how-to-cite",
    "href": "tennis/Teaching_ANOVA_Through_Aces/index.html#how-to-cite",
    "title": "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface",
    "section": "How to Cite",
    "text": "How to Cite\nIf you use this module in your work, please cite it as follows:\nBinney, Z., & Yang, H. (2025, January 22). Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface. ‚ÄúThe SCORE Network.‚Äù https://doi.org/10.17605/OSF.IO/UA9XP\nYou can include this citation directly in your references or bibliography.",
    "crumbs": [
      "Home",
      "Tennis",
      "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface"
    ]
  },
  {
    "objectID": "triathlons/index.html",
    "href": "triathlons/index.html",
    "title": "Triathlons",
    "section": "",
    "text": "These modules use triathlons data to teach topics in statistics and data science.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIronman Triathlon (Canadian Females) - Multiple Linear Regression\n\n\n\nLinear regression\n\n\n\nUsing Lake Placid Ironman triathlon results for female Canadian finishers to predict run times for participants based on both swim and bike times.\n\n\n\n\n\nFeb 5, 2024\n\n\nA.J. Dykstra, Ivan Ramler\n\n\n\n\n\n\n\n\n\n\n\n\nIronman Triathlete Performance\n\n\n\nScatterplots\n\nCorrelation\n\n\n\nGaining insight into the performance patterns of triathletes by exploring the relationships between swimming, biking, and running times.\n\n\n\n\n\nJul 23, 2023\n\n\nMichael Schuckers, Matt Abell, AJ Dykstra, Sarah Weaver, Ivan Ramler, and Robin Lock\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Triathlons"
    ]
  },
  {
    "objectID": "triathlons/ironman-triathlete-performance/index.html",
    "href": "triathlons/ironman-triathlete-performance/index.html",
    "title": "Ironman Triathlete Performance",
    "section": "",
    "text": "The motivation for this data analysis is to explore the relationships between swim times, bike times, and run times (in minutes) in order to gain insights into the performance patterns of the athletes. By analyzing these relationships, we can understand the interplay between different segments of the race and potentially identify areas of improvement for athletes. For this activity, we will specifically focus on times from finishers in the years 2018 and 2019.",
    "crumbs": [
      "Home",
      "Triathlons",
      "Ironman Triathlete Performance"
    ]
  },
  {
    "objectID": "triathlons/ironman-triathlete-performance/index.html#motivation",
    "href": "triathlons/ironman-triathlete-performance/index.html#motivation",
    "title": "Ironman Triathlete Performance",
    "section": "",
    "text": "The motivation for this data analysis is to explore the relationships between swim times, bike times, and run times (in minutes) in order to gain insights into the performance patterns of the athletes. By analyzing these relationships, we can understand the interplay between different segments of the race and potentially identify areas of improvement for athletes. For this activity, we will specifically focus on times from finishers in the years 2018 and 2019.",
    "crumbs": [
      "Home",
      "Triathlons",
      "Ironman Triathlete Performance"
    ]
  },
  {
    "objectID": "triathlons/ironman-triathlete-performance/index.html#module",
    "href": "triathlons/ironman-triathlete-performance/index.html#module",
    "title": "Ironman Triathlete Performance",
    "section": "Module",
    "text": "Module\nhttps://isle.stat.cmu.edu/SCORE/ironman_triathlon/",
    "crumbs": [
      "Home",
      "Triathlons",
      "Ironman Triathlete Performance"
    ]
  },
  {
    "objectID": "triathlons/ironman-triathlete-performance/index.html#how-to-cite",
    "href": "triathlons/ironman-triathlete-performance/index.html#how-to-cite",
    "title": "Ironman Triathlete Performance",
    "section": "How to Cite",
    "text": "How to Cite\nIf you use this module in your work, please cite it as follows:\nSchuckers, M., Abell, M., Dykstra, A., Weaver, S., Ramler, I., & Lock, R. (2024, December 3). Ironman Triathalon. ‚ÄúThe SCORE Network.‚Äù https://doi.org/10.17605/OSF.IO/VN4FA\nYou can include this citation directly in your references or bibliography.",
    "crumbs": [
      "Home",
      "Triathlons",
      "Ironman Triathlete Performance"
    ]
  }
]