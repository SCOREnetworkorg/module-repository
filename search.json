[
  {
    "objectID": "by-statsds-topic.html",
    "href": "by-statsds-topic.html",
    "title": "Modules By Topic",
    "section": "",
    "text": "2023 Boston Marathon - Variability in Finish Times\n\n\n\nhistograms\n\nsummary statistics\n\nbimodal data\n\n\n\nDescribing finish time for runners in the 2023 Boston Marathon\n\n\n\n\n\nMay 13, 2024\n\n\nIvan Ramler, Jack Fay\n\n\n\n\n\n\n\n\n\n\n\n\nExpected Goals in Soccer\n\n\n\nLogistic Regression\n\nFeature Engineering\n\nUnder Sampling\n\n\n\nAn Introduction to Expected Goals Using Soccer\n\n\n\n\n\nMay 19, 2025\n\n\nColman Kim, Andrew Lee\n\n\n\n\n\n\n\n\n\n\n\n\nFIRST Robotics Competition - Winning Chances\n\n\n\nBrier score\n\nprediction assessment\n\n\n\nEvaluating the predicted winning probabilities against the actual outcomes.\n\n\n\n\n\nMar 5, 2024\n\n\nJake Tan\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Elo ratings\n\n\n\nElo ratings\n\nBrier score\n\nprediction\n\n\n\nAn introduction to Elo ratings using NFL game outcomes.\n\n\n\n\n\nJul 9, 2024\n\n\nRon Yurko\n\n\n\n\n\n\n\n\n\n\n\n\nIronman Triathlete Performance\n\n\n\nScatterplots\n\nCorrelation\n\n\n\nGaining insight into the performance patterns of triathletes by exploring the relationships between swimming, biking, and running times.\n\n\n\n\n\nJul 23, 2023\n\n\nMichael Schuckers, Matt Abell, AJ Dykstra, Sarah Weaver, Ivan Ramler, and Robin Lock\n\n\n\n\n\n\n\n\n\n\n\n\nIronman Triathlon (Canadian Females) - Multiple Linear Regression\n\n\n\nLinear regression\n\n\n\nUsing Lake Placid Ironman triathlon results for female Canadian finishers to predict run times for participants based on both swim and bike times.\n\n\n\n\n\nFeb 5, 2024\n\n\nA.J. Dykstra, Ivan Ramler\n\n\n\n\n\n\n\n\n\n\n\n\nLacrosse Faceoff Proportions\n\n\n\nHypothesis testing\n\nSingle proportion\n\n\n\nUsing data from NCAA Div I lacrosse teams to explore the importance of winning faceoffs\n\n\n\n\n\nFeb 5, 2024\n\n\nJack Fay, Ivan Ramler, A.J. Dykstra\n\n\n\n\n\n\n\n\n\n\n\n\nLacrosse PLL vs. NLL\n\n\n\nDifference in two means\n\n\n\nComparing scoring rates between indoor and outdoor profesional lacrosse leagues.\n\n\n\n\n\nFeb 5, 2024\n\n\nJack Cowan, Ivan Ramler, A.J. Dykstra, Robin Lock\n\n\n\n\n\n\n\n\n\n\n\n\nLeague of Legends - Buffing and Nerfing\n\n\n\noutliers\n\nsummary statistics\n\n\n\nInvestigating game play statistics for League of Legends champions in two different patches.\n\n\n\n\n\nFeb 21, 2024\n\n\nIvan Ramler, George Charalambous, A.J. Dykstra\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface\n\n\n\nANOVA\n\nTennis\n\n\n\nUsing tennis to teach ANOVA and linear regression with categorical variables\n\n\n\n\n\nJan 22, 2025\n\n\nZachary O. Binney, Heyi Yang\n\n\n\n\n\n\n\n\n\n\n\n\nMLB Injuries - Introductory Time Series Analysis\n\n\n\nTime series plots\n\nTime series decomposition\n\nResidual analysis\n\nSimple forecasting\n\n\n\nExploring MLB injury data through time series analysis and forecasting.\n\n\n\n\n\nNov 26, 2024\n\n\nJonathan Lieb\n\n\n\n\n\n\n\n\n\n\n\n\nMMA Inter-rater Reliability Data Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarathon Record-Setting Over Time\n\n\n\nExponential distribution\n\nPoisson process\n\n\n\nDetermining whether the setting of world records in the marathon is historically a Poisson process.\n\n\n\n\n\nJul 23, 2023\n\n\nNicholas Clark, Rodney Sturdivant, and Kate Sanborn\n\n\n\n\n\n\n\n\n\n\n\n\nNASCAR Transformation Module\n\n\n\nLinear regression\n\nTransformations\n\nPolynomial regression\n\n\n\nUsing NASCAR driver rating data to explore a series of transformations to improve linearity in regression.\n\n\n\n\n\nFeb 5, 2024\n\n\nAlyssa Bigness, Ivan Ramler, Jack Fay\n\n\n\n\n\n\n\n\n\n\n\n\nOlympic Rowing Medals Between 1900 and 2022 - Data Wrangling\n\n\n\ndplyr\n\nfiltering\n\ngrouping and summarizing\n\nmutating\n\n\n\nArranging data to analyze the total number of medals and the weighted points for nations competing in rowing events in the Summer Olympic Games between 1900 and 2022.\n\n\n\n\n\nJun 5, 2025\n\n\nAbigail Smith, Robin Lock, Ivan Ramler\n\n\n\n\n\n\n\n\n\n\n\n\nOlympic Rowing Medals Between 1900 and 2022 - Summary Statistics\n\n\n\ndistribution and skewness\n\noutlier detection\n\nsummary statistics\n\nconfounding variable\n\n\n\nThe total number of medals and the weighted points for nations competing in rowing events in the Summer Olympic Games between 1900 and 2022.\n\n\n\n\n\nJun 5, 2025\n\n\nAbigail Smith, Ivan Ramler, Robin Lock\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting NHL Shooting Percentages\n\n\n\nlinear regression\n\n\n\nAn Introduction to Simple Linear Regression\n\n\n\n\n\nJul 23, 2023\n\n\nSam Ventura\n\n\n\n\n\n\n\n\n\n\n\n\nStolen Bases\n\n\n\nNormality tests\n\n\n\n\n\n\n\n\n\nJul 23, 2023\n\n\nAndrew Lee and Jacob Hurtubise\n\n\n\n\n\n\n\n\n\n\n\n\nUnbreakable Records in Baseball\n\n\n\nBernoulli distribution\n\nBinomial distribution\n\nChi-Square Test\n\n\n\n\n\n\n\n\n\nJul 23, 2023\n\n\nAndrew Lee and Fr Gabriel Costa\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Modules By Topic"
    ]
  },
  {
    "objectID": "triathlons/ironman-triathlete-performance/index.html",
    "href": "triathlons/ironman-triathlete-performance/index.html",
    "title": "Ironman Triathlete Performance",
    "section": "",
    "text": "The motivation for this data analysis is to explore the relationships between swim times, bike times, and run times (in minutes) in order to gain insights into the performance patterns of the athletes. By analyzing these relationships, we can understand the interplay between different segments of the race and potentially identify areas of improvement for athletes. For this activity, we will specifically focus on times from finishers in the years 2018 and 2019.",
    "crumbs": [
      "Home",
      "Triathlons",
      "Ironman Triathlete Performance"
    ]
  },
  {
    "objectID": "triathlons/ironman-triathlete-performance/index.html#motivation",
    "href": "triathlons/ironman-triathlete-performance/index.html#motivation",
    "title": "Ironman Triathlete Performance",
    "section": "",
    "text": "The motivation for this data analysis is to explore the relationships between swim times, bike times, and run times (in minutes) in order to gain insights into the performance patterns of the athletes. By analyzing these relationships, we can understand the interplay between different segments of the race and potentially identify areas of improvement for athletes. For this activity, we will specifically focus on times from finishers in the years 2018 and 2019.",
    "crumbs": [
      "Home",
      "Triathlons",
      "Ironman Triathlete Performance"
    ]
  },
  {
    "objectID": "triathlons/ironman-triathlete-performance/index.html#module",
    "href": "triathlons/ironman-triathlete-performance/index.html#module",
    "title": "Ironman Triathlete Performance",
    "section": "Module",
    "text": "Module\nhttps://isle.stat.cmu.edu/SCORE/ironman_triathlon/",
    "crumbs": [
      "Home",
      "Triathlons",
      "Ironman Triathlete Performance"
    ]
  },
  {
    "objectID": "triathlons/ironman-triathlete-performance/index.html#how-to-cite",
    "href": "triathlons/ironman-triathlete-performance/index.html#how-to-cite",
    "title": "Ironman Triathlete Performance",
    "section": "How to Cite",
    "text": "How to Cite\nIf you use this module in your work, please cite it as follows:\nSchuckers, M., Abell, M., Dykstra, A., Weaver, S., Ramler, I., & Lock, R. (2024, December 3). Ironman Triathalon. “The SCORE Network.” https://doi.org/10.17605/OSF.IO/VN4FA\nYou can include this citation directly in your references or bibliography.",
    "crumbs": [
      "Home",
      "Triathlons",
      "Ironman Triathlete Performance"
    ]
  },
  {
    "objectID": "tennis/Teaching_ANOVA_Through_Aces/index.html",
    "href": "tennis/Teaching_ANOVA_Through_Aces/index.html",
    "title": "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface",
    "section": "",
    "text": "An accompanying worksheet for instructors is available here\nA worksheet for students to follow is available here\nThis module is interactive and can be accessed here",
    "crumbs": [
      "Home",
      "Tennis",
      "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface"
    ]
  },
  {
    "objectID": "tennis/Teaching_ANOVA_Through_Aces/index.html#welcome",
    "href": "tennis/Teaching_ANOVA_Through_Aces/index.html#welcome",
    "title": "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface",
    "section": "",
    "text": "An accompanying worksheet for instructors is available here\nA worksheet for students to follow is available here\nThis module is interactive and can be accessed here",
    "crumbs": [
      "Home",
      "Tennis",
      "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface"
    ]
  },
  {
    "objectID": "tennis/Teaching_ANOVA_Through_Aces/index.html#authors",
    "href": "tennis/Teaching_ANOVA_Through_Aces/index.html#authors",
    "title": "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface",
    "section": "Authors",
    "text": "Authors\nZachary O. Binney, PhD MPH and Heyi Yang\nOxford College of Emory University",
    "crumbs": [
      "Home",
      "Tennis",
      "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface"
    ]
  },
  {
    "objectID": "tennis/Teaching_ANOVA_Through_Aces/index.html#how-to-cite",
    "href": "tennis/Teaching_ANOVA_Through_Aces/index.html#how-to-cite",
    "title": "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface",
    "section": "How to Cite",
    "text": "How to Cite\nIf you use this module in your work, please cite it as follows:\nBinney, Z., & Yang, H. (2025, January 22). Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface. “The SCORE Network.” https://doi.org/10.17605/OSF.IO/UA9XP\nYou can include this citation directly in your references or bibliography.",
    "crumbs": [
      "Home",
      "Tennis",
      "Linear Regression with Categorical Variables and ANOVA: Ace Rates in Tennis by Surface"
    ]
  },
  {
    "objectID": "rowing/olympic_rowing_introstat/index.html",
    "href": "rowing/olympic_rowing_introstat/index.html",
    "title": "Olympic Rowing Medals Between 1900 and 2022 - Summary Statistics",
    "section": "",
    "text": "Introduction to Rowing\nIf you are unfamiliar with the sport of rowing, we encourage you to watch the following video from World Rowing\n\n\n\n\nIntroduction to Module\nThis activity looks at the total number of medals and points for nations in Olympic rowing between 1900 and 2022.\nThe Summer Olympic Games are an international athletics event held every four years and hosted in different countries around the world. Rowing was added to the Olympics in 1896 and has been in every Summer Olympics since. Rowing races in the Olympic context are typically regatta style, meaning that there are multiple boats racing head-to-head against each other in multiple lanes. Since 1912, the standard distance for Olympic regattas has been 2000m. The boat that is first to cross the finish line is awarded a gold medal, the second a silver medal, and the third a bronze. Over the course of its time as an Olympic sport there have been 25 different event entries.\nIn this dataset, the medals are counted as one medal towards each boat as opposed to each athlete in the boat. In looking at the total medals and total points for each nation, it is interesting to see which nations dominate in Olympic rowing. Additionally, looking at the overall distribution of the medals for all countries provides insight on just how lob-sided medaling can be in rowing at the Olympic level.\n\n\n\n\n\n\nActivity Length\n\n\n\n\n\nThis activity could be used as an example or a short take home assessment.\n\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nBy the end of the activity, students will be able to:\n\nAssess and interpret data distribution using histograms\nObtain summary statistics with with statistical software\nIdentify outliers with IQR\n\n\n\n\n\n\n\n\n\n\nMethods\n\n\n\n\n\nStudents will use an understanding of histograms and summary statistics to assess data distribution. Students will also use the IQR method to identify outliers.\n\n\n\n\n\n\n\n\n\nTechnology Requirements\n\n\n\n\n\nThe non tech version of the worksheet will only require a calculator, but the tech version will require the use of basic statistical software.\n\n\n\n\n\nData\nIn the data set there are 41 medalling nations that competed in 25 different events. Each row represents a nation and their medals and points which are cumulative from all rowing Olympics between 1900 and 2022. In total, there are 41 rows with 3 variables. In the original dataset, there were 101 nations in rowing, but the data has been adjusted to include only nations that medalled.\n\nDownload data:\n\nrowing_medals.csv\n\n\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nNOC\nNational Olympic Committee or the nation competing.\n\n\nmedals\nThe total number of medals for that country in that event.\n\n\npoints\nThe total number of points for that country in that event. The points are scaled with a gold medal counting for 3 points, a silver for 2, and a bronze for 1.\n\n\ngold\nThe total number of gold medals for that country.\n\n\nsilver\nThe total number of silver medals for that country.\n\n\nbronze\nThe total number of bronze medals for that country.\n\n\n\nData Source\nOriginal Kaggle Dataset - 120-years-of-olympic-history-athletes-and-results\n\n\n\nMaterials\nWe provide editable MS Word handouts that don’t require additional technology. We also provide editable worksheets that require the use of R (MS Word and Quarto format). Solutions are provided for all versions.\nNo Tech Required\n\nWorksheet\nWorksheet Answers\n\nTech Required\nMS Word Documents - solutions written in R, but any software will suffice.\n\nTech Worksheet - MS Word\nTech Worksheet Answers - MS Word\n\nQuarto Documents - assumes students will use R\n\nTech Worksheet - R Quarto\nTech Worksheet Answers - R Quarto\n\n\n\n\n\n\n\nConclusion\n\n\n\n\n\nThis Olympic rowing medals worksheet builds students’ understanding of data distribution through histograms, summary statistics, and outliers. It also strengthens students’ ability to critically evaluate confounding variables and devising relationships amongst variables through looking at barplots. Additionally, it provides an interesting opportunity for students to look at patterns in medals for Olympic rowing.\n\n\n\n\n\nHow to Cite\nIf you use this module in your work, please cite it as follows:\nSmith, A., Ramler, I., & Lock, R. (2025, June 12). Olympic Rowing - Summary Statistics. “The SCORE Network.” https://doi.org/10.17605/OSF.IO/6YGJV\nYou can include this citation directly in your references or bibliography.",
    "crumbs": [
      "Home",
      "Rowing",
      "Olympic Rowing Medals Between 1900 and 2022 - Summary Statistics"
    ]
  },
  {
    "objectID": "robotics/FIRST_Robotics_Competition/index.html",
    "href": "robotics/FIRST_Robotics_Competition/index.html",
    "title": "FIRST Robotics Competition - Winning Chances",
    "section": "",
    "text": "Introduction\nThe FIRST Robotics Competition (FRC) is a high school level robotics competition, in which “[u]nder strict rules and limited time and resources, teams of high school students are challenged to build industrial-size robots to play a difficult field game in alliance with other teams.” It combines “the excitement of sport with the rigors of science and technology”.\nOne of the key features of FRC is that robot/team competes not individually, but in alliance with other teams. So, it is important for teams to “scout” other teams as potential alliance partners. Various methodologies/models to evaluate each team’s potential contribution were developed. One of the popular models is called Expected Points Added (EPA) model.\nDetailed algorithm of the EPA model can be found at here (https://www.statbotics.io/blog/epa). Briefly, the EPA model builds upon the Elo rating system which is a “well-known method for ranking chess players, and has been adapted to many other domains.” It produces predicted probabilities of winning for the alliance based on the past performances of each team in the alliance, as well as teams in the opposition alliance. As such, there is a desire/need to assess how good the EPA model prediction is.\nBrier score originated with weather forecast research. It was designed to evaluate the predicted probabilities against the actual outcomes and is straight forward to calculate. While it is not widely used outside specific use cases, it is one of many approaches for the important step of evaluating models based on their predictions. Since the EPA model provides the predicted winning probabilities, Brier score is useful for evaluating its performance by comparing its predicted winning probabilities to the actual FRC outcomes.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nBy the end of this activity, you will be able to:\n\nCalculate Brier score.\nInterpret Brier score.\n\n\n\n\n\n\nData\nIn this lesson, we will use the EPA data and competition outcomes calculated and compiled by the website statbotics.io.\n\ndtAll &lt;- read.csv(\"matches.csv\")\ndtUse &lt;- subset(dtAll, status==\"Completed\" & offseason==\"f\", \n                select=c(year, event, playoff, comp_level, winner, epa_win_prob))\n\nBelow is a description of the variables:\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nyear\nthe year/season of the FRC event\n\n\nevent\nunique identifier for each FRC event\n\n\nplayoff\n“t” for playoff match; “f” for qualifying match\n\n\ncomp_level\n“qm” for qualifying match; “sf” for semifinals match; “f” for finals match\n\n\nwinner\nwinning alliance (“red” or “blue”) of the match\n\n\nepa_win_prob\npredicted winning probability for the Red Alliance by EPA model\n\n\n\n\nThe data covers the competition seasons from 2002 to 2023, except for 2021 due to the COVID pandemic.\n\nunique(dtUse$year)\n\n [1] 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016\n[16] 2017 2018 2019 2020 2022 2023\n\n\nFor our example, we will use a particular event, the Hopper Division competition at the 2023 FRC World Championship in Houston, to illustrate the calculation of Brier score.\n\ndt2023hop &lt;- subset(dtUse, event==\"2023hop\")\n\nBelow is what the raw data looks like. Each row is a match between a Red alliance and a Blue alliance. Each alliance consists of three robots/teams.\n\n\n\n\n\n\n\n\nBrier Score\nFor match \\(i\\), let \\(f_i\\) denote the probability forecast. In our case, it is the predicted winning probability for the Red Alliance by EPA model, i.e., the variable epa_win_prob. Let \\(o_i\\) denote the match outcome: \\(o_i=1\\) when the Red alliance won and \\(o_i=0\\) when the Blue alliance won. The Brier score for match \\(i\\) is calculated as \\((f_i - o_i)^2\\). For example, suppose it is predicted that the Red alliance will win with 80% probability, i.e., \\(f_i=0.8\\), if the actual outcome is that the Red alliance won, the Brier score is \\((0.8-1)^2=0.04\\). If the actual outcome is that the Blue alliance won, the Brier score is \\((0.8-0)^2=0.64\\).\nBrier score is a quantity bounded by \\(0\\) and \\(1\\). Brier score of \\(0\\) means correctly predicting the outcome with 100% certainty. 50:50 random guess would give a Brier score of \\(0.25\\). The overall Brier score for all the matches during a competition event or season is simply the average of individual match scores: \\[\\frac{1}{N} \\sum_{i=1}^N (f_i - o_i)^2\\] The following table shows the calculation for each match.\n\n\n\n\n\n\nThe overall Brier score for the 2023 Hopper Division event is 0.1366352, which is slightly worse than the half way between perfect \\(0\\) and random guess \\(0.25\\).\n\n\nYour Turn\nNow, it’s your turn. Please use the data from the Turing Division competition at the 2022 FRC World Championship to calculate the average Brier score for the event. You should find the data in the file dt2022tur.csv.\n\ndt2022tur &lt;- subset(dtUse, event==\"2022tur\")\nwrite.csv(dt2022tur, \"dt2022tur.csv\")\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe average Brier score for the 2022 Turing Division event is 0.1160236, which is better than the Brier score for the 2023 Hopper Division event.\n\n\n\n\n\nOver the Years\nSince we have the data for more than 20 years, we could answer an interesting question: did the predictive ability of the EPA model change over the years?\nWe build two simple functions to do the calculations.\n\n\nTwo Functions\n\nThe first function calculates the Brier score for a given data set. Occasionally, a game can end in a draw. We assign the value of \\(0.5\\) to \\(o_i\\) for a draw.\n\ncalcBS &lt;- function(dt){\n  n &lt;- nrow(dt)\n  outcome &lt;- rep(NA, n)\n  outcome[dt$winner==\"red\"] &lt;- 1\n  outcome[dt$winner==\"draw\"] &lt;- 0.5\n  outcome[dt$winner==\"blue\"] &lt;- 0\n  diff &lt;- dt$epa_win_prob - outcome\n  Brier &lt;- mean(diff^2)\n  c(n=n, Brier=Brier)\n}\n\nThe second function separates the data by year and does the calculation for each year.\n\nbyYear &lt;- function(dt=dtUse) {\n  yrs &lt;- unique(dt$year)\n  m &lt;- length(yrs)\n  size &lt;- Brier &lt;- rep(NA, m)\n  for (i in 1:m) {\n    dat &lt;- subset(dt, year==yrs[i])\n    res &lt;- calcBS(dt=dat)\n    size[i] &lt;- res[1]\n    Brier[i] &lt;- res[2]\n  }\n  data.frame(year=yrs, n=size, Brier=Brier)\n}\nFRC &lt;- byYear()\n\n\nBelow are the Brier scores from 2002 to 2023.\n\nknitr::kable(FRC)\n\n\n\n\nyear\nn\nBrier\n\n\n\n\n2002\n2197\n0.2351889\n\n\n2003\n3173\n0.2225493\n\n\n2004\n3198\n0.2069319\n\n\n2005\n2059\n0.2043631\n\n\n2006\n3283\n0.1997467\n\n\n2007\n3563\n0.2099309\n\n\n2008\n4036\n0.1892942\n\n\n2009\n4567\n0.1961946\n\n\n2010\n5564\n0.1691369\n\n\n2011\n6224\n0.1621286\n\n\n2012\n7707\n0.1841302\n\n\n2013\n8242\n0.1704309\n\n\n2014\n10663\n0.1906669\n\n\n2015\n11810\n0.1841460\n\n\n2016\n13286\n0.1794790\n\n\n2017\n15429\n0.2043697\n\n\n2018\n16930\n0.1750251\n\n\n2019\n18022\n0.1758972\n\n\n2020\n4634\n0.1817734\n\n\n2022\n14645\n0.1480655\n\n\n2023\n16319\n0.1604984\n\n\n\n\n\nIt is interesting to note that the predictive ability of the EPA model has improved for the past 20 years. Since the model has not changed, I believe the improvement comes from established teams becoming more consistent and predictable. Meanwhile, the pool of newer, less experienced teams has stayed healthy.\n\nplot(FRC$year, FRC$Brier)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusion\n\n\n\n\n\nIn conclusion, Brier score is a simple statistic that assesses the probability prediction against actual outcome. A smaller Brier score corresponds to better prediction.\nThe EPA model has been getting better at predicting FRC match outcome.\n\n\n\n\n\nAuthor\nCreated by Jake Tan (Wissahickon High School). Jake is a subsystem leader at FRC Team 341, Miss Daisy. Team 341 competed at FRC World Championship in the Turing Division in 2022 and Hopper Division in 2023.\n\n\nHow to Cite\nIf you use this module in your work, please cite it as follows:\nTan, J. (2025, January 15). FIRST Robotics Module. “The SCORE Network,” https://doi.org/10.17605/OSF.IO/BRG8Z\nYou can include this citation directly in your references or bibliography.",
    "crumbs": [
      "Home",
      "Robotics",
      "FIRST Robotics Competition - Winning Chances"
    ]
  },
  {
    "objectID": "mixed_martial_arts/mma_interrater_reliability/index.html#learning-objectives",
    "href": "mixed_martial_arts/mma_interrater_reliability/index.html#learning-objectives",
    "title": "MMA Inter-rater Reliability Data Analysis",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this module you will be able to:\n\nunderstand the basic concepts of inter-rater reliability.\nunderstand the purposes of inter-rater reliability\nunderstand and interpret measures of reliability:\n\nPercent agreement\nCohen’s Kappa\nWeighted Kappa\nFleiss’ Kappa\n\napply reliability measures to judging of MMA fight data",
    "crumbs": [
      "Home",
      "Mixed Martial Arts",
      "MMA Inter-rater Reliability Data Analysis"
    ]
  },
  {
    "objectID": "mixed_martial_arts/mma_interrater_reliability/index.html#percent-agreement",
    "href": "mixed_martial_arts/mma_interrater_reliability/index.html#percent-agreement",
    "title": "MMA Inter-rater Reliability Data Analysis",
    "section": "Percent Agreement",
    "text": "Percent Agreement\nWe’ll begin by evaluating the inter-rater-reliability between two judges. One simple way is to calculate the percentage of fights in which they agree on the outcome. This is appropriately termed percent agreement. This reduces each fight into a simple outcome: agree or disagree, so we lose any information on the scores or margin of the fight (we will return to the original scores later.) Another limitation of percent agreement is that it ignores the possibility of judges arriving at agreement through chance. Still, it provides a foundation for the widely used Cohen’s kappa. There will be more on this later, but for now, let’s calculate the percent agreement for some judges.\nBelow, we see a table of the ten most frequently-appearing judges. The table includes the number of fights that they judged. D’Amato, Lee, and Cleary are the most experienced judges in our data set.\n\n\njudgefightsD'Amato846Lee534Cleary532Cartlidge413Weeks384Bell383Colón372Crosby368Rosales321Lethaby305\n\n\n\nExample: D’Amato and Lee\nLet’s take the top two judges: D’Amato and Lee, and compare their rulings on fights where they both judged the same fight. We’ll look specifically at the outcome categorical variable: judge_out.\nHere is a contingency table that summarizes all the fights that D’Amato and Lee judged together. Contingency tables are used to assess the association between two paired categorical variables. They tabulate the distribution of each variable and compare their results.\n\n\nLeeD'Amatofighter1drawfighter2totalfighter16201274draw4206fighter21005262total76264142\n\n\nThis table helps us to organize and compare D’Amato’s and Lee’s ratings. Each row consists of D’Amato’s votes and each column consists of Lee’s corresponding votes. Thus, the table forms a downward sloping diagonal where the judges agree. These are called concordant responses.\nThey both selected fighter 1 as the victor 62 times, a draw twice, and fighter 2 as victor 52 times. However, they selected opposing fighters 22 times (10 + 12), and D’Amato voted for a draw four times where Lee disagreed.\nOur total concordant values is 62 + 2 + 52 = 116. We can calculate the simple percent agreement by adding up all the concordant values, and dividing it by the total number of results. Another word for this is the proportion of observed agreement (\\(p_{o}\\)). This term will be important later.\n\n\nAgreeDisagreePercent Agreement1162681.69%\n\n\nD’Amato and Lee’s simple agreement (proportion of observed agreement) is only 81.69%. This means they disagreed on almost 1/5 of their rulings.\n\n\nExercise 1: Percent Agreement for D’Amato and Cleary\nLet’s compare D’Amato and Lee’s consistency with that of D’Amato and Cleary. Below is the contingency table of D’Amato and Cleary.\n\n\nClearyD'Amatofighter1drawfighter2totalfighter1650469draw1315fighter2707178total73376152\n\n\n1.1. Can you identify D’Amato and Cleary’s concordant responses?\n\n 139 152 65 71 73\n\n1.2. Use the concordant responses to calculate D’Amato and Cleary’s percent agreement yourself. Remember, the formula for percent agreement \\((p_{o})\\) = concordant values/total.\n\n 48% 46.7% 91.4% 89%\n\n\n\nAgreeDisagreePercent Agreement\n\n\n1.3. We’ve left the table above empty for you. Fill it in with the appropriate values (Agree/Disagree/Percent Agreement).\n\n 139/13/91.4% 65/8/89% 73/79/48% 139/152/91.4%\n\n1.4. How do the results compare with D’Amato and Lee?\n\n D'Amato and Lee agree the same amount D'Amato and Lee tend to agree less often D'Amato and Lee tend to agree more often\n\n\n\nExercise 2: Cartlidge and Lethaby\nWe next consider an example using the top judge combination Cartlidge and Lethaby.\n\n\nLethabyCartlidgefighter1drawfighter2totalfighter188016104draw2136fighter27096103total971115213\n\n\n2.1. Assess the table. How well do the judges tend to agree?\n\n Reasonable agreement (less than 30 times disagree) They agree very rarely (&lt;50%, 88 times) Excellent, they only disagree 1 time\n\n\n\nAgreeDisagreePercent Agreement\n\n\n2.2. Once again, we’ve left an empty percent agreement table for you (above). Fill in the cells with the appropriate values and calculate the percent agreement.\n\n 186/28/86.8% 115/98/54% 186/213/86.8% 88/16/84.6%\n\n2.3. How do Cartlidge and Lethaby’s results compare to the other judges we assessed?\n\n They have slightly better agreement than D'Amato and Lee but not as strong as D'Amato and Cleary They have agreement about as strong as D'Amato and Cleary They have much worse agreement than either of the previous examples. They have much better agreement than either of the previous examples.\n\n\n\nExercise 3 - Advanced (Optional): Other Judges\nUse R code (or the package you use for data analysis) to select any two judges to compare from the list below.\n\n\njudge1judge2fights_judgedCartlidgeLethaby213ClearyD'Amato152D'AmatoLee142CollettLethaby116CartlidgeCollett113ClearyLee98ColónTirelli90BellMccarthy78D'AmatoKamijo75D'AmatoWeeks74CrosbyD'Amato70BellD'Amato69BellCleary67KamijoWeeks64ColónD'Amato63\n\n\n\n\nLimitations of Percent Agreement\nPercent agreement is helpful, because it gives us a general understanding of the judges’ reliability, but it is limited. In particular, it cannot account for the judges’ arriving at similar conclusions via chance.\nSo, how likely is it for judges to arrive at similar conclusions via chance even if they do not necessarily judge consistently?\nLet’s look at a simple simulation. Here, we have two hypothetical judges - we’ll call them Jimmy and Mateo - rating 1000 fights. Except, instead of watching and analyzing the fights before carefully determining a winner, both Jimmy and Mateo slept through all 1000 fights. Luckily for them, they remembered the historical voting trends of MMA judges. Both of them, independently, decided to randomly select a winner for each of the 1000 fights in a way that was consistent with the likelihoods of the historical rulings.\nThe historical rulings are below. In the 1000 fights, they selected fighter 1 and fighter 2 about 49% of the time and a draw about 2% of the time.\n\n\n\n\n\n\n\n\n\nThey made these ratings without consulting each other or watching the fights. Below are the first 15 observations of the data set we created.\n\n\nfightJimmyMateo1fighter1fighter22fighter2fighter23fighter2fighter24fighter2fighter15fighter1fighter26fighter1fighter17fighter1fighter18fighter1fighter19fighter1fighter110fighter2fighter211fighter2fighter112fighter2fighter213fighter1fighter214fighter2fighter215fighter2fighter1\n\n\nAnd here is their contingency table.\n\n\nMateoJimmyfighter1drawfighter2totalfighter124112248501draw901322fighter22246247477total474185081,000\n\n\nTheir simple agreement numbers come out like this:\n\n\nAgreeDisagreePercent Agreement48851248.80%\n\n\nWoah! Jimmy and Mateo agreed 48.80% of the time. This certainly is not a good rate of agreement, but it does suggest caution in interpreting percent agreement rates of our real judges D’Amato, Lee, and Cleary. Their percent agreements fell in the 80-90% range, but we can get over half that agreement with just random chance.\nWe next consider metrics that account for the fact that some agreement is likely just due to chance.",
    "crumbs": [
      "Home",
      "Mixed Martial Arts",
      "MMA Inter-rater Reliability Data Analysis"
    ]
  },
  {
    "objectID": "mixed_martial_arts/mma_interrater_reliability/index.html#cohens-kappa",
    "href": "mixed_martial_arts/mma_interrater_reliability/index.html#cohens-kappa",
    "title": "MMA Inter-rater Reliability Data Analysis",
    "section": "Cohen’s Kappa",
    "text": "Cohen’s Kappa\nCohen’s kappa is a second, more rigorous method, that assesses the agreement between two judges. Like percent agreement, it measures the reproducibility of repeated assessments of the same event. It was developed by Jacob Cohen in the 1960s as an alternative agreement method that accounts for the possibility of chance agreement.\nCohen’s kappa makes a few assumptions about the data:\n\nThe same two individuals must rate each event.\nThe principle of independence. The judges rate the same events without consultation or communication. This means the judges’ results are paired.\nThe judgments are made between the same defined categories. In our context, the judges categorize the fight result as win/lose/draw for each fighter.\n\nAll three of these assumptions are met by our data. We will filter our data to ensure the same two judges score each event. Judges in MMA fights are kept in separate areas around the fight. All our judges vote for fighter 1, fighter 2, or a draw.\nLike percent agreement, Cohen’s kappa works with any categorical variable.\nCohen’s kappa isolates the judges’ real agreement from their chance agreement. It produces a correlation coefficient kappa (\\(\\kappa\\)) that assesses the agreement between the two judges and ranges from -1 to 1.\n\nAt \\(\\kappa\\) = -1, the two judges produced exactly opposite assessments of the event.\nAt \\(\\kappa\\) = 0, the agreement between the two judges is tantamount to an agreement entirely produced by chance.\nAt \\(\\kappa\\) = 1, the two judges have perfect agreement. Their assessments of the events are identical.\n\n\nExample: D’Amato and Lee\nAs we walk through the methodology of Cohen’s kappa, let’s revisit our example of Lee and D’Amato.\nAgain, we begin with a contingency table.\nLeeD'Amatofighter1drawfighter2totalfighter16201274draw4206fighter21005262total76264142\nEarlier, we found the proportion of observed agreement for this table is 81.69%. If we’re going to account for chance, we need to estimate what the agreement rate would be if the results were completely randomized.\nWe can estimate these random results by producing theoretical estimates. This is called the expected value. We calculate the expected value of each cell by multiplying together three values. The first judge’s probability of producing a result, the second judge’s probability of producing the corresponding result independent of the first judge, and the total number of fights.\nFor example, to find the expected value in the draw-draw concordant cell. We can multiply D’Amato’s draw rate of \\(\\frac{6}{142}\\) by Lee’s draw rate of \\(\\frac{2}{142}\\) and by the total number of fights: 142. We end up with 0.085.\nIn other words, If D’Amato and Lee were to judge a new set of 142 fights and randomly pick their results from a hat containing their historical results together, we’d expect them to both pick a draw 0.085 times.\nWe created a table full of the expected values.\n\n\nLeeD'Amatofighter1drawfighter2totalfighter139.611.0433.3574draw3.210.082.706fighter233.180.8727.9462total76.002.0064.00142\n\n\nWith the table, we sum up the three concordant cells: 67.63, and divide by the total number of fights: 142. This gives us the proportion of expected agreement (\\(p_{e}\\)). A value of 47.63%.\nNow, with both the proportion of observed agreement and the proportion of expected agreement, we can calculate kappa using the formula:\n\n\\(kappa(\\kappa) = \\displaystyle \\frac{p_{o} - p_{e}}{1 - p_{e}}\\)\n\nWhen we plug in those values and solve for kappa, we find that the kappa between D’Amato and Lee is 0.65.\nWe’re past all the calculations and math, but what does our kappa mean?\n\n\nInterpreting Kappa\nThe kappa value represents the percentage of the two judges’ results that agree with one another over and above what we would expect from chance. Conversely, the complement of kappa represents the percentage of the two judge’s results that result from chance or straight-up disagreement.\nThus, the magnitude of the agreement is important. A higher kappa is always better, because it suggests a higher reproducibility in the measurement system. Unlike some statistical tests, the kappa statistic is not evaluated by passing a threshold. Instead, the exact assessment of a kappa often depends on a myriad of factors.\nThis contextual nature of kappa makes interpretation difficult. There is a lot of disagreement over the interpretations for different kappa values, and the guidelines typically vary with the field of study. For example, health related studies demand a stronger reliability than fields that have less widespread influence over the population’s well-being like, say, judging MMA fights.\nBelow is one evaluation method, that has been generally agreed upon by several prominent statisticians:\n\n\\(\\kappa &gt; 0.75\\) Excellent reproducibility\n\\(0.4 \\le \\kappa \\le 0.75\\) Good reproducibility\n\\(0 \\le \\kappa &lt; 0.4\\) Marginal reproducibility\n\nNote that a negative kappa value is possible, and would represent agreement that is worse than that expected by chance. Clearly such a value would indicate very poor reproducibility.\nApplying this method to our judges, D’Amato and Lee’s kappa of 0.65 indidcates the judges have “good” reproducibility in their ratings. They have decent consistency and interchangeability. We should be careful, because an estimated 35% of their relationship is comprised of chance agreement or disagreement. However, we cannot speak to D’Amato and Lee’s accuracy or validity in their ratings. We cannot assess if their judgments were correct.\n\nAdvanced (optional): Confidence intervals and tests for Kappa\n\n\nWe can produce a confidence interval and hypothesis test for our kappa.\nWith a large enough sample size, kappa is normally distributed with a standard error (se).\n\n\\(se(\\kappa) = \\sqrt{\\displaystyle \\frac{1}{n(1 - p_{e})^{n}} * [p_{e} + p_{e}^{2} - \\sum_{i=1}^{c}{(a_{i}b_{i}(a_{i} + b_{i}))}]}\\)\n\nUsing this standard error, we can calculate the 95% confidence interval by:\n\n\\(\\kappa = \\pm 1.96 * se(\\kappa)\\)\n\nA 95% confidence interval produces an estimated range for the true value of kappa. We can say with 95% confidence that the interval includes the true kappa. Like all confidence intervals, a larger sample size reduces this interval.\nThe confidence interval is important, because it helps us to see how much we can trust our kappa. A high kappa statistic that has a large confidence interval is far from ideal.\nOur 95% confidence interval for the kappa of D’Amato and Lee is 0.529 to 0.772. Thus, we can say with 95% confidence that the interval (0.529, 0.772) includes the true value of kappa. In other words, we would not be surprised if the true kappa is as low as 0.529.\nWe can also create a hypothesis test for our kappa. We’re looking to test that there is at least some non-random association between the judges.\nOur kappa test has a null and alternative hypothesis of:\n\n\\(H_{o}: \\kappa = 0\\)\n\\(H_{a}: \\kappa &gt; 0\\)\n\nWe will hold to our null hypothesis unless we have significant evidence to reject it. This evidence is held in a p-value. If our p-value is less than our \\(\\alpha\\) of 0.05, then we have sufficient evidence to reject our null hypothesis and agree with our alternative. Moreover, if our confidence interval does not include 0 within its range, then we can reject the null hypothesis without checking for the p-value.\nThe hypothesis test can be misleading, however, because a small kappa value can reject the null hypothesis despite indicating only poor agreement. For this reason, confidence intervals are preferable.\nOur p-value for D’Amato and Lee is 2.22^{-16}. We can thoroughly reject the null hypothesis that there is no association in the decisions of D’Amato and Lee.\n\n\n\nExercise 4: D’Amato and Cleary\nNow that we’ve analyzed D’Amato and Lee. We’d like to give you the opportunity to describe the agreement between D’Amato and Cleary. We’ll produce the results and you produce the analysis.\n\n\nClearyD'Amatofighter1drawfighter2totalfighter1650469draw1315fighter2707178total73376152\n\n\nClearyD'Amatofighter1drawfighter2totalfighter133.141.3634.569draw2.400.102.55fighter237.461.5439.078total73.003.0076.0152\n\n\n4.1. Recall our percent agreement assessment from earlier. Do D’Amato and Cleary tend to agree?\n\n Very good agreement (less than 15 times disagree) Perfect agreement (0 values in the draw column) They agree very rarely (&lt;50%, 71 times)\n\n4.2. Compare the two tables. Do the expected values for the cells surprise you?\n\n Yes, it shows too much disagreement No. Expected values are based on random selections so we would expect about 50 percent agreement\n\n4.3. After some calculations, we find that D’Amato and Cleary’s kappa is 0.84. Using the guidelines demonstrated above, interpret the value.\n\n This value suggests excellent reproducibility This value suggests marginal reproducibility This value suggests good reproducibility\n\n4.4. (Optional - Advanced) We can run the confidence interval and hypothesis test through our program:\n\n\n\n    Estimate Cohen's kappa statistics and test the null hypothesis that the\n    extent of agreement is same as random (kappa=0)\n\ndata:  .\nZ = 10.844, p-value &lt; 2.2e-16\n95 percent confidence interval:\n 0.7522942 0.9217408\nsample estimates:\n[1] 0.8370175\n\n\nAnalyze the results. Produce explanations of the confidence interval and hypothesis test, and provide your own assessment of the association between D’Amato and Cleary. Try to use the wording and phrases that we explained earlier.\n\n\nExercise 5: Cleary and Lee\nNext, we have selected to analyze Cleary and Lee. Note: the lower the sample size, the wider our confidence interval and the less we can trust our kappa value.\n5.1. How does the sample size between these two judges likely impact the kappa estimate?\n\n The sample size is larger than previous examples so we will have LESS confidence in the kappa estimate The sample size is smaller than previous examples so we will have MORE confidence in the kappa estimate The sample size is larger than previous examples so we will have MORE confidence in the kappa estimate The sample size is smaller than previous examples so we will have LESS confidence in the kappa estimate\n\n\n\nLeeClearyfighter1drawfighter2totalfighter14411156draw2002fighter2703340total5314498\n\n\nLeeClearyfighter1drawfighter2totalfighter130.290.5725.1456draw1.080.020.902fighter221.630.4117.9640total53.001.0044.0098\n\n\n5.2. Assess the two tables. Do the judges appear to agree?\n\n The agree almost perfectly They almost never They do disagree a fair amount relative to the sample size, but still near 80 percent\n\n5.3. Now, compare the two tables. Do the expected values for the cells surprise you? How similar are they to the observed values?\n\n Very surprising. Expected agreement is as high as the observed. Expected agreement is roughly 50 percent so not surprising.\n\n5.4. After solving for kappa, our value is 0.21. Using the guidelines demonstrated previously, interpret the value. How does it compare to previous judge-pairings kappas?\n\n This value suggests excellent reproducibility similar to previous examples This value suggests good reproducibility, somewhat lower than previous examples This value suggests marginal reproducibility, much worse than previous examples\n\n\n\n\n\n    Estimate Cohen's kappa statistics and test the null hypothesis that the\n    extent of agreement is same as random (kappa=0)\n\ndata:  .\nZ = 4.1376, p-value = 1.755e-05\n95 percent confidence interval:\n 0.08973446 0.32838582\nsample estimates:\n[1] 0.2090601\n\n\n5.5. (Optional advanced) Interpret the confidence interval and p-value. What does they mean for the relationship between the two judges?\n\n The small sample leads to a wide interval so the reproducibility has high probability of still being excellent The interval provides strong evidence of marginal reproducibility",
    "crumbs": [
      "Home",
      "Mixed Martial Arts",
      "MMA Inter-rater Reliability Data Analysis"
    ]
  },
  {
    "objectID": "mixed_martial_arts/mma_interrater_reliability/index.html#weighted-kappa",
    "href": "mixed_martial_arts/mma_interrater_reliability/index.html#weighted-kappa",
    "title": "MMA Inter-rater Reliability Data Analysis",
    "section": "Weighted Kappa",
    "text": "Weighted Kappa\nGreat. We’ve analyzed our data and produced a kappa value that assesses the true agreement between judges while accounting for random chance.\nStill, we are losing some information. Our judges supply score cards with point values for each fighter. They don’t just assign a winner. When we reduce each judge’s ruling to win, lose, or draw, we miss out on the degree of these victories. Instead of looking at the outcome variable, let’s analyze the margin variable.\nIn this case, the margin variable is an ordinal variable. Ordinal variables are a type of categorical variable that has a similar function to nominal variables, except that there is a clear ordering in the results. Height, for example, can be divided into ordinal categories like “very tall”, “tall”, “normal”, “short”, and “very short”.\nThis clear ordering of the categories allows for partial agreement. Partial agreement affords some credit to close responses. The judge’s responses may not be identical, but they could be close. Short is a lot closer to very short than very tall. Partial agreement takes this into account.\nWeighted kappa is a variant of Cohen’s kappa (also created by Jacob Cohen) that permits this partial agreement between responses. Like Cohen’s kappa, it accounts for any chance agreement, but it also takes into account the proximity of the judges’ results. A large disparity in the two judge’s margin will lower the agreement much more than smaller disparities. The unweighted Cohen’s kappa, however, treats all disparities equally.\nThe weighted kappa makes assumptions that are similar to Cohen’s kappa about the data:\n\nThe same two individuals must rate each event.\nThe principle of independence. The judges rate the same events without consultation or communication. This means the judges’ results are paired.\nThe judgments are made between the same ordinal categories.\n\n\nExercise 6: D’Amato and Lee\nWeighted kappa begins like the Cohen’s kappa with a contingency table. To simplify the analysis for this example, we only kept the fights that went three rounds.\n\n\nLeeD'Amato-5-4-3-2-1012345Total-5010000000001-4040000000004-310130400000018-2001210000004-1002114090100270000002300005100107018150032200000012000330010006011202040000000003035000000001102Total151832623731860119\n\n\n6.1. Take a look at the contingency table. Trace your eyes along the diagonal of concordant values. How often do the judges completely agree?\n\n 69 times, 58 percent 97 times, 82 percent 8 times, 7 percent\n\n6.2. In the first column, there is a single observation (a 1 in row three). What does this value represent?\n\n Agreement about the fight outcome (winner) but Lee had a wider margin in scores Agreement about the fight outcome (winner) but D'Amato had a wider margin in scores Disagreement about the fight outcome with the judges picking different winners\n\n6.3. What are the most common frequencies? Why?\n\n 0. The judges disagree too much. 18. Score differences of three are very likely since fights are three rounds. 0. There are many possible score combinations for the size of the sample.\n\n\n\nLeeD'Amato-5-4-3-2-1012345Total-50.00.00.20.00.20.00.30.00.20.101-40.00.20.60.10.90.11.20.10.60.204-30.20.82.70.53.90.35.60.52.70.9018-20.00.20.60.10.90.11.20.10.60.204-10.21.14.10.75.90.58.40.74.11.402700.00.20.80.11.10.11.60.10.80.30510.31.34.80.87.00.59.90.84.81.603220.00.10.50.10.70.10.90.10.50.20330.20.83.00.54.40.36.20.53.01.002040.00.10.50.10.70.10.90.10.50.20350.00.10.30.10.40.00.60.10.30.102Total1.05.018.03.026.02.037.03.018.06.00119\n\n\n6.4. Now look at the expected values. What values are the largest? Is this surprising?\n\n Large differences between judges (5 and -5). The judges should be expected to disagree by a lot. All diagnonal values. The judges are expected to agree often. All less than 10. There are many possible score combinations for the size of the sample.\n\n6.5. Does the observed agreement surpass the expected agreement? By how much?\n\n No 7 observed vs 58 percent expected. No. 58 observed vs 78 percent expected. Yes. 58 observed vs 18 percent expected. Yes. 82 observed vs 58 percent expected.\n\n\n\nWeights\nLike Cohen’s kappa, the weighted kappa calculates the proportion of observed agreement and the proportion of expected agreement by using the concordant values along the diagonal of our contingency tables. However, the calculation of these two agreements becomes more complex, because we can allow for partial agreement for close matches. The formulas are the same as Cohen’s kappa, except for the addition of the weights:\n\n\\(p_{o} = \\sum_{i}\\sum_{j} W_{ij} P_{ij}\\)\n\\(p_{e} = \\sum_{i}\\sum_{j} W_{i+} P_{+j}\\)\n\nwhere W is the weight for each cell and P is the proportion of each cells frequency.\nThe weights W are proportions between 0 and 1 that reflect the level of agreement. All concordant values have complete agreement, so their weight is 1. Values to the left and right of the diagonal have proprtions slightly less than 1 and so on. In the standard unweighted Cohen’s kappa, all the diagonal values have weights of 1 and the non-diagonal values have weights of 0.\nThere are many different ways to calculate the weights and selecting them generally depends on the size of the table and the distribution of the variables. Two common methods are linear and quadratic weighting.\nLinear weights, formally known as Cicchetti-Allison weights, create equal distance between the weights. A cell’s weight is directly proportional to its distance from the concordant value.\nThe formula for the linear weights are:\n\n\\(W_{ij} = 1 - (|i - j|)/(R - 1)\\)\n\nR is the total number of categories and |i - j| is the distance between the two cells.\nLet’s calculate the weights of the first few cells using the the formula. We’ll begin with the (-5, -5) cell and move right on the table.\n\n\\(W_{-5,-5} = 1 - (|0|/(11-1))\\) = 1\n\\(W_{-5,-4} = 1 - (|1|/(11-1))\\) = .9\n\\(W_{-5,-3} = 1 - (|2|/(11-1))\\) = .8\n\\(W_{-5,-2} = 1 - (|3|/(11-1))\\) = .7\n\\(W_{-5,-1} = 1 - (|4|/(11-1))\\) = .6\n\n\n\nJudge2Judge1-5-4-3-2-1012345-51.00.90.80.70.60.50.40.30.20.10.0-40.91.00.90.80.70.60.50.40.30.20.1-30.80.91.00.90.80.70.60.50.40.30.2-20.70.80.91.00.90.80.70.60.50.40.3-10.60.70.80.91.00.90.80.70.60.50.400.50.60.70.80.91.00.90.80.70.60.510.40.50.60.70.80.91.00.90.80.70.620.30.40.50.60.70.80.91.00.90.80.730.20.30.40.50.60.70.80.91.00.90.840.10.20.30.40.50.60.70.80.91.00.950.00.10.20.30.40.50.60.70.80.91.0\n\n\nNotice that each concordant value is 1 and all values next to it are 0.9. This creates a cascade effect for the weights.\nQuadratic weights, formally known as Fleiss-Cohen weights, use quadratic distancing between the weights. A cell’s weight is quadratically related to its distance from the concordant value.\nThe formula for the quadratic weights are:\n\n\\(W_{ij} = 1 - (|i - j|)^{2}/(R - 1)^{2}\\)\n\nAgain, let’s calculate the weights of the first few cells using the the formula. We’ll begin with the (-5, -5) cell and move right on the table.\n\n\\(W_{-5,-5} = 1 - (|0|^{2}/(11-1)^{2})\\) = 1\n\\(W_{-5,-4} = 1 - (|1|^{2}/(11-1)^{2})\\) = .99\n\\(W_{-5,-3} = 1 - (|2|^{2}/(11-1)^{2})\\) = .96\n\\(W_{-5,-2} = 1 - (|3|^{2}/(11-1)^{2})\\) = .91\n\\(W_{-5,-1} = 1 - (|4|^{2}/(11-1)^{2})\\) = .84\n\n\n\nJudge2Judge1-5-4-3-2-1012345-51.000.990.960.910.840.750.640.510.360.190.00-40.991.000.990.960.910.840.750.640.510.360.19-30.960.991.000.990.960.910.840.750.640.510.36-20.910.960.991.000.990.960.910.840.750.640.51-10.840.910.960.991.000.990.960.910.840.750.6400.750.840.910.960.991.000.990.960.910.840.7510.640.750.840.910.960.991.000.990.960.910.8420.510.640.750.840.910.960.991.000.990.960.9130.360.510.640.750.840.910.960.991.000.990.9640.190.360.510.640.750.840.910.960.991.000.9950.000.190.360.510.640.750.840.910.960.991.00\n\n\nAgain, notice how each concordant value is 1 and all values next to it are .99. This creates a steeper cascade than the linear weighting as the differences in judging increase.\nAssess the two weighting methods for yourself. What are the advantages and disadvantages of each? Can you imagine any problems arising for either? Which would you choose for our MMA data and why?\nLinear weighting values the distance between the fourth and fifth category the same as the distance between the first and second category. If this constant effect fits the data, then it’s best to choose linear weighting.\nQuadratic weighting determines that the distance between the first and second category is much less than the distance between the fourth and fifth category. As the categories get furthered removed from the concordant value, the difference becomes more egregious.\nFor the MMA data, we tend to think the quadratic weighting method works best. Generally, egregious misses are the errors that cast doubt on the judging system. The difference in a 3 point and 2 point win is basically none. Still, we need to be careful. Under the quadratic weighting method, a 1 point win for fighter 1 and a 1 point win for fighter 2 are essentially in agreement (w = .96).\n\n\nCalculating and Interpreting Weighted Kappa\nThe calculation and interpretation of the weighted kappa \\(\\kappa\\) are the same as Cohen’s kappa. If you need a refresher, read through our explanation in the previous tab.\nOur weighted kappa \\((\\kappa)\\) is calculated once again by \\(kappa(\\kappa) = \\displaystyle \\frac{p_{o} - p_{e}}{1 - p_{e}}\\).\nwith weights:\n\n\\(p_{o} = \\sum_{i}\\sum_{j} W_{ij} P_{ij}\\)\n\\(p_{e} = \\sum_{i}\\sum_{j} W_{i+} P_{+j}\\).\n\nUsing quadratic weights, the observed proportion of agreement is 0.982. This is extremely high, because we have so many partial agreements. If you’re curious, look again through our contingency table.\nHowever, the expected proportion of agreement is also very high at 0.9. The weights may inflate our observed agreement levels by adding in partial agreement, but they also inflate our expected agreement.\nAfter solving for D’Amato and Lee’s weighted kappa, we find it at 0.82. This is higher than the unweighted kappa (for the outcome variable) of 0.65, likely because a lot of judges disagree marginally.\nOur interpretation for the weighted kappa is identical to that of Cohen’s kappa. Below is a reminder:\n\n\\(\\kappa &gt; 0.75\\) Excellent reproducibility\n\\(0.4 \\le \\kappa \\le 0.75\\) Good reproducibility\n\\(0 \\le \\kappa &lt; 0.4\\) Marginal reproducibility\n\nUsing quadratic weights, we can say there is excellent reproducibility in the scoring margins between D’Amato and Lee. This means the judges are generally consistent in their scores and it is possible to replace one with the other and expect similar results. Remember, as with the other measures, this kappa does not mean that the judges are accurate in their assessments.\nWe calculate the confidence interval the same way as before, and our confidence interval is from 0.581 to 1.059. Thus, with 95% confidence, the interval includes the true kappa value for these judges.\nThis should give us pause. The lower end of our confidence interval is 0.581. This means the true kappa could be this low. This would drop our verdict to “good reproducibility” and change our overall assessment of the relationship.\nLike Cohen’s kappa, our kappa test has a null and alternative hypothesis of:\n\n\\(H_{o}: \\kappa = 0\\)\n\\(H_{a}: \\kappa &gt; 0\\)\n\nThe confidence interval doesn’t include 0, so we have sufficient evidence to reject the null hypothesis that there is no association between the judges’ scores.\n\n\nComparing Kappa and the Weighted Kappa\nLet’s compare our results with the linear weights. The observed proportion of agreement is 0.919 and the expected proportion of agreement is 0.747. The linear-weighted kappa is 0.68.\nThis drops our interpretation to only “good reproducibility”. We can be reasonably confident in the judges’ reproducibility, but it’s also feasible that swapping D’Amato for Lee could lead to a different result. An estimated 32% of the data is composed of chance agreement or disagreement. Once again, this would not indicate that D’Amato or Lee are somehow less accurate than before, it only speaks to their consistency and reproducibility.\nWe can say with 95% confidence that 0.486 and 0.874 contains the true kappa value. Once again, the interval does not include 0, so we have sufficient evidence to reject our null hypothesis that there is no association between the judges’ rulings. The lower end of the interval is 0.486. This would indicate “good reproducibility”. As with the quadratic weighting, this should lower our assessment of the relationship between the two judges.\n\n\nExercise 7: D’Amato and Cleary\nNow that we’ve walked through an example of weighted kappa on the consistency of D’Amato’s and Lee’s scoring margins, let’s look at the scoring margins of D’Amato and Cleary. We’ll present the data and the findings to you, and you can reproduce the analysis. Feel free to look at our earlier phrasings and points.\nOnce again, we filtered the data to only include three rounds. We displayed all of the scoring margins by the judges in a contingency table below.\n\n\nClearyD'Amato-5-4-3-2-1012345Total-5210000000003-4031000000004-301170300000021-2000010000001-1003120050000290000003000003100002020021025200000001010230000007014002140000000212165000000000112Total252112633231752117\n\n\n7.1. How often do D’Amato and Cleary completely agree?\n\n 71 times (61 percent) 98 times (84 percent) 83 times (71 percent)\n\n7.2. Do D’Amato and Cleary seem to agree on the fight often but not on the score?\n\n No. There are many values in the off diagnonal. Yes. There are only 7 times one is positive and the other negative. Yes. There are few values in the off diagnonal.\n\n7.3. How does D’Amato and Cleary agreement compare to D’Amato and Lee (previous exercise)?\n\n They agree more. (71 vs 58 percent) They agree much less (28 vs 71 percent) They agree less (58 vs 71 percent) They agree much more (92 vs 58 percent)\n\n\n\nClearyD'Amato-5-4-3-2-1012345Total-50.10.10.50.00.70.10.80.10.40.10.13-40.10.20.70.00.90.11.10.10.60.20.14-30.40.93.80.24.70.55.70.53.10.90.421-20.00.00.20.00.20.00.30.00.10.00.01-10.51.25.20.26.40.77.90.74.21.20.52900.10.10.50.00.70.10.80.10.40.10.1310.41.14.50.25.60.66.80.63.61.10.42520.00.10.40.00.40.10.50.10.30.10.0230.40.93.80.24.70.55.70.53.10.90.42140.10.31.10.11.30.21.60.20.90.30.1650.00.10.40.00.40.10.50.10.30.10.02Total2.05.021.01.026.03.032.03.017.05.02.0117\n\n\n7.4. Look through the expected value table. Do the highly expected values also occur frequently in the observed table?\n\n No. The highest observed values of 20 are 0 in the expected table. Yes. The highest expected values of 5 and above are all observed at least 10 times. Somewhat. The highest observed values have relatively high exepected, but some high expected values are not observed often.\n\nQuadratic Weights:\n\nProportion of Observed Agreement: 0.99\nProportion of Expected Agreement: 0.878\nWeighted kappa with Quadratic Weights: 0.92\n95% Confidence Interval for kappa: 0.772 and 1.068\n\n7.5. Using quadratic weights do D’Amato and Cleary have good reproducibility?\n\n Yes, the kappa value is over 0.9. No. The observed agreement is not much better than the expected.\n\n7.6. (Optional - Advanced) Assess the confidence interval based on quadratic weights. What can you conclude?\n\n We have evidence to support excellent reproducibility for these two judges. We have too much variability so there is not enough evidence to conclude reproducability is excellent.\n\nLinear Weights:\n\nProportion of Observed Agreement: 0.948\nProportion of Expected Agreement: 0.722\nWeighted kappa with Linear Weights: 0.81\n95% Confidence Interval for kappa: 0.665 and 0.955\n\n7.7. Using linear weights do D’Amato and Cleary have good reproducibility?\n\n No. The observed agreement is not much better than the expected. Yes, the kappa value is over 0.9.\n\n7.8. (Optional - Advanced) Assess the confidence interval based on linear weights. What can you conclude?\n\n We have evidence to support excellent reproducibility for these two judges. We have too much variability so there is not enough evidence to conclude reproducability is excellent.\n\n7.9. (Optional Advanced) How large is the difference between the quadratic and linear weights?\n\n A large enough difference to matter when drawing inference from the confidence interval. Hardly any difference so the choice does not matter here.",
    "crumbs": [
      "Home",
      "Mixed Martial Arts",
      "MMA Inter-rater Reliability Data Analysis"
    ]
  },
  {
    "objectID": "mixed_martial_arts/mma_interrater_reliability/index.html#fleiss-kappa",
    "href": "mixed_martial_arts/mma_interrater_reliability/index.html#fleiss-kappa",
    "title": "MMA Inter-rater Reliability Data Analysis",
    "section": "Fleiss’ Kappa",
    "text": "Fleiss’ Kappa\nThus far, we’ve assessed the inter-rater reliability within data sets of two judges, but what about three or more judges? MMA fights are evaluated by three judges, and in both the weighted and unweighted variations of Cohen’s kappa, we completely ignore the third judge. This ignorance becomes even more egregious if we have larger quantities of judges.\nSeveral different methodologies have been created to account for this. Light’s kappa, for example, takes the average of every combination of Cohen’s kappa within the pool of raters. We’ll turn to a slightly more complex version. Fleiss’ kappa is a variation of Cohen’s kappa that allows for three or more judges. It measures the level of agreement or consistency within the group of judges. A high Fleiss’ kappa would indicate a high rate of reliability between the group of judges.\nFleiss’ kappa works with nominal variables. It does not give weight to partial agreement like weighted kappa. There are methods that work with ordinal variables and partial agreement with three or more judges, but they extend beyond the scope of this module. Search for Kendall’s Coefficient of Concordance if you are interested.\nLike all other kappa values, Fleiss’ kappa removes chance agreement. Because the method is unweighted and gives out no partial agreement, we’ll use the outcome variable for our analysis.\nFleiss’s kappa makes a few assumptions about the data. They are similar to the assumptions made by weighted kappa and Cohen’s kappa, but not exactly the same.\n\nEach of the raters are independent.\nThe raters are selecting from the same defined categories of a categorical variable.\n\nWe’ve selected a new set of three judges from our data set that judged lots of fights together. Cartlidge, Collett, and Lethaby judged 96 fights that went to a decision together.\nWith three or more judges, it becomes difficult to observe the data using a contingency table.\nBelow is a table of each judge’s verdict for the 96 fights. We’ve created three columns on the right to help summarize the judge’s votes. They sum up the total number of verdicts of that type for each fight.\n\n\nfightCartlidgeCollettLethabyfighter2drawfighter11fighter2fighter2fighter23002fighter1fighter1fighter10033fighter1fighter1fighter10034fighter2fighter2fighter23005fighter2fighter2fighter23006fighter1fighter1fighter10037fighter1fighter1fighter10038fighter1fighter1fighter10039fighter1fighter1fighter100310fighter1fighter2fighter220111fighter1fighter1fighter100312fighter2fighter2fighter230013fighter1fighter1fighter100314fighter2fighter2fighter230015fighter1fighter1fighter1003\n\n\n\nExercise 8: Cartlidge, Collett, and Lethaby\n8.1. Take a look at the table. How often do the judges agree?\n\n All but one fight About half of the fights Very rarely (less than 20 percent)\n\n\n\nCalculating and Interpreting Fleiss’ Kappa\nAs with the other kappas, we begin by calculating the the proportion of observed agreement \\((p_{o})\\) and proportion of expected agreement \\((p_{e})\\). However, for Fleiss’ kappa, they are calculated in more complex ways.\nThis makes sense. As we add more judges, we have so many more levels of agreement. For example, if Collett and Lethaby agree, but Cartlidge disagrees (like fight 10 in our data above), this is still better agreement than if all three judges give different verdicts. These options are only magnified if we were to consider sets of four or more judges or events with four of more different outcomes.\nThe proportion of observed agreement is calculated by a long formula:\n\n\\(p_{o} = \\displaystyle \\frac{1}{N * n * (n - 1)} (\\sum_{i=1}^{N} \\sum_{j=1}^{k}n^{2}_{ij} - N * n)\\)\n\nwhere N is the number of observations and n is the number of raters.\nFor our example, N = 96 and n = 3.\nYou won’t have to calculate it by hand, and in this case intuition for the formula is not easy and beyond the scope of the module.\nWe show the calculation using this formula for the proportion of observed agreement for our set of Cartlidge, Collett, and Lethaby:\n\n\\(p_{o} = \\displaystyle \\frac{1}{96 * 3 * (3 - 1)} (3^{2} + 0^{2} + 0^{2} + ... + 1^{2} - 96 * 3)\\)\n\n(Optional) Take a moment if you are interested to see how we entered the values into the formula.\nAfter evaluating, we end up with a \\(p_{o}\\) = 0.882. This is our total observed agreement. It includes both real agreement and chance agreement.\nThe proportion of expected agreement is computed by a much less complex formula.\n\n\\(p_{e} = \\sum p_{j}^{2}\\)\n\nWe calculate the frequency (or expected rate) for each of the three categories \\((p_{j})\\), square them, and add them all together. This is like finding the concordant values with two judges. We’re finding the probability that the selections appear together randomly.\n\n\\(p_{fighter1}\\) = 0.524\n\\(p_{draw}\\) = 0.017\n\\(p_{fighter2}\\) = 0.458\n\nIf we square these frequencies and sum them up, we’ll find that \\(p_{e}\\) = 0.485.\nThis means that if the three judges were to issue random verdicts without watching the fights or consulting with each other, we’d expect the three of them to agree about 48.5% of the time.\nWe can solve for Fleiss’ kappa \\((\\kappa)\\) with the same formula as the weighted and unweighted kappa values.\n\n\\(kappa(\\kappa) = \\displaystyle \\frac{p_{o} - p_{e}}{1 - p_{e}}\\).\n\nFleiss’ kappa for Cartlidge, Collett, and Lethaby is 0.771.\nOur interpretation for the Fleiss’ kappa is identical to that of the weighted and unweighted kappa. Below is a reminder:\n\n\\(\\kappa &gt; 0.75\\) Excellent reproducibility\n\\(0.4 \\le \\kappa \\le 0.75\\) Good reproducibility\n\\(0 \\le \\kappa &lt; 0.4\\) Marginal reproducibility\n\nWe can claim that Cartlidge, Collett, and Lethaby have excellent reproducibility in their judgments. This means they are likely to evaluate the fights in similar ways, and if we substituted one for another, we would not expect exceedingly different results. About 23% of the data is a result of chance agreement or disagreement. Once again, this cannot prove that the three of them are good at selecting the correct victor, only that they are likely to select similar victors.\nAs with the other kappa values, we can calculate a confidence interval. Our 95% confidence interval for Fleiss’ kappa is 0.661 to 0.881. Thus, with 95% confidence, we can claim that the interval includes the true value of Fleiss’ kappa. This interval does not include 0, so we can conclude with at least 95% confidence that there is some real association between the three judges. The lower end of the confidence interval is 0.661, which would be in the upper portion of the “good reproducibility” bracket.\nFleiss’ kappa does afford us an extra piece of analysis. We can look at the individual kappas for each of the categories to assess the level of agreement across their verdicts. This can help us to break down our kappa into simpler results that assess raters reliability on only one category.\nThis can be especially helpful for certain tests of reliability. For example, a survey evaluating the inter-rater-reliability of several doctors prescribing or diagnosing patients would immensely benefit by seeing which prescriptions or diagnoses the doctors are most and least consistent in the ratings.\nFor our data, we’ll look at the individual kappas for the categories: fighter1, draw, and fighter2.\n\n\nCategoryKappazp.valuedraw0.1863.1540.002fighter10.79113.4270.000fighter20.79013.4100.000\n\n\nFighter 1 and fighter 2 are arbitrary assignments, so it is fitting that their values are almost identical. Their difference would not tell us anything meaningful regardless. However, the individual kappa of the draw category is much smaller than the others. This demonstrates that the judges have a much lower level of agreement when issuing draws than when they select a fighter.\nThis makes contextual sense. Draws are unlikely and less desirable. Collett, Cartlidge, and Lethaby never put forth a unanimous draw, and they rarely even had two of three vote draw.\n\n\nExercise 9: Other Judges\nWe’ll provide a second example using the judge combination of Cartlidge, Sledge, and Lethaby. We’ll ask you some general questions to help guide your analysis.\n\n\njudge1judge2judge3fightsCartlidgeCollettLethaby96CartlidgeLethabySledge36ChampionDivilbissGraham33ClearyD'AmatoLee24CrosbyD'AmatoValel22ClearyD'AmatoKamijo18CartlidgeLethabyOglesby14ColónTirelliUrso14GuearyMillerSwanberg14MathisenSutherlandTurnage14\n\n\n\n\nfightCartlidgeLethabySledgefighter2drawfighter11fighter2fighter2fighter23002fighter2fighter2fighter23003fighter1fighter2fighter22014fighter2fighter2fighter23005fighter1fighter1fighter10036fighter2fighter2fighter23007fighter2fighter2fighter23008fighter1fighter1fighter10039fighter1fighter2fighter220110fighter1fighter1fighter100311fighter1fighter1fighter100312fighter1fighter1fighter100313fighter2fighter2fighter230014fighter2fighter2fighter230015fighter2fighter1fighter1102\n\n\n9.1. Take a look at the table. How often do the judges agree?\n\n Most fights (12 of 15) Very rarely (less than 20 percent) About half of the fights\n\n9.2. Does one judge tend to differ more?\n\n Yes, Cartlidge is the only one to disagree Yes, Sledge disagrees the most No, they all disagree at least one time\n\nResults\n\nProportion of Observed Agreement: 0.926\nProportion of Expected Agreement: 0.529\nFleiss’ kappa: 0.843\n95% Confidence Interval for kappa: 0.843 and 0.654\n\nTable of Individual kappas:\nCategoryKappazp.valuefighter10.8438.7580fighter20.8438.7580\n9.3. Based on the results, what is the percent disagreement that is attributable to chance?\n\n Most (about 84 percent) About two thirds (65 percent) Just over half (53 percent)\n\n9.4. How good is the estimated reproducibility for the three judges?\n\n Good, higher than chance agreement (84 percent) Marginal, not much better than chance agreement (65 percent) Excellent, well above chance agreement (over 90 percent)\n\n9.5. (Optional advanced) Assess the confidence interval. Can you reject a null hypothesis of excellent reproducibility?\n\n Cannot determine from the information provided. Yes, we reject that kappa is equal to 0 No, the interval includes the possiblity of a value that is only good",
    "crumbs": [
      "Home",
      "Mixed Martial Arts",
      "MMA Inter-rater Reliability Data Analysis"
    ]
  },
  {
    "objectID": "marathons/boston-marathon-finish_times-2023/index.html",
    "href": "marathons/boston-marathon-finish_times-2023/index.html",
    "title": "2023 Boston Marathon - Variability in Finish Times",
    "section": "",
    "text": "Welcome video\n\n\n\n\nIntroduction\nFor this activity, you will be exploring the result times from female and male runners that finished the 2023 Boston Marathon.\nIn particular, you will examine both visualizations and summary statistics of result times to explore the variation in finish times as well as use comparative techniques, such as z-scores, to compare and contrast male and female participants.\nInvestigating these trends is useful for several reasons. Firstly, exploring these trends can help to deepen our understanding of how different factors, such as gender, impact marathon performances. Secondly, analyzing the distribution of finish times and the performance of top finishers against the masses provides insights into the competitive landscape of the marathon. It can identify outliers or exceptional performances and understand how elite athletes compare to average participants. Although not directly connected to this data, analyses like these can inform training strategies, highlight the effectiveness of different preparation methods, and inspire both new and experienced runners by showcasing the range of achievable performances.\n\n\n\n\n\n\nActivity Length\n\n\n\n\n\nThis activity would be suitable for an in-class example or quiz.\n\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nBy the end of the activity, you will be able to:\n\nAnalyze distributions using histograms\nIdentify potential confounding variables to explain bimodal data\nCompare and contrast distributions for a pair of groups\nCalculate and compare z-scores for individual cases\n\n\n\n\n\n\n\n\n\n\nMethods\n\n\n\n\n\nFor this activity, students will primarily use basic concepts of histograms and summary statistics to analyze distributions. Students will also likely require knowledge of z-scores.\n\n\n\n\n\n\n\n\n\nTechnology Requiremens\n\n\n\n\n\nThe provided worksheets do not require any specific statistical software. (Although they will likely require access to a calculator.)\nSince the data are provided, instructors are encouraged to modify the worksheets to have student construct visualizations and calculate summary statistics using whichever software they choose.\n\n\n\n\n\nData\nThe data set contains 26598 rows and 15 columns. Each row represents a runner who completed the Boston Marathon in 2023\nDownload data:\nAvailable on the SCORE Data Repository: boston_marathon_2023.csv\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nage_group\nage group of the runner\n\n\nplace_overall\nfinishing place of the runner out of all runners\n\n\nplace_gender\nfinishing place of runner among the same gender\n\n\nplace_division\nfinishing place of runner among runners of the same gender and age group\n\n\nname\nname of runner\n\n\ngender\ngender of runner\n\n\nteam\nteam the runner is affiliated with\n\n\nbib_number\nbib number of runner\n\n\nhalf_time\nhalf marathon time of runner\n\n\nfinish_net\nfinishing time timed from when they cross the starting gate\n\n\nfinish_gun\nfinishing time of runner timed from when the starter gun is fired\n\n\nhalf_time_sec\nhalf marathon time in seconds\n\n\nfinish_net_sec\nnet finish in seconds\n\n\nfinish_gun_sec\ngun finish in seconds\n\n\nfinish_net_minutes\nnet finish in minutes\n\n\n\nData Source\nBoston Athletic Association\n\n\n\nMaterials\n\nWe provide editable MS Word handouts along with their solutions.\n\nClass handout\n\n\nClass handout - with solutions\n\n\n\n\n\n\n\n\nConclusion\n\n\n\n\n\nIn conclusion, the Boston Marathon Times worksheet provides valuable learning opportunities for students in several key areas. It allows them to understand reasons by variability might exist and to discover multimodal distributions can occur simply due to excluding an important explanatory variable that otherwise confounds the analysis. The calculation of z-scores or other similar measurement of relative location enables students to compare and contrast the remarkable achievements of the top female and male finishers, shedding light on their talent in their respective fields. Overall, this worksheet allows students to critically analyze the 2023 marathon result data and draw meaningful conclusions about the extraordinary performances of athletes in the race.",
    "crumbs": [
      "Home",
      "Marathons",
      "2023 Boston Marathon - Variability in Finish Times"
    ]
  },
  {
    "objectID": "lacrosse/college_lacrosse_faceoffs/index.html",
    "href": "lacrosse/college_lacrosse_faceoffs/index.html",
    "title": "Lacrosse Faceoff Proportions",
    "section": "",
    "text": "Introduction\nIn this engaging activity, we explore the exciting sport of NCAA Division I Lacrosse, with a special focus on faceoff percentages—a critical aspect of the game. A faceoff occurs at the start of each quarter and after every goal, where two players compete to gain possession of the ball, setting the stage for their team’s offensive play. Winning a high percentage of faceoffs is often key to controlling the game and can significantly impact a team’s overall performance.\n\n\n\n\n\n\nVideo Demonstrating a Faceoff\n\n\n\n\n\n\n\n\n\nOur primary goal is to compare a specific team’s faceoff performance with overall league statistics for the 2022-2023 season. Through this exploration, we’ll introduce you to the concept of one-sample proportion hypothesis testing, a powerful statistical tool widely used in sports analytics. By the end of this exercise, you’ll gain a fundamental understanding of hypothesis testing and how it can be practically applied to evaluate team performance in lacrosse and beyond.\n\n\n\n\n\n\nActivity Length\n\n\n\n\n\nThis activity would be suitable for an in-class example (of approximately 10 - 20 minutes) or can be modified to be a quiz or part of an exam.\n\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\n\nComprehend the concept of one sample proportion hypothesis testing and its relevance in sports statistics.\nAnalyze and interpret dataset variables related to faceoff percentages in NCAA Division I Lacrosse.\nEvaluate a specific team’s faceoff performance by comparing it with league-wide statistics using hypothesis testing.\n\n\n\n\n\n\n\n\n\n\nMethods\n\n\n\n\n\nStudents are expected to have been exposed to the following concepts and use the activity to reinforce their understanding of these methods.\n\nBasic probability and percentages.\nNull and alternative hypotheses.\nSample size and sample proportion calculations.\nSuccess-failure condition for hypothesis testing.\nCalculation of test statistics (Z-score).\nUnderstanding significance levels (⍺) and p-values.\nDrawing conclusions and implications from hypothesis test results.\n\n\n\n\n\n\nData\nNote that because the activity only uses results from one team, students do not necessarily need to directly access this data. However, the activity can easily be adapted to use other teams. Instructors are encouraged to personalize the activity if they so choose.\nThe data set where the activities statistics come from contains 72 rows and 22 columns. Each row represents the season results for a lacrosse team at the NCAA Division 1 level from the 2022-2023 season.\nDownload data: lax_2022_2023.csv\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nTeam\ncollege of the team\n\n\navg_assists\naverage assists to goals per game\n\n\navg_caused_turnovers\naverage turnovers forced by the team per game\n\n\nclearing_pctg\npercentage of successful attempts to earn an offensive opportunity after gaining the ball in the teams own half\n\n\ntotal_faceoffs\ntotal faceoffs taken by a team for the season\n\n\nfaceoff_wins\ntotal faceoff wins by a team for the season\n\n\nfaceoff_win_pct\nproportion of total faceoff wins out of total faceoffs\n\n\navg_goals\naverage goals per game\n\n\navg_goals_allowed\naverage goals allowed by the team per game\n\n\navg_ground_balls\naverage loose balls picked up by the team per game\n\n\nman_down_defense_pctg\nproportion of times a team stops the opponent from scoring while man down due to a penalty\n\n\nman_up_offense_pctg\nproportion of times the offense scores out of total opportunities while man up\n\n\navg_scoring_margin\naverage margin of goals per game\n\n\nopp_clear_pctg\nopponents clearing percentage averaged by game\n\n\navg_points\naverage team points per game\n\n\navg_saves\naverage saves per game\n\n\nshot_pctg\nproportion of shots that go in out of total shots\n\n\navg_turnovers\naverage turnovers that are directly the fault of a player per game\n\n\nW\ntotal wins by the team\n\n\nL\ntotal losses by the team\n\n\nwin_loss_pctg\nproportion of games won out of total games\n\n\n\nData Source\nThe data were collected from the NCAA Website for Men’s Lacrosse Division I\nhttp://stats.ncaa.org/rankings/change_sport_year_div\nInstructors interested in updating the data to a newer season can do so via the following\n\nGo to http://stats.ncaa.org/rankings/change_sport_year_div\nSelect Men’s Lacrosse, season of choice, Division I, Final Statistics\nIn the “Teams”, download each of the data tables.\nRead in each file, join the tables, and do some light cleaning. The code below shows an example used for the 2022-2023 season.\n\n\n\nShow the code\nlibrary(tidyverse)\n\n\n# reading\n# the files listed here are what\n# you will download from the site\n\nassists&lt;- read_csv(\"assists_l.csv\", col_select = 1:2)\ncaused_turnovers&lt;- read_csv(\"caused_turnovers_l.csv\", col_select = 1:2)\nclearing&lt;- read_csv(\"clearing_pctg_l.csv\", col_select = 1:2)\nfo &lt;- read_csv(\"fo_win_pctg.csv\", col_select = 1:4)\ngoals_against&lt;- read_csv(\"goals_against.csv\", col_select = 1:2)\ngoals&lt;- read_csv(\"goals_l.csv\", col_select = 1:2)\ngroundballs&lt;- read_csv(\"ground_balls_l.csv\", col_select = 1:2)\nman_down &lt;- read_csv(\"man_down_defense_l.csv\", col_select = 1:2)\nman_up &lt;- read_csv(\"man_up__offense_l.csv\", col_select = 1:2)\nmargin &lt;- read_csv(\"margin_l.csv\", col_select = 1:2)\nopp_clear &lt;- read_csv(\"opp_clear_l.csv\", col_select = 1:2)\npoints &lt;- read_csv(\"points_l.csv\", col_select = 1:2)\nsaves &lt;- read_csv(\"saves_l.csv\", col_select = 1:2)\nshot &lt;- read_csv(\"shot_pctg_l.csv\", col_select = 1:2)\nturnovers&lt;- read_csv(\"turnovers_l.csv\", col_select = 1:2)\nshots_per_game &lt;- read_csv(\"shots_per_game.csv\", col_select = 1:3)\nwin_loss &lt;- read_csv(\"win_loss_l.csv\")\n\n# joining\n# students familiar with the purrr package could\n# use the reduce function to reduce the amount of code\n\nlax_2022_2023 &lt;- \n  left_join(assists, caused_turnovers, by = \"Team\") %&gt;%\n  left_join(clearing, by = \"Team\") %&gt;%\n  left_join(fo, by = \"Team\")  %&gt;%\n  left_join(goals, by = \"Team\")  %&gt;%\n  left_join(goals_against, by = \"Team\")  %&gt;%\n  left_join(groundballs, by = \"Team\") %&gt;%\n  left_join(man_down, by = \"Team\") %&gt;%\n  left_join(man_up, by = \"Team\") %&gt;%\n  left_join(margin, by = \"Team\") %&gt;%\n  left_join(opp_clear, by = \"Team\") %&gt;%\n  left_join(points, by = \"Team\") %&gt;%\n  left_join(saves, by = \"Team\") %&gt;%\n  left_join(shot, by = \"Team\") %&gt;%\n  left_join(turnovers, by = \"Team\") %&gt;%\n  left_join(shots_per_game, by = \"Team\") %&gt;%\n  left_join(win_loss, by = \"Team\")\n\n# cleaning\nlax_2022_2023 &lt;- lax_2022_2023 %&gt;%\n  separate(Team, into = c(\"Team\",\"Conference\"), sep = \"\\\\(\", extra = \"merge\")%&gt;%\n  mutate(Conference = str_remove_all(Conference,\"\\\\)\"),\n         Team = str_trim(Team))%&gt;%\n  mutate(shots_per_game = Shots/Games)%&gt;%\n  select(-20, -21)\n\n# saving\nwrite_csv(x = lax_2022_2023, file = \"lax_2022_2023.csv\")\n\n\n\n\n\nMaterials\nClass handout\nClass handout - with solutions\n\n\n\n\n\n\nConclusion\n\n\n\n\n\nIn this insightful exploration of NCAA Division I Lacrosse faceoff percentages, we have embarked on a statistical journey to evaluate a specific team’s performance in comparison to league-wide statistics. Through the application of one sample proportion hypothesis testing, we gained valuable insights into the team’s faceoff win percentage, unveiling strong evidence that their performance exceeded what we would expect by random chance alone. As we consider the broader implications of faceoffs in Division I Lacrosse, it becomes evident that faceoff wins play a pivotal role in team rankings and outcomes. The fact that Duke, the second-best team in the country, exhibited a faceoff win percentage above the league average highlights the significance of excelling in this aspect of the game. Winning faceoffs likely translates to higher goal-scoring opportunities, ultimately leading to more successful game outcomes.",
    "crumbs": [
      "Home",
      "Lacrosse",
      "Lacrosse Faceoff Proportions"
    ]
  },
  {
    "objectID": "football/nfl-elo-ratings/index.html",
    "href": "football/nfl-elo-ratings/index.html",
    "title": "Introduction to Elo ratings",
    "section": "",
    "text": "Elo ratings are one of the most popular approaches for estimating player/team strength across a variety of sports. You can find a number of different sports examples maintained by sportswriter Neil Paine, as well as older versions that were featured in the popular website FiveThirtyEight. These dynamic ratings are adjusted for opponent strength and can be used for historical comparisons, such as who is the greatest tennis player of all time?, and for predicting outcomes. In this module you will learn the basics of Elo ratings in the context of measuring NFL team strength, walking through steps to implement and assess Elo ratings from scratch in R.",
    "crumbs": [
      "Home",
      "Football",
      "Introduction to Elo ratings"
    ]
  },
  {
    "objectID": "football/nfl-elo-ratings/index.html#motivation",
    "href": "football/nfl-elo-ratings/index.html#motivation",
    "title": "Introduction to Elo ratings",
    "section": "",
    "text": "Elo ratings are one of the most popular approaches for estimating player/team strength across a variety of sports. You can find a number of different sports examples maintained by sportswriter Neil Paine, as well as older versions that were featured in the popular website FiveThirtyEight. These dynamic ratings are adjusted for opponent strength and can be used for historical comparisons, such as who is the greatest tennis player of all time?, and for predicting outcomes. In this module you will learn the basics of Elo ratings in the context of measuring NFL team strength, walking through steps to implement and assess Elo ratings from scratch in R.",
    "crumbs": [
      "Home",
      "Football",
      "Introduction to Elo ratings"
    ]
  },
  {
    "objectID": "football/nfl-elo-ratings/index.html#learning-objectives",
    "href": "football/nfl-elo-ratings/index.html#learning-objectives",
    "title": "Introduction to Elo ratings",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this module, you will be able to:\n\nCompute the expected outcome or predicted probability based on player/team ratings.\nUpdate ratings following the observed outcome of a match/game.\nImplement the complete Elo ratings framework in R for a full NFL season.\nAssess Elo rating predictions using Brier scores.\nTune the update factor and other settings to yield more optimal predictions.",
    "crumbs": [
      "Home",
      "Football",
      "Introduction to Elo ratings"
    ]
  },
  {
    "objectID": "football/nfl-elo-ratings/index.html#data",
    "href": "football/nfl-elo-ratings/index.html#data",
    "title": "Introduction to Elo ratings",
    "section": "Data",
    "text": "Data\nThe dataset and description are available at the SCORE Network Data Repository.",
    "crumbs": [
      "Home",
      "Football",
      "Introduction to Elo ratings"
    ]
  },
  {
    "objectID": "football/nfl-elo-ratings/index.html#module-materials",
    "href": "football/nfl-elo-ratings/index.html#module-materials",
    "title": "Introduction to Elo ratings",
    "section": "Module Materials",
    "text": "Module Materials\n\n\n\n\n\n\nPrerequisites\n\n\n\nPrior to working on through this module, students are expected to know the following:\n\nFamiliar with R with the ability to read and write functions.\nSome exposure to predicting outcomes with probabilities.\n\nThe module has sections indicating which portions are challenging exercises, and is designed to take an undergraduate student roughly 1-3 hours to complete (and 3-4 hours with the challenge exercise).\n\n\nStudent assignment qmd file\nView instructor solutions",
    "crumbs": [
      "Home",
      "Football",
      "Introduction to Elo ratings"
    ]
  },
  {
    "objectID": "football/nfl-elo-ratings/index.html#how-to-cite",
    "href": "football/nfl-elo-ratings/index.html#how-to-cite",
    "title": "Introduction to Elo ratings",
    "section": "How to Cite",
    "text": "How to Cite\nIf you use this module in your work, please cite it as follows:\nYurko, R. (2025, March 11). Introduction to ELO Ratings. “The SCORE Network.” https://doi.org/10.17605/OSF.IO/DHUQ2\nYou can include this citation directly in your references or bibliography.",
    "crumbs": [
      "Home",
      "Football",
      "Introduction to Elo ratings"
    ]
  },
  {
    "objectID": "baseball/unbreakable-records/index.html",
    "href": "baseball/unbreakable-records/index.html",
    "title": "Unbreakable Records in Baseball",
    "section": "",
    "text": "This lesson introduces students to the Bernoulli trial and Binomial Experiments to understand the probability of breaking one of the longest lasting records in baseball. We also explain how to execute a Chi-Square test using baseball data on handedness (right or left-handed) of batters versus pitchers.",
    "crumbs": [
      "Home",
      "Baseball",
      "Unbreakable Records in Baseball"
    ]
  },
  {
    "objectID": "baseball/unbreakable-records/index.html#motivation",
    "href": "baseball/unbreakable-records/index.html#motivation",
    "title": "Unbreakable Records in Baseball",
    "section": "",
    "text": "This lesson introduces students to the Bernoulli trial and Binomial Experiments to understand the probability of breaking one of the longest lasting records in baseball. We also explain how to execute a Chi-Square test using baseball data on handedness (right or left-handed) of batters versus pitchers.",
    "crumbs": [
      "Home",
      "Baseball",
      "Unbreakable Records in Baseball"
    ]
  },
  {
    "objectID": "baseball/unbreakable-records/index.html#module",
    "href": "baseball/unbreakable-records/index.html#module",
    "title": "Unbreakable Records in Baseball",
    "section": "Module",
    "text": "Module\nhttps://isle.stat.cmu.edu/SCORE/Unbreakable_Records_Baseball_Hits/",
    "crumbs": [
      "Home",
      "Baseball",
      "Unbreakable Records in Baseball"
    ]
  },
  {
    "objectID": "baseball/mlb_injuries/index.html",
    "href": "baseball/mlb_injuries/index.html",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "",
    "text": "Facilitation notes\n\n\n\n\n\n\nThis module would be suitable for an in-class lab or take-home assignment in an intermediate statistics course.\nIt assumes a familiarity with the RStudio Environment and R programming language.\nStudents should be provided with the following data file (.csv) and Quarto document (.qmd) to produce visualizations and write up their answers to each exercise. Their final deliverable is to turn in an .html document produced by “Rendering” the .qmd.\n\nMonthly Injury Data\nTommy John Surgeries Data\nStudent Quarto template\n\nPosit Cloud (via an Instructor account) or Github classroom are good options for disseminating files to students, but simply uploading files to your university’s course management system works, too.\nThe data for the mlb_injuries_monthly.csv file was derived from data found on prosportstransactions.com. The original data from the site contained rows of observations showing each transaction that involved sending a player to the injured list or bringing a player off of the injured list. The data was then filtered to include only times when a player was sent to the injured list and aggregated by month. The 2000 season was originally included in the data, but was removed after incomplete data was found for that season.\nThe data for the tj_surgeries_mlb_milb.csv file was derived from data accumulated by @MLBPlayerAnalys over many years. The original data lists any reported Tommy John surgery for a player now playing in the MLB or MiLB. This data was aggregated by year to produce the data used in this module.",
    "crumbs": [
      "Home",
      "Baseball",
      "MLB Injuries - Introductory Time Series Analysis"
    ]
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#classical-decomposition",
    "href": "baseball/mlb_injuries/index.html#classical-decomposition",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "Classical Decomposition",
    "text": "Classical Decomposition\nThere are two common methods for decomposing time series data: classical decomposition and STL decomposition. In this module we will focus on classical decomposition.\nClassical decomposition (additive) has four main steps:\n\nComputing a trend-cycle component (\\(T_t\\)) using a moving average. A moving average is a technique for smoothing time series data by averaging the values of neighboring points. This helps to remove short-term fluctuations and highlight longer-term trends.\nComputing a series without the trend-cycle component (\\(y_t - T_t\\)).\nEstimating the seasonal component (\\(S_t\\)) by averaging the values from the detrended series for the season.\nComputing the remainder component (\\(R_t\\)) by subtracting the trend-cycle and seasonal components from the original series. \\(R_t = y_t - T_t - S_t\\)\n\nWe can use the classical_decomposition function inside of the model() function to decompose our time series data.\n\ninjuries |&gt; \n  model(classical_decomposition(Count)) |&gt;\n  components() |&gt;\n  autoplot() +\n  labs(title = \"Classical Additive Decomposition of Monthly MLB Injury Counts\")\n\n\n\n\n\n\n\n\nClassical multiplicative decomposition works similarly to the additive decomposition, but with a few key differences.\n\nThe detrended series is computed as \\(y_t / T_t\\) instead of \\(y_t - T_t\\).\nThe seasonal component is estimated as \\(S_t = y_t / T_t\\) instead of \\(y_t - T_t\\).\nThe remainder component is computed as \\(R_t = y_t / (T_t \\times S_t)\\) instead of \\(y_t - T_t - S_t\\).\n\n\n\nNOTE: Ideally, after doing a decomposition, the remainder component should be white noise.\nClassical multiplicative decomposition can be used by setting the type argument to \"multiplicative\" in the classical_decomposition() function.",
    "crumbs": [
      "Home",
      "Baseball",
      "MLB Injuries - Introductory Time Series Analysis"
    ]
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#stl-decomposition",
    "href": "baseball/mlb_injuries/index.html#stl-decomposition",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "STL Decomposition",
    "text": "STL Decomposition\nSeasonal and Trend decomposition using Loess (STL) is a more advanced method for decomposing time series data. It is more flexible than classical decomposition and can handle any type of seasonality, not just monthly or quarterly. It also allows the user to control the length of the smoothing window for the trend-cycle. Lastly, it is more robust to outliers so that they do not affect the trend and seasonal estimates as much.\nBelow is an example of how to use the STL() function to decompose the time series data.\n\ninjuries |&gt; \n  model(STL(Count ~ trend(window = 21)+ \n            season(window = \"periodic\"),\n            robust = TRUE)) |&gt;\n  components() |&gt;\n  autoplot() +\n  labs(title = \"STL Additive Decomposition of Monthly MLB Injury Counts\")\n\n\n\n\n\n\n\n\n\n\nTIP: The window argument in the trend() function controls the length of the smoothing window for the trend-cycle. The larger the window, the smoother the trend. This value should always be an odd number so that a central point can be used. trend(window = 21) is a common choice for monthly data. This relatively large window size helps to prevent the trend from being influenced by short-term fluctuations in just a single year, but rather capture long-term trends and cycles.\nTIP: The window argument in the season() function controls how many years the seasonal component should be estimated over. The default value is 11. When the seasonal window is set to periodic season(window = \"periodic\"), it is the equivalent setting the window to all of the data. When periodic is used, the seasonal component is assumed to be the same each year. The seasonal window argument should always be an odd number or “periodic”.\n\n\n\n\n\n\nExercise 3: Changing Decomposition Types\n\n\n\n\n\nCreate a classical multiplicative decomposition of the monthly MLB injury counts. How do the components differ from the additive decomposition? Which decomposition method’s remainder component looks more like white noise (classical additive or classical multiplicative)?\nCreate an STL decomposition of the monthly MLB injury counts with a shorter length for the trend smoothing window. How does the decomposition change with a shorter trend smoothing window? Particularly, how does the trend component change?\n\n\n\n\nNOTE: You can actually forecast with decomposition as well. If you’d like to learn more about this click here",
    "crumbs": [
      "Home",
      "Baseball",
      "MLB Injuries - Introductory Time Series Analysis"
    ]
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#the-mean-method",
    "href": "baseball/mlb_injuries/index.html#the-mean-method",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "The Mean Method",
    "text": "The Mean Method\nAn extremely simple method for forecasting is the mean method. This method forecasts the next observation as the average of all the observations in the training data. This method will produce a flat forecast that is equal to the mean of the training data. The mean method is useful when the data doesn’t have a trend or seasonality.",
    "crumbs": [
      "Home",
      "Baseball",
      "MLB Injuries - Introductory Time Series Analysis"
    ]
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#the-naive-method",
    "href": "baseball/mlb_injuries/index.html#the-naive-method",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "The Naive Method",
    "text": "The Naive Method\nThe naive method is another simple forecasting method. It forecasts the next observation as the value of the last observation in the training data. This method will produce a flat forecast that is equal to the last observation in the training data. The naive method is useful when the data appears to be random.",
    "crumbs": [
      "Home",
      "Baseball",
      "MLB Injuries - Introductory Time Series Analysis"
    ]
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#seasonal-naive-method",
    "href": "baseball/mlb_injuries/index.html#seasonal-naive-method",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "Seasonal Naive Method",
    "text": "Seasonal Naive Method\nThe seasonal naive method is similar to the naive method, but it forecasts the next observation as the value from the same season in the previous year. This method will produce a repeating pattern of forecasts that are equal to the observations from the same season in the previous year. (Basically forever repeating the last year’s pattern). The seasonal naive method is useful when the data has a strong seasonal pattern but no trend.",
    "crumbs": [
      "Home",
      "Baseball",
      "MLB Injuries - Introductory Time Series Analysis"
    ]
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#drift-method",
    "href": "baseball/mlb_injuries/index.html#drift-method",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "Drift Method",
    "text": "Drift Method\nThe drift method is a simple forecasting method that assumes a linear trend in the data. It forecasts the next observation as the value of the last observation plus the average change between observations. This method will produce a forecast that will continue on a linear trend from the first observation and through the last observation in the training data. The drift method is useful when the data has a linear trend but no seasonality.\n\n\n\n\n\n\nExercise 4: Basic Forecasting Methods\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich of the above time series plots would be best forecasted using the mean method?\nWhich of the above time series plots would be best forecasted using the seasonal naive method?\nWhich of the above time series plots would be best forecasted using the drift method?\n\nUse the following plots of the monthly MLB injury counts and forecasts to answer the following questions.\n\n\n\n\n\n\n\n\n\n\nAfter looking at the forecasts from the mean, naive, seasonal naive, and drift methods for the MLB injury data, which method appears to be the best forecast?\nWhich ones appear to be the worst forecasts?\nWhat is one major issue seen in all of the forecasts in regards to their prediction intervals?",
    "crumbs": [
      "Home",
      "Baseball",
      "MLB Injuries - Introductory Time Series Analysis"
    ]
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#residuals",
    "href": "baseball/mlb_injuries/index.html#residuals",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "Residuals",
    "text": "Residuals\nChecking the residuals of a model is one of the most effective ways to see how well the model is performing. Residuals are the differences between the observed values and the values predicted by the model. \\(e_t = y_t - \\hat{y}_t\\)\n\n\nIt is important to note that when we are talking about residuals in this module that we are referring to the innovation residuals. Most of the time innovation residuals are the same as regular residuals, such as with our seasonal naive model. Innovation residuals are the residuals that are left over after accounting for changes made to the data such as transformations or differencing. These residuals are the ones that are used to check the model assumptions and to evaluate the model’s performance.\nThere are 3 main things to look at when evaluating residuals:\n\nDo the residuals appear to be white noise? Remember that white noise has a mean of 0, constant variance, and shows no obvious patterns. This can be checked by looking at a time plot of the residuals.\nAre the residuals normally distributed? This can be checked by looking at a histogram of the residuals or by using a normal probability plot.\nAre the residuals uncorrelated? This can be checked by looking at the ACF plot of the residuals. There are also statistical tests that can be used to check for autocorrelation in the residuals such as the Ljung-Box test. We can use the Box.test() function in R to perform this test.\n\n\n\nThe formula for the test statistic for a Ljung-Box test is:\n\\[Q^{*} = T(T+2) \\sum_{k=1}^{l} \\frac{r^2_k}{T-k}\\]\nwhere:\n\n\\(T\\) is the number of observations\n\\(l\\) is the max number of lags\n\\(r_k\\) is the sample autocorrelation at lag \\(k\\)\n\nThe null hypothesis for the Ljung-Box test is that the data is not distinguishable from white noise. If the p-value is less than the significance level, we reject the null hypothesis and conclude that the data is autocorrelated.\nThankfully there is a very easy way to check all of these at once in R using the gg_tsresiduals() function.\n\n\n\n\n\n\nExercise 5: Residuals Analysis\n\n\n\nBelow is code that will create a time plot, histogram, and ACF plot of the residuals from the seasonal naive method.\n\nsnaive_mod &lt;- injuries |&gt;\n  model(SNAIVE(Count ~ lag('year')))\n  \nsnaive_mod |&gt; \n  gg_tsresiduals()\n\n\n\n\n\n\n\n\n\nDo the residuals appear to be white noise?\nWhat stands out about the time plot of the residuals?\nDoes the histogram of the residuals appear to be normally distributed?\nAre the residuals uncorrelated? What lag(s) show the most significant autocorrelation and what could this mean for the model?",
    "crumbs": [
      "Home",
      "Baseball",
      "MLB Injuries - Introductory Time Series Analysis"
    ]
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#testing-and-training-for-point-estimate-evaluations",
    "href": "baseball/mlb_injuries/index.html#testing-and-training-for-point-estimate-evaluations",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "Testing and Training for Point Estimate Evaluations",
    "text": "Testing and Training for Point Estimate Evaluations\nIf you want to evaluate the point estimates of a model, you can use a testing and training split. This involves training the model on the first part of the data and then testing the model on the second part of the data. This allows you to see how well the model can forecast future observations.\nFor this example, we will split the data into a training set that contains the first 75% of the data and a testing set that contains the last 25% of the data. This means we will train the models on the data from January 2001 to December 2017 and test the models on the data from January 2018 to December 2023.\n\n\nTIP: 75-25 is a common split for training and testing data, but you can use any split that makes sense for your data. Generally the more data you have, the less percentage you need for testing. Other common splits are 70-30 or 80-20.\n\ntraining &lt;- injuries |&gt; filter(year(Month) &lt; 2018)\ntesting &lt;- injuries |&gt; filter(year(Month) &gt;= 2018)\n\nNow that we have our training and testing data, we can fit the models to the training data and then forecast the testing data.\n\ninjury_fit &lt;- training |&gt; \n  model(mean = MEAN(Count),\n        naive = NAIVE(Count),\n        snaive = SNAIVE(Count ~ lag('year')),\n        drift = RW(Count ~ drift()))\n\ninjury_forecasts &lt;- injury_fit |&gt; \n  forecast(new_data = testing)\n\n\n\nTIP: You can fit multiple models at once by using the model() function as seen in the code to the left. The values to the left of the = are the names we are giving to the models and the values to the right of the = are the models we are fitting to the data.\nTIP: When using the forecast() function, you can specify the new data you want to forecast by using the new_data argument. In this case, we are forecasting for the testing data.\nLet’s visualize the forecasts from the training data and compare them to the testing data.\n\ninjury_forecasts |&gt;\n  autoplot(injuries, level = NULL) +\n  labs(title = \"Forecasting Methods for Monthly MLB Injury Counts\")+\n  guides(color = guide_legend(title = \"Forecast\"))\n\n\n\n\n\n\n\n\nThe seasonal naive method certainly appears to be the best forecast.\nWe can also evaluate these models using the accuracy() function. This function calculates a variety of accuracy measures for the forecasts, including the mean absolute error, root mean squared error, mean absolute percentage error, and more.\n\nThe mean absolute error (MAE) is the average of the absolute errors between the forecasts and the actual values. It is a measure of the average magnitude of the errors in the forecasts. We want this value to be as close to 0 as possible.\n\n\nThe formula for the mean absolute error is: \\[\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} | y_i - \\hat{y}_i |\\] where \\(y_i\\) is the actual value and \\(\\hat{y}_i\\) is the forecasted value.\n\nThe root mean squared error (RMSE) is the square root of the average of the squared errors between the forecasts and the actual values. It is a measure of the standard deviation of the errors in the forecasts. We want this value to be as close to 0 as possible.\n\n\nThe formula for the root mean squared error is: \\[\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\\] Where \\(y_i\\) is the actual value and \\(\\hat{y}_i\\) is the forecasted value.\n\nThe mean absolute percentage error (MAPE) is the average of the absolute percentage errors between the forecasts and the actual values. It is a measure of the accuracy of the forecasts. We want this value to be as close to 0 as possible.\n\n\nThe formula for the mean absolute percentage error is: \\[\\text{MAPE} = \\frac{100}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right|\\] where \\(y_i\\) is the actual value and \\(\\hat{y}_i\\) is the forecasted value.\n\nThe code below displays the accuracy measures for the forecasts.\n\naccuracy(injury_forecasts, testing)\n\n# A tibble: 4 × 10\n  .model .type    ME  RMSE   MAE   MPE  MAPE  MASE RMSSE  ACF1\n  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 drift  Test   86.0 122.   86.0   Inf   Inf   NaN   NaN 0.698\n2 mean   Test   36.2  93.9  79.2  -Inf   Inf   NaN   NaN 0.698\n3 naive  Test   86.0 122.   86.0   100   100   NaN   NaN 0.698\n4 snaive Test   14.3  58.4  34.5  -Inf   Inf   NaN   NaN 0.424\n\n\nThis confirms that the seasonal naive method is the best forecast, as it has the lowest MAE and RMSE values. The MAPE is shown at -Inf to Inf because some of the actual values are 0, which causes the percentage error to be infinite.",
    "crumbs": [
      "Home",
      "Baseball",
      "MLB Injuries - Introductory Time Series Analysis"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SCORE Module Repository",
    "section": "",
    "text": "The SCORE Network Module Repository enables you to search for modules by either sport (along the left), or you can browse by statistics and data science topic. The modules listed in this repository have completed the required SCORE Network pedagogical and industry peer reviews to become a published module.\nInterested in submitting your own module? Click here to find out more information about the SCORE Network module submission process.\nYou can also access preprint modules from various SCORE Network affiliates below (note that these materials have not yet completed the SCORE Network review process):\n\nCarnegie Mellon University\nSt. Lawrence University\nBaylor University + Azusa Pacific University\nWest Point\n\nThe development of the SCORE with Data network is funded by the National Science Foundation (award 2142705)."
  },
  {
    "objectID": "baseball/stolen-bases/index.html",
    "href": "baseball/stolen-bases/index.html",
    "title": "Stolen Bases",
    "section": "",
    "text": "This lesson introduces students to the concept of normality tests (Shapiro-Wilks and Kolmogorov-Smirnov) and summation of normal distributions to investigate stolen base success rates. Featuring Jacob Hurtubise, a West Point’s all-time leader in stolen bases and baseball player for the Cincinnati Reds’ Double-A affiliate, the Chattanooga Lookouts.",
    "crumbs": [
      "Home",
      "Baseball",
      "Stolen Bases"
    ]
  },
  {
    "objectID": "baseball/stolen-bases/index.html#motivation",
    "href": "baseball/stolen-bases/index.html#motivation",
    "title": "Stolen Bases",
    "section": "",
    "text": "This lesson introduces students to the concept of normality tests (Shapiro-Wilks and Kolmogorov-Smirnov) and summation of normal distributions to investigate stolen base success rates. Featuring Jacob Hurtubise, a West Point’s all-time leader in stolen bases and baseball player for the Cincinnati Reds’ Double-A affiliate, the Chattanooga Lookouts.",
    "crumbs": [
      "Home",
      "Baseball",
      "Stolen Bases"
    ]
  },
  {
    "objectID": "baseball/stolen-bases/index.html#module",
    "href": "baseball/stolen-bases/index.html#module",
    "title": "Stolen Bases",
    "section": "Module",
    "text": "Module\nhttps://isle.stat.cmu.edu/SCORE/stolen-bases-module/",
    "crumbs": [
      "Home",
      "Baseball",
      "Stolen Bases"
    ]
  },
  {
    "objectID": "esports/league-of-legends-buffing-nerfing/index.html",
    "href": "esports/league-of-legends-buffing-nerfing/index.html",
    "title": "League of Legends - Buffing and Nerfing",
    "section": "",
    "text": "Welcome video\n\n\n\n\nIntroduction\nLeague of Legends (LoL) is a 5 v. 5 multiplayer online battle arena (MOBA) game developed by Riot Games. In this game, players assume the role of a “champion” with unique abilities and engage in intense battles against a team of other players or computer-controlled champions. Riot Games continually collects data to evaluate the impact of each champion, adjusting and fine-tuning various aspects to ensure fair and competitive gameplay. With regular updates (patches) occurring every two weeks, champions can become either extremely efficient and strong or in need of adjustments to enhance their abilities. Maintaining overall game balance is crucial, and developers employ strategies known as “nerfing” and “buffing” to achieve this balance. “Nerfing” refers to reducing the power or effectiveness of a champion or item, while “buffing” involves increasing its power or effectiveness.\nIn this worksheet, we will analyze and describe histograms of Win Rates for different champions in LoL. The Win Rate, a key metric in the game, represents the percentage of games won by a champion out of the total games played. Understanding the distribution of Win Rates and identifying potential outliers can provide valuable insights into champion balance and performance, informing strategic decision-making in LoL gameplay.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nBy the end of this activity, you will be able to:\n\nUnderstand the concept of histograms and their relevance in statistical analysis.\nAnalyze and describe histograms to gain insights into the distribution of Win Rates in League of Legends. In particular, being able to describe the center, shape, and spread of a distribution based on the displayed graph.\nIdentify potential outliers in a numerical variable using numerical methods such as the “1.5 IQR Rule” or z-scores.\nInterpret the implications of outliers in terms of champion balance and performance.\n\n\n\n\n\n\n\n\n\n\nMethods\n\n\n\n\n\nTechnology requirement: The activity handout provides histograms and summary statistics so that no statistical software is required. However, the activity could be modified to ask students to produce that information from the raw dataset and/or extend the activity to investigate other variables available in the data.\n\nHistograms: Familiarity with histograms as a graphical representation of the distribution of a continuous variable, such as Win Rates, is crucial. You should understand how to interpret histograms, including the concepts of bins, frequencies, and the shape, center, and spread of distributions.\nOutliers: Knowledge of outliers, which are data points that deviate significantly from the overall pattern, is important.\nFamiliarity with basic statistical analysis techniques, such as measures of central tendency (mean, median) and measures of dispersion (standard deviation, range), will aid in interpreting and analyzing the histograms. These techniques can provide insights into the overall characteristics and variability of the Win Rates.\nKnowledge of outlier detection methods (such as the 1.5 IQR Rule and/or z-scores) is fundamental to the activity.\n\n\n\n\n\n\nData\nA data frame for 162 champions of the following 7 variables. Each row represents a Champion that you can choose when playing League of Legends during patches 12.22 and 12.23. Note that the activity provided in this module does not use all of the variables provided. Instead they are provided for further analyses at the discretion of the user.\nAvailable on the SCORE Data Repository\nDownload data: LOL_patch_12.22.csv\nDownload data: LOL_patch_12.23.csv\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nName\nname of the champion\n\n\nRole\nrole of the champion in a game\n\n\nKDA\nAverage kills, deaths and assists associated with each champion\n\n\nWRate\nwin rates of each champion\n\n\nPickRate\npick rates of each champion\n\n\nRolePerc\npercentage of time playing as a role\n\n\nBanPerc\nban percentages associated with each champion\n\n\n\n\n\nData Source\nLol champion stats, 12.22 master, win rates. METAsrc. (n.d.). https://www.metasrc.com/5v5/12.22/stats?ranks=master\nLol champion stats, 12.23 master, win rates. METAsrc. (n.d.-b). https://www.metasrc.com/5v5/12.23/stats?ranks=master\n\n\n\nMaterials\nClass handout\nClass handout - with solutions\n\n\n\n\n\n\nConclusion\n\n\n\n\n\nIn conclusion, the analysis of Win Rates histograms in League of Legends has provided valuable insights into champion balance and performance. One notable finding from this worksheet is the identification of Sion as a low outlier in the Win Rates for the 12.22 patch. However, in the subsequent 12.23 patch, Sion’s Win Rate improved, and was no longer an outlier. Sion was given a Buff in patch 12.23, resulting in an enhanced performance and a more balanced Win Rate. However, patch 12.23 resulted in two new outliers with low win rates.\nThe continued presence of outliers highlights the importance of continuous monitoring and adjustments by game developers to ensure fair and competitive gameplay.\n\n\n\n\n\nHow to Cite\nIf you use this module in your work, please cite it as follows:\nRamler, I., Charalambous, G., & Dykstra, A. J. (2025, April 30). League of Legends. “The SCORE Network.” https://doi.org/10.17605/OSF.IO/8R3YG\nYou can include this citation directly in your references or bibliography.",
    "crumbs": [
      "Home",
      "Esports",
      "League of Legends - Buffing and Nerfing"
    ]
  },
  {
    "objectID": "hockey/nhl-shooting-percentage-ventura/index.html",
    "href": "hockey/nhl-shooting-percentage-ventura/index.html",
    "title": "Predicting NHL Shooting Percentages",
    "section": "",
    "text": "https://isle.stat.cmu.edu/SCORE/NHLShots/",
    "crumbs": [
      "Home",
      "Hockey",
      "Predicting NHL Shooting Percentages"
    ]
  },
  {
    "objectID": "hockey/nhl-shooting-percentage-ventura/index.html#module",
    "href": "hockey/nhl-shooting-percentage-ventura/index.html#module",
    "title": "Predicting NHL Shooting Percentages",
    "section": "",
    "text": "https://isle.stat.cmu.edu/SCORE/NHLShots/",
    "crumbs": [
      "Home",
      "Hockey",
      "Predicting NHL Shooting Percentages"
    ]
  },
  {
    "objectID": "hockey/nhl-shooting-percentage-ventura/index.html#how-to-cite",
    "href": "hockey/nhl-shooting-percentage-ventura/index.html#how-to-cite",
    "title": "Predicting NHL Shooting Percentages",
    "section": "How to Cite",
    "text": "How to Cite\nIf you use this module in your work, please cite it as follows:\nSchuckers, M., Macdonald, B., & Ventura, S. (2025, April 30). Hockey Regression. “The SCORE Network.” https://doi.org/10.17605/OSF.IO/YUX6T\nYou can include this citation directly in your references or bibliography.",
    "crumbs": [
      "Home",
      "Hockey",
      "Predicting NHL Shooting Percentages"
    ]
  },
  {
    "objectID": "lacrosse/lacrosse_pll_vs_nll/index.html",
    "href": "lacrosse/lacrosse_pll_vs_nll/index.html",
    "title": "Lacrosse PLL vs. NLL",
    "section": "",
    "text": "Please note that these material have not yet completed the required pedagogical and industry peer-reviews to become a published module on the SCORE Network. However, instructors are still welcome to use these materials if they are so inclined.\n\n\nThis module examines the goals and shots in two prominent lacrosse leagues: the Premier Lacrosse League (PLL) and the National Lacrosse League (NLL). The PLL and NLL are highly regarded professional lacrosse leagues that feature top-tier athletes from around the world.\nThe PLL is played in an outdoor setting, following the field lacrosse format. This style of lacrosse is characterized by its larger field size, typically 110 yards by 60 yards. Field lacrosse involves 10 players per team and promotes a style of play that emphasizes long passes, intricate plays, and individual skills. Founded in 2019, the PLL operates with a touring model where teams travel to different cities each weekend, bringing the sport to a wide audience and fostering a festival-like atmosphere at each event. The league’s modernized approach includes a strong emphasis on media presence and player engagement. The PLL features approximately 200 athletes across its teams.\nIn contrast, the NLL follows the box lacrosse format, which is played indoors on a smaller, enclosed field, generally 200 feet by 85 feet. Box lacrosse involves 6 players per team, and the gameplay is marked by frequent physical interactions, quick ball movements, and high-intensity transitions. Established in 1986, the NLL has a traditional franchise model with teams based in specific cities across the United States and Canada. This structure has cultivated strong local fan bases and deep community ties, contributing to the league’s longevity. The NLL features approximately 500 athletes, many of which also play in the PLL.\nThese data, from the 2021-2022 seasons, allow for an analysis of goal-scoring within these leagues to identify differences between indoor (NLL) and outdoor (PLL) play. By examining goals and shots, we aim to understand how the environment and format of the game influence offensive strategies and overall scoring trends in professional lacrosse. It’s important to recognize that outdoor field lacrosse and indoor box lacrosse are distinct sports, each with its own unique dynamics, rules, and playing styles. By acknowledging these nuances, we can may gain a deeper understanding of how various playing conditions and league structures impact statistical outcomes.\n\n\n\n\n\n\nActivity Length\n\n\n\n\n\nThis activity would be suitable for an in-class example or can be modified to be a quiz or part of an exam.\n\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nThe learning goals associated with this module are:\n\nStudents will be able to test for a difference in means between two groups.\nStudents will be able to find a confidence interval for a difference in means between two groups.\n\n\n\n\n\n\n\n\n\n\nMethods\n\n\n\n\n\nThis module requires students use a two-sample test and confidence interval (e.g., t-test or randomization) to compare the means of two groups.\nStudents are expected to have access any equations and/or lecture notes to complete the activity.\nTechnology requirement:\n\nThe provided handout assumes that students can use technology such as calculators to perform a two-sample t-test and interval for a difference in means between two groups.\nThe raw data is provided to allow instructors to customize the handout to incorporate other forms of technology. e.g., Students can use software such as Minitab to calculate test statistics and p-values for a t-test or StatKey for simulation based inference.\n\n\n\n\n\n\n\nThe data set has 162 rows with 9 columns. Each row represents a single lacrosse match either in the Premier Lacrosse League or the National Lacrosse League during the 2021-2022 season.\nDownload data: lacrosse_pll_nll_2021-2022.csv\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nLeague\nThe Premier Lacrosse league and the National Lacrosse League\n\n\nAway_team\nThe traveling team\n\n\nHome_team\nThe hosting team\n\n\nAway_shots\nHow many shots the Away_team had on net\n\n\nHome_shots\nHow many shots the Home_team had on net\n\n\nAway_goals\nHow many goals the Away_team had on net\n\n\nHome_goals\nHow many goals the Home_team had on net\n\n\nGoals\nThe total amount of goals scored each game\n\n\nGoals_per_48\nThe average amount of goals for the first 48 minutes of a game\n\n\n\n\n\n\nPremier Lacrosse League stats. Premier Lacrosse League Stats. (n.d.). https://stats.premierlacrosseleague.com/\nPlayer stats. NLL. (2023, January 26). https://www.nll.com/stats/all-player-stats/\n\n\n\n\nThe data and worksheet associated with this module are available for download through the following links.\n\nlacrosse_pll_nll_2012-2022.csv - Dataset with game-by-game shots and goals scored for both leagues in the 2021-2022 season..\nlacrosse_pll_vs_nll_t-test_worksheet.docx- Activity worksheet to compare scoring and shots between indoor and outdoor leagues using t-distributions.\nlacrosse_pll_vs_nll_randomization_worksheet.docx- Activity worksheet to compare scoring and shots between indoor and outdoor leagues using randomization tests implements via StatKey.\n\nSample solutions to the worksheets\n\nlacrosse_pll_vs_nll_t-test_worksheet_key.docx - Activity worksheet using t-distributions with sample solutions.\nlacrosse_pll_vs_nll_randomization_worksheet_key.docx - Activity worksheet using randomization tests with sample solutions.\n\n\n\n\n\n\n\nConclusion\n\n\n\n\n\nStudents should notice that while no discernible difference between average goals per game was discovered, after adjusting for the length of the game, PLL (i.e., the 48-minute outdoor league) has the higher rate per 48-minutes.\nFurther, students will ideally see how a confidence interval can be used to supplement the conclusion of a hypothesis test by bringing effect sizes into the interpretation in addition to statistical significance.",
    "crumbs": [
      "Home",
      "Lacrosse",
      "Lacrosse PLL vs. NLL"
    ]
  },
  {
    "objectID": "lacrosse/lacrosse_pll_vs_nll/index.html#module",
    "href": "lacrosse/lacrosse_pll_vs_nll/index.html#module",
    "title": "Lacrosse PLL vs. NLL",
    "section": "",
    "text": "Please note that these material have not yet completed the required pedagogical and industry peer-reviews to become a published module on the SCORE Network. However, instructors are still welcome to use these materials if they are so inclined.\n\n\nThis module examines the goals and shots in two prominent lacrosse leagues: the Premier Lacrosse League (PLL) and the National Lacrosse League (NLL). The PLL and NLL are highly regarded professional lacrosse leagues that feature top-tier athletes from around the world.\nThe PLL is played in an outdoor setting, following the field lacrosse format. This style of lacrosse is characterized by its larger field size, typically 110 yards by 60 yards. Field lacrosse involves 10 players per team and promotes a style of play that emphasizes long passes, intricate plays, and individual skills. Founded in 2019, the PLL operates with a touring model where teams travel to different cities each weekend, bringing the sport to a wide audience and fostering a festival-like atmosphere at each event. The league’s modernized approach includes a strong emphasis on media presence and player engagement. The PLL features approximately 200 athletes across its teams.\nIn contrast, the NLL follows the box lacrosse format, which is played indoors on a smaller, enclosed field, generally 200 feet by 85 feet. Box lacrosse involves 6 players per team, and the gameplay is marked by frequent physical interactions, quick ball movements, and high-intensity transitions. Established in 1986, the NLL has a traditional franchise model with teams based in specific cities across the United States and Canada. This structure has cultivated strong local fan bases and deep community ties, contributing to the league’s longevity. The NLL features approximately 500 athletes, many of which also play in the PLL.\nThese data, from the 2021-2022 seasons, allow for an analysis of goal-scoring within these leagues to identify differences between indoor (NLL) and outdoor (PLL) play. By examining goals and shots, we aim to understand how the environment and format of the game influence offensive strategies and overall scoring trends in professional lacrosse. It’s important to recognize that outdoor field lacrosse and indoor box lacrosse are distinct sports, each with its own unique dynamics, rules, and playing styles. By acknowledging these nuances, we can may gain a deeper understanding of how various playing conditions and league structures impact statistical outcomes.\n\n\n\n\n\n\nActivity Length\n\n\n\n\n\nThis activity would be suitable for an in-class example or can be modified to be a quiz or part of an exam.\n\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nThe learning goals associated with this module are:\n\nStudents will be able to test for a difference in means between two groups.\nStudents will be able to find a confidence interval for a difference in means between two groups.\n\n\n\n\n\n\n\n\n\n\nMethods\n\n\n\n\n\nThis module requires students use a two-sample test and confidence interval (e.g., t-test or randomization) to compare the means of two groups.\nStudents are expected to have access any equations and/or lecture notes to complete the activity.\nTechnology requirement:\n\nThe provided handout assumes that students can use technology such as calculators to perform a two-sample t-test and interval for a difference in means between two groups.\nThe raw data is provided to allow instructors to customize the handout to incorporate other forms of technology. e.g., Students can use software such as Minitab to calculate test statistics and p-values for a t-test or StatKey for simulation based inference.\n\n\n\n\n\n\n\nThe data set has 162 rows with 9 columns. Each row represents a single lacrosse match either in the Premier Lacrosse League or the National Lacrosse League during the 2021-2022 season.\nDownload data: lacrosse_pll_nll_2021-2022.csv\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nLeague\nThe Premier Lacrosse league and the National Lacrosse League\n\n\nAway_team\nThe traveling team\n\n\nHome_team\nThe hosting team\n\n\nAway_shots\nHow many shots the Away_team had on net\n\n\nHome_shots\nHow many shots the Home_team had on net\n\n\nAway_goals\nHow many goals the Away_team had on net\n\n\nHome_goals\nHow many goals the Home_team had on net\n\n\nGoals\nThe total amount of goals scored each game\n\n\nGoals_per_48\nThe average amount of goals for the first 48 minutes of a game\n\n\n\n\n\n\nPremier Lacrosse League stats. Premier Lacrosse League Stats. (n.d.). https://stats.premierlacrosseleague.com/\nPlayer stats. NLL. (2023, January 26). https://www.nll.com/stats/all-player-stats/\n\n\n\n\nThe data and worksheet associated with this module are available for download through the following links.\n\nlacrosse_pll_nll_2012-2022.csv - Dataset with game-by-game shots and goals scored for both leagues in the 2021-2022 season..\nlacrosse_pll_vs_nll_t-test_worksheet.docx- Activity worksheet to compare scoring and shots between indoor and outdoor leagues using t-distributions.\nlacrosse_pll_vs_nll_randomization_worksheet.docx- Activity worksheet to compare scoring and shots between indoor and outdoor leagues using randomization tests implements via StatKey.\n\nSample solutions to the worksheets\n\nlacrosse_pll_vs_nll_t-test_worksheet_key.docx - Activity worksheet using t-distributions with sample solutions.\nlacrosse_pll_vs_nll_randomization_worksheet_key.docx - Activity worksheet using randomization tests with sample solutions.\n\n\n\n\n\n\n\nConclusion\n\n\n\n\n\nStudents should notice that while no discernible difference between average goals per game was discovered, after adjusting for the length of the game, PLL (i.e., the 48-minute outdoor league) has the higher rate per 48-minutes.\nFurther, students will ideally see how a confidence interval can be used to supplement the conclusion of a hypothesis test by bringing effect sizes into the interpretation in addition to statistical significance.",
    "crumbs": [
      "Home",
      "Lacrosse",
      "Lacrosse PLL vs. NLL"
    ]
  },
  {
    "objectID": "marathons/marathon-records/index.html",
    "href": "marathons/marathon-records/index.html",
    "title": "Marathon Record-Setting Over Time",
    "section": "",
    "text": "https://isle.stat.cmu.edu/SCORE/Marathons_SCORE_Template/",
    "crumbs": [
      "Home",
      "Marathons",
      "Marathon Record-Setting Over Time"
    ]
  },
  {
    "objectID": "marathons/marathon-records/index.html#module",
    "href": "marathons/marathon-records/index.html#module",
    "title": "Marathon Record-Setting Over Time",
    "section": "",
    "text": "https://isle.stat.cmu.edu/SCORE/Marathons_SCORE_Template/",
    "crumbs": [
      "Home",
      "Marathons",
      "Marathon Record-Setting Over Time"
    ]
  },
  {
    "objectID": "motor_sports/nascar_regression_transformation/index.html",
    "href": "motor_sports/nascar_regression_transformation/index.html",
    "title": "NASCAR Transformation Module",
    "section": "",
    "text": "Introduction\nIn NASCAR, driver rating is a metric used to evaluate the performance of drivers in races. It provides a comprehensive assessment of a driver’s overall competitiveness, efficiency, and consistency during a race. The driver rating is based on several key performance factors and is designed to offer a more objective view of a driver’s abilities. For this activity, you will be exploring the relationship between average position a driver finishes per lap over a season and their corresponding driver rating. Using data transformations techniques and polynomial regression to create different variations of linear models, you will enhance the capabilities of your models to make them more effective and accurate.\n\n\n\n\n\n\nMore about NASCAR\n\n\n\n\n\nNASCAR, the National Association for Stock Car Auto Racing, is a preeminent motorsports organization in the United States, distinguished by its high-speed competitions and fervent fanbase. Established in 1948 by Bill France Sr., NASCAR has evolved into a leading racing series that encompasses three national divisions: the NASCAR Cup Series, the Xfinity Series, and the Truck Series. The Cup Series is the most prestigious, showcasing the elite drivers and teams as they compete in events that span a variety of track types, typically oval in shape, and ranging from short tracks (0.24 miles) to expansive superspeedways (over 3 miles).\nThe races take place on diverse tracks across the nation, including renowned venues such as Daytona International Speedway and Talladega Superspeedway, where vehicles frequently exceed speeds of 200 miles per hour. NASCAR races serve as a complex interplay of driver skill, team strategy, and vehicle performance. Key metrics such as lap times, pit stop efficiency, and vehicle dynamics offer insights into the determinants of success in the sport. This not only enhances the understanding of race outcomes but also contributes to the development of strategies that optimize performance.\n\n\n\n\n\n\n\n\n\nHow NASCAR uses data\n\n\n\n\n\nThe video linked below is a panel discussion from the 2023 Sloan Sports Analytics Conference entitled Start Your Engines: How Data is Fueling NASCAR’s Strategy to Engage an Evolving Customer Base.\nAlthough the panel discussion is not directly related to this module, Justin Marks discusses how data is used in NASCAR during the 26:53 to 30:44 minute interval.\n\n\n\n\n\n\n\n\n\n\n\nActivity Length\n\n\n\n\n\nThis activity would be suitable for an in-class activity lasting approximately one class period or as an out-of-class assignment.\n\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nBy the end of this activity, students will have reinforced the following skills:\n\nAssessing the effectiveness of simple linear regression models\nChecking regression model assumptions\nUsing log transformations to improve linear regression model fit\nApplying polynomial regression to model curved relationships\n\n\n\n\n\n\n\n\n\n\nMethods\n\n\n\n\n\nFor this activity, students will need to use software to create scatterplots and plots of residual vs fitted values of models they will create. They will also need to create polynomial models and mutate the data by applying mathematic functions to columns.\nIt is assumed that students are already exposed to these concepts as the activity is intended to reinforce the skills instead of introduce them.\n\n\n\n\n\nData\nThe data comes from the NASCAR website and shows the season statistics from 2007-2022. Each row displays the metrics of a racer for that specific year. The data frame contains 1111 rows of observations and 20 variables.\nDownload data: nascar_driver_statistics.csv\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nWins\nThe sum of the driver’s victories\n\n\nAvgStart\nThe sum of the driver’s starting positions divided by the number of races\n\n\nAvgMidRace\nThe sum of the driver’s mid race positions divided by the number of races\n\n\nAvgFinish\nThe sum of the driver’s finishing positions divided by the number of races\n\n\nAvgPos\nThe sum of the driver’s position each lap divided by the number of laps\n\n\nPassDiff\nThe sum of green flag passes minus green times passed\n\n\nGreenFlagPasses\nNumber of green flag passes performed by the driver\n\n\nGreenFlagPassed\nNumber of times driver is passed during green flag\n\n\nQualityPasses\nNumber of passes in the top 15 while under green flag conditions by driver\n\n\nPercentQualityPasses\nThe sum of quality passes divided by green flag passes\n\n\nNumFastestLaps\nNumber of where the driver had the fastest speed on the lap\n\n\nLapsInTop15\nNumber of laps completed while running in a top 15 position\n\n\nPercentLapsInTop15\nThe sum of the laps run in the top 15 divided by total laps completed\n\n\nLapsLed\nThe sum of the laps led in a race\n\n\nPercentLapsLed\nThe sum of the laps led in the race\n\n\nTotalLaps\nThe sum of the laps completed by a driver that year\n\n\nDriverRating\nFormula combining wins, finish, top15-finish, average running position while on lead lap, average speed under green, fastest lap, led most laps, and lead lap finish with a maximum rating of 150 points\n\n\n\n\n\n\nMaterials\nClass handout\nClass handout - with solutions: MS Word\nClass handout - with solutions: Quarto\n\n\n\n\n\n\nConclusion\n\n\n\n\n\nIn conclusion, the Transforming NASCAR Driver Data worksheet offers valuable insights into the relationship between average position and driver rating in NASCAR. Through the transformation of the average position variable, the worksheet enables students to enhance linearity in their models, thereby improving the accuracy of their predictions and analysis. The identification of curvature in the variable relationship also allows for the model using quadratic regression to be highly effective. The model can be conceptually compared to the model using a square root transformation to capture the same curve. Assessing the effectiveness of each model, students can critically evaluate the different approaches and determine the most suitable model for the data. This exercise equips students with essential data transformation and modeling skills, empowering them to make informed decisions and gain a deeper understanding of the factors influencing a driver’s performance.",
    "crumbs": [
      "Home",
      "Motor Sports",
      "NASCAR Transformation Module"
    ]
  },
  {
    "objectID": "rowing/olympic_rowing_datawrangling/index.html",
    "href": "rowing/olympic_rowing_datawrangling/index.html",
    "title": "Olympic Rowing Medals Between 1900 and 2022 - Data Wrangling",
    "section": "",
    "text": "Introduction to Rowing\nIf you are unfamiliar with the sport of rowing, we encourage you to watch the following video from World Rowing\n\n\n\n\nIntroduction to Module\nThis activity organizes the data in Olympic rowing between 1900 and 2022 so that the total medals and points for countries can be analyzed.\nThe Summer Olympic Games are an international athletics event held every four years and hosted in different countries around the world. Rowing was added to the Olympics in 1896 but was cancelled due to weather. It has been in every Summer Olympics since 1900. Rowing races in the Olympic context are regatta style, meaning that there are multiple boats racing head to head against each other in multiple lanes. Since 1912, the standard distance for Olympic regattas has been 2000m, but before then there had been a range in distances. The boat that is first to cross the finish line is awarded a gold medal, the second a silver medal, and the third a bronze.\nOver the course of its time as an Olympic sport there have been 25 different event entries. These events are separated by gender and range with the number of rowers in the boat (1, 2, 4, 6, 8, 17), the rigging (inrigged, outrigged), sculling, sweeping, and whether or not they are coxed. An inrigged shell means the riggers (where the oar is attached to the boat) are on the inside of the boat, outrigged shells mean the riggers are on the outside. Sculling is where the rowers have an oar on each side and sweeping is when each rower only has one oar on one side. The coxswain steers the boat and guides the rowers, some events have coxed boats whereas some others do not.\nThe original data set (athlete_events.csv) is not just about rowing but for every Olympic sport since 1896. It will be important to adjust the data so that it is just rowing. Moreover, medals are given to each athlete in the boat. In Olympic scoring however, the medals should be counted as one towards the entire boat. It is important to make sure this is the case when arranging the data.\nIn looking at the total medals and total points for each nation, it is interesting to see which nations dominate in Olympic rowing. Additionally, looking at the overall distribution of the medals for all countries provides insight on just how lop-sided medaling can be in rowing at the Olympic level. This effect could likely be attributed towards how much funding nations are placing towards their rowing teams.\n\n\n\n\n\n\nActivity Length\n\n\n\n\n\nThis activity could be used as an example or a short take home assessment. Best to use as a predecessor to the introductory statistics rowing module associated with this module.\n\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nBy the end of the activity, students will be able to:\n\nFilter a dataset so there are no NA values\nGroup data to procure summarized data\nMutate data to create new variables\n\n\n\n\n\n\n\n\n\n\nMethods\n\n\n\n\n\nStudents will use basic dplyr skills to structure data to match the descriptions listed in the questions. Basic ggplot2 functions are also needed to visualize the data.\n\n\n\n\n\n\n\n\n\nTechnology Requirements\n\n\n\n\n\nAll the questions will require the use of R or a similar programming language.\n\n\n\n\n\nData\nIn the data set there are 271,106 athletes from 230 countries competing in 66 sports at 35 different Olympics. Each row represents an individual athlete competing in an event at an Olympics. There are 15 variables in the dataset.\n\nDownload data:\n\nathlete_events.csv\n\n\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nID\nThe ID number assigned to the athlete.\n\n\nName\nThe name of the athlete in first name last name order.\n\n\nSex\nThe sex of the athlete.\n\n\nAge\nThe age of the athlete at the time of competing.\n\n\nHeight\nThe height of the athlete at the time of competing. Measured in cm.\n\n\nWeight\nThe weight of the athlete at the time of competing. Measured in kg.\n\n\nTeam\nThe team the athlete is a member of. In some cases this is different than theNOC.\n\n\nNOC\nThe nation the athlete is representing, usually the best variable to choose to analyze nations.\n\n\nGames\nThe Olympic Games the case is from. e.g. “1992 Summer Olympics”.\n\n\nYear\nThe year the athlete was competing.\n\n\nSeason\nThe season the athlete was competing, “Summer” or “Winter”.\n\n\nCity\nThe city the Olympics were hosted in the case.\n\n\nSport\nThe sport the athlete competed in.\n\n\nEvent\nThe event the athlete competed in.\n\n\nMedal\nIf the athlete medaled, which medal they won.\n\n\n\nData Source\nKaggle: 120 years of Olympic history: athletes and results\n\n\n\nMaterials\nWe provide editable qmd files along with their solutions.\nWorksheet\nWorksheet Answers\n\n\n\n\n\n\nConclusion\n\n\n\n\n\nThis dataprep worksheet helps students familiarize themselves with the use of basic dplyr tools to structure data in a way that is easier to analyze. In doing so, it enables to students to draw conclusions about different patterns in rowing as an Olympic sport when it comes to medaling.\n\n\n\n\n\nHow to Cite\nIf you use this module in your work, please cite it as follows:\nSmith, A., Lock, R., & Ramler, I. (2025, June 12). Olympic Rowing - Data Wrangling. “The SCORE Network.” https://doi.org/10.17605/OSF.IO/XRYTQ\nYou can include this citation directly in your references or bibliography.",
    "crumbs": [
      "Home",
      "Rowing",
      "Olympic Rowing Medals Between 1900 and 2022 - Data Wrangling"
    ]
  },
  {
    "objectID": "soccer/expected_goals/index.html",
    "href": "soccer/expected_goals/index.html",
    "title": "Expected Goals in Soccer",
    "section": "",
    "text": "This module introduces students to Logistic Regression, Feature Engineering, and Undersampling using a soccer-specific Expected Goals Model. We explain how to create a logistic regression model, using data gathered by Statsbomb from the 2022 World Cup.\nThis module is available on the ISLE platform Expected Goals in Soccer Module",
    "crumbs": [
      "Home",
      "Soccer",
      "Expected Goals in Soccer"
    ]
  },
  {
    "objectID": "soccer/expected_goals/index.html#module",
    "href": "soccer/expected_goals/index.html#module",
    "title": "Expected Goals in Soccer",
    "section": "",
    "text": "This module introduces students to Logistic Regression, Feature Engineering, and Undersampling using a soccer-specific Expected Goals Model. We explain how to create a logistic regression model, using data gathered by Statsbomb from the 2022 World Cup.\nThis module is available on the ISLE platform Expected Goals in Soccer Module",
    "crumbs": [
      "Home",
      "Soccer",
      "Expected Goals in Soccer"
    ]
  },
  {
    "objectID": "soccer/expected_goals/index.html#how-to-cite",
    "href": "soccer/expected_goals/index.html#how-to-cite",
    "title": "Expected Goals in Soccer",
    "section": "How to Cite",
    "text": "How to Cite\nIf you use this module in your work, please cite it as follows:\nKim, C., & Lee, A. (2025, May 19). Soccer - Expected Goals. “The SCORE Network.” https://doi.org/10.17605/OSF.IO/953BP\nYou can include this citation directly in your references or bibliography.",
    "crumbs": [
      "Home",
      "Soccer",
      "Expected Goals in Soccer"
    ]
  },
  {
    "objectID": "triathlons/ironman-lakeplacid-mlr/index.html",
    "href": "triathlons/ironman-lakeplacid-mlr/index.html",
    "title": "Ironman Triathlon (Canadian Females) - Multiple Linear Regression",
    "section": "",
    "text": "Welcome video\n\n\n\nIntroduction\nAn Ironman triathlon is one of the most grueling endurance events in the world, consisting of three sequential races: a 2.4-mile (3.86 km) swim, a 112-mile (180.25 km) bike ride, and a marathon 26.2-mile (42.20 km) run. The event tests the physical and mental limits of athletes, requiring months or even years of dedicated training. Originating in 1978 in Hawaii, the Ironman triathlon has become a global phenomenon, symbolizing the ultimate challenge in long-distance triathlon competitions.\nThe Triathlon Multiple Linear Regression module focuses on analyzing the relationships and predictive capabilities of multiple variables in the context of triathlon performance. Specifically, we will explore the prediction of run times (the last stage) using swim times and bike times as predictors. By employing simple regression models, checking conditions with residuals plots, and conducting hypothesis tests and confidence intervals, we aim to identify significant predictors and understand their contextual implications. The dataset used for this analysis comprises the 2022 Canadian finishers of the Lake Placid Ironman.\n\n\n\n\n\n\nActivity Length\n\n\n\n\n\nThis activity would be suitable for an long in-class example (of approximately 30 - 60 minutes) or out-of-class assignment.\n\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\n\nUnderstand the concept of multiple linear regression and its application in analyzing sports data.\nRecognize the importance of checking assumptions, such as assessing residuals vs. fits plots, for validating the regression models.\nGain insights into the significance of predictors through hypothesis tests and confidence intervals.\nComprehend the interpretation and contextual implications of the findings from multiple linear regression analyses in the triathlon setting.\n\n\n\n\n\n\n\n\n\n\nMethods\n\n\n\n\n\nTo successfully complete this worksheet, students should have prior knowledge of the following statistical concepts:\n\nFamiliarity with simple linear regression, including interpretation of slopes, intercepts, and residuals.\nUnderstanding of hypothesis testing and confidence intervals, particularly in the context of regression analysis.\nKnowledge of residual analysis, including the interpretation of residual plots.\nBasic understanding of the triathlon sport and its components (swimming, biking, and running) to better grasp the contextual implications of the statistical analyses.\n\nTechnology requirement:\n\nThe “no tech” version of the activity uses Minitab to display the output and requires only simple hand calculations (for predictions and residuals).\nThe provided “tech required” handout assumes that students can use technology (such as Minitab) to perform a multiple linear regression analysis.\n\n\n\n\n\n\nData\nThe data set has 64 rows with 17 columns. Each row represents a Canadian female who has participated in the 2022 Lake Placid Ironman. Note that this data set includes more variables than what are needed to complete the activity. Students are welcome to further explore the data using these additional variables.\nDownload data: ironman_lake_placid_female_2022_canadian.csv\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nBib\nregistration number of each runner used for identification\n\n\nName\nThe participant’s name\n\n\nCountry\nWhat country the participant is from\n\n\nGender\nThe participant’s gender\n\n\nDivision\nThe age range or membership a runner is\n\n\nDivision.Rank\nWithin the divisions, the place each runner has obtained over all races\n\n\nOverall.Time\nThe total time it took to complete the Ironman in minutes\n\n\nOverall.Rank\nThe runner’s finishing place for that particular triathlon\n\n\nSwim.Time\nThe time in minutes it took to complete the swimming portion\n\n\nSwim.Rank\nThe place the runner finished for the swim portion\n\n\nBike.Time\nThe time in minutes it took to complete the biking portion\n\n\nBike.Rank\nThe place the runner finished for the bike portion\n\n\nRun.Time\nThe time in minutes it took to complete the running portion\n\n\nRun.Rank\nThe place the runner finished for the running portion\n\n\nFinish.Status\nStates whether someone completed the Ironman successfully\n\n\nLocation\nWhere the Ironman takes place\n\n\nYear\nThey year when the mentioned participant ran\n\n\n\n\n\n\nMaterials\nClass handout - requires technology\nClass handout - no technology required\nClass handout - with solutions\n\n\n\n\n\n\nConclusion\n\n\n\n\n\nIn conclusion, the Triathlon Multiple Linear Regression worksheet has provided valuable insights into the predictors of run times in the Lake Placid 2022 Ironman Canadian Finishers dataset. Initially, both Swim Time and Bike Time demonstrated significance as individual predictors, supported by hypothesis tests and confidence intervals. The hypothesis test for the slope term of Bike Time yielded a p-value of 0, indicating its significance. Additionally, the 95% confidence interval for the slope term of Swim Time excluded 0, further affirming its significance.\nHowever, when both Swim Times and Bike Times were included as predictors in the multiple linear regression model, we observed a change in significance. Swim Times were no longer a significant predictor. This finding suggests that when considering both predictors simultaneously, the predictive power of Swim Times in estimating run times diminishes. These findings emphasize the importance of evaluating multiple predictors in regression analysis and understanding how their inclusion can impact the significance and interpretation of individual predictors.",
    "crumbs": [
      "Home",
      "Triathlons",
      "Ironman Triathlon (Canadian Females) - Multiple Linear Regression"
    ]
  },
  {
    "objectID": "preprints.html",
    "href": "preprints.html",
    "title": "Preprint Access",
    "section": "",
    "text": "This page links to preprint repositories maintained by members of the SCORE Network. These preprint repositories are created and maintained by faculty and students from the respective institutions. Please note that these materials have not yet completed the required pedagogical and industry peer reviews to become a published module on the SCORE Network. However, instructors are still welcome to use these materials if they are so inclined. The following are actively maintained preprint repositories:\n\nCarnegie Mellon University\nSt. Lawrence University\nBaylor University + Azusa Pacific University\nWest Point",
    "crumbs": [
      "Home",
      "Preprint Access"
    ]
  }
]